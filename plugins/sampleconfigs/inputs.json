[
	{
		"Name": "activemq",
		"Description": " Gather ActiveMQ metrics",
		"SampleConfig": "[[inputs.activemq]]\n  ## ActiveMQ WebConsole URL\n  url = \"http://127.0.0.1:8161\"\n\n  ## Required ActiveMQ Endpoint\n  ##   deprecated in 1.11; use the url option\n  # server = \"192.168.50.10\"\n  # port = 8161\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Required ActiveMQ webadmin root path\n  # webadmin = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "aerospike",
		"Description": " Read stats from aerospike server(s)",
		"SampleConfig": "[[inputs.aerospike]]\n  ## Aerospike servers to connect to (with port)\n  ## This plugin will query all namespaces the aerospike\n  ## server has configured and get stats for them.\n  servers = [\"localhost:3000\"]\n\n  # username = \"telegraf\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # enable_tls = false\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # tls_name = \"tlsname\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n  # Feature Options\n  # Add namespace variable to limit the namespaces executed on\n  # Leave blank to do all\n  # disable_query_namespaces = true # default false\n  # namespaces = [\"namespace1\", \"namespace2\"]\n\n  # Enable set level telemetry\n  # query_sets = true # default: false\n  # Add namespace set combinations to limit sets executed on\n  # Leave blank to do all sets\n  # sets = [\"namespace1/set1\", \"namespace1/set2\", \"namespace3\"]\n\n  # Histograms\n  # enable_ttl_histogram = true # default: false\n  # enable_object_size_linear_histogram = true # default: false\n\n  # by default, aerospike produces a 100 bucket histogram\n  # this is not great for most graphing tools, this will allow\n  # the ability to squash this to a smaller number of buckets\n  # To have a balanced histogram, the number of buckets chosen\n  # should divide evenly into 100.\n  # num_histogram_buckets = 100 # default: 10\n"
	},
	{
		"Name": "aliyuncms",
		"Description": " Pull Metric Statistics from Aliyun CMS",
		"SampleConfig": "[[inputs.aliyuncms]]\n  ## Aliyun Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Ram RoleArn credential\n  ## 2) AccessKey STS token credential\n  ## 3) AccessKey credential\n  ## 4) Ecs Ram Role credential\n  ## 5) RSA keypair credential\n  ## 6) Environment variables credential\n  ## 7) Instance metadata credential\n\n  # access_key_id = \"\"\n  # access_key_secret = \"\"\n  # access_key_sts_token = \"\"\n  # role_arn = \"\"\n  # role_session_name = \"\"\n  # private_key = \"\"\n  # public_key_id = \"\"\n  # role_name = \"\"\n\n  ## Specify the ali cloud region list to be queried for metrics and objects discovery\n  ## If not set, all supported regions (see below) would be covered, it can provide a significant load on API, so the recommendation here\n  ## is to limit the list as much as possible. Allowed values: https://www.alibabacloud.com/help/zh/doc-detail/40654.htm\n  ## Default supported regions are:\n  ## 21 items: cn-qingdao,cn-beijing,cn-zhangjiakou,cn-huhehaote,cn-hangzhou,cn-shanghai,cn-shenzhen,\n  ##           cn-heyuan,cn-chengdu,cn-hongkong,ap-southeast-1,ap-southeast-2,ap-southeast-3,ap-southeast-5,\n  ##           ap-south-1,ap-northeast-1,us-west-1,us-east-1,eu-central-1,eu-west-1,me-east-1\n  ##\n  ## From discovery perspective it set the scope for object discovery, the discovered info can be used to enrich\n  ## the metrics with objects attributes/tags. Discovery is supported not for all projects (if not supported, then\n  ## it will be reported on the start - for example for 'acs_cdn' project:\n  ## 'E! [inputs.aliyuncms] Discovery tool is not activated: no discovery support for project \"acs_cdn\"' )\n  ## Currently, discovery supported for the following projects:\n  ## - acs_ecs_dashboard\n  ## - acs_rds_dashboard\n  ## - acs_slb_dashboard\n  ## - acs_vpc_eip\n  regions = [\"cn-hongkong\"]\n\n  # The minimum period for AliyunCMS metrics is 1 minute (60s). However not all\n  # metrics are made available to the 1 minute period. Some are collected at\n  # 3 minute, 5 minute, or larger intervals.\n  # See: https://help.aliyun.com/document_detail/51936.html?spm=a2c4g.11186623.2.18.2bc1750eeOw1Pv\n  # Note that if a period is configured that is smaller than the minimum for a\n  # particular metric, that metric will not be returned by the Aliyun OpenAPI\n  # and will not be collected by Telegraf.\n  #\n  ## Requested AliyunCMS aggregation Period (required - must be a multiple of 60s)\n  period = \"5m\"\n\n  ## Collection Delay (required - must account for metrics availability via AliyunCMS API)\n  delay = \"1m\"\n\n  ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"5m\"\n\n  ## Metric Statistic Project (required)\n  project = \"acs_slb_dashboard\"\n\n  ## Maximum requests per second, default value is 200\n  ratelimit = 200\n\n  ## How often the discovery API call executed (default 1m)\n  #discovery_interval = \"1m\"\n\n  ## Metrics to Pull (Required)\n  [[inputs.aliyuncms.metrics]]\n  ## Metrics names to be requested,\n  ## described here (per project): https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n  names = [\"InstanceActiveConnection\", \"InstanceNewConnection\"]\n\n  ## Dimension filters for Metric (these are optional).\n  ## This allows to get additional metric dimension. If dimension is not specified it can be returned or\n  ## the data can be aggregated - it depends on particular metric, you can find details here: https://help.aliyun.com/document_detail/28619.html?spm=a2c4g.11186623.6.690.1938ad41wg8QSq\n  ##\n  ## Note, that by default dimension filter includes the list of discovered objects in scope (if discovery is enabled)\n  ## Values specified here would be added into the list of discovered objects.\n  ## You can specify either single dimension:\n  #dimensions = '{\"instanceId\": \"p-example\"}'\n\n  ## Or you can specify several dimensions at once:\n  #dimensions = '[{\"instanceId\": \"p-example\"},{\"instanceId\": \"q-example\"}]'\n\n  ## Enrichment tags, can be added from discovery (if supported)\n  ## Notation is \u003cmeasurement_tag_name\u003e:\u003cJMES query path (https://jmespath.org/tutorial.html)\u003e\n  ## To figure out which fields are available, consult the Describe\u003cObjectType\u003e API per project.\n  ## For example, for SLB: https://api.aliyun.com/#/?product=Slb\u0026version=2014-05-15\u0026api=DescribeLoadBalancers\u0026params={}\u0026tab=MOCK\u0026lang=GO\n  #tag_query_path = [\n  #    \"address:Address\",\n  #    \"name:LoadBalancerName\",\n  #    \"cluster_owner:Tags.Tag[?TagKey=='cs.cluster.name'].TagValue | [0]\"\n  #    ]\n  ## The following tags added by default: regionId (if discovery enabled), userId, instanceId.\n\n  ## Allow metrics without discovery data, if discovery is enabled. If set to true, then metric without discovery\n  ## data would be emitted, otherwise dropped. This cane be of help, in case debugging dimension filters, or partial coverage\n  ## of discovery scope vs monitoring scope\n  #allow_dps_without_discovery = false\n"
	},
	{
		"Name": "amd_rocm_smi",
		"Description": " Query statistics from AMD Graphics cards using rocm-smi binary",
		"SampleConfig": "[[inputs.amd_rocm_smi]]\n  ## Optional: path to rocm-smi binary, defaults to $PATH via exec.LookPath\n  # bin_path = \"/opt/rocm/bin/rocm-smi\"\n\n  ## Optional: timeout for GPU polling\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "amqp_consumer",
		"Description": " AMQP consumer plugin",
		"SampleConfig": "[[inputs.amqp_consumer]]\n  ## Brokers to consume from.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Name of the exchange to declare.  If unset, no exchange will be declared.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n\n  ## AMQP queue name.\n  queue = \"telegraf\"\n\n  ## AMQP queue durability can be \"transient\" or \"durable\".\n  queue_durability = \"durable\"\n\n  ## If true, queue will be passively declared.\n  # queue_passive = false\n\n  ## A binding between the exchange and queue using this binding key is\n  ## created.  If unset, no binding is created.\n  binding_key = \"#\"\n\n  ## Maximum number of messages server should give to the worker.\n  # prefetch_count = 50\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "apache",
		"Description": " Read Apache status information (mod_status)",
		"SampleConfig": "[[inputs.apache]]\n  ## An array of URLs to gather from, must be directed at the machine\n  ## readable version of the mod_status page including the auto query string.\n  ## Default is \"http://localhost/server-status?auto\".\n  urls = [\"http://localhost/server-status?auto\"]\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "apcupsd",
		"Description": " Monitor APC UPSes connected to apcupsd",
		"SampleConfig": "[[inputs.apcupsd]]\n  # A list of running apcupsd server to connect to.\n  # If not provided will default to tcp://127.0.0.1:3551\n  servers = [\"tcp://127.0.0.1:3551\"]\n\n  ## Timeout for dialing server.\n  timeout = \"5s\"\n"
	},
	{
		"Name": "aurora",
		"Description": " Gather metrics from Apache Aurora schedulers",
		"SampleConfig": "[[inputs.aurora]]\n  ## Schedulers are the base addresses of your Aurora Schedulers\n  schedulers = [\"http://127.0.0.1:8081\"]\n\n  ## Set of role types to collect metrics from.\n  ##\n  ## The scheduler roles are checked each interval by contacting the\n  ## scheduler nodes; zookeeper is not contacted.\n  # roles = [\"leader\", \"follower\"]\n\n  ## Timeout is the max time for total network operations.\n  # timeout = \"5s\"\n\n  ## Username and password are sent using HTTP Basic Auth.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "azure_storage_queue",
		"Description": " Gather Azure Storage Queue metrics",
		"SampleConfig": "[[inputs.azure_storage_queue]]\n  ## Required Azure Storage Account name\n  account_name = \"mystorageaccount\"\n\n  ## Required Azure Storage Account access key\n  account_key = \"storageaccountaccesskey\"\n\n  ## Set to false to disable peeking age of oldest message (executes faster)\n  # peek_oldest_message_age = true\n"
	},
	{
		"Name": "bcache",
		"Description": " Read metrics of bcache from stats_total and dirty_data",
		"SampleConfig": "[[inputs.bcache]]\n  ## Bcache sets path\n  ## If not specified, then default is:\n  bcachePath = \"/sys/fs/bcache\"\n\n  ## By default, Telegraf gather stats for all bcache devices\n  ## Setting devices will restrict the stats to the specified\n  ## bcache devices.\n  bcacheDevs = [\"bcache0\"]\n"
	},
	{
		"Name": "beanstalkd",
		"Description": " Collects Beanstalkd server and tubes stats",
		"SampleConfig": "[[inputs.beanstalkd]]\n  ## Server to collect data from\n  server = \"localhost:11300\"\n\n  ## List of tubes to gather stats about.\n  ## If no tubes specified then data gathered for each tube on server reported by list-tubes command\n  tubes = [\"notifications\"]\n"
	},
	{
		"Name": "beat",
		"Description": " Read metrics exposed by Beat",
		"SampleConfig": "[[inputs.beat]]\n  ## An URL from which to read Beat-formatted JSON\n  ## Default is \"http://127.0.0.1:5066\".\n  url = \"http://127.0.0.1:5066\"\n\n  ## Enable collection of the listed stats\n  ## An empty list means collect all. Available options are currently\n  ## \"beat\", \"libbeat\", \"system\" and \"filebeat\".\n  # include = [\"beat\", \"libbeat\", \"filebeat\"]\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Override HTTP \"Host\" header\n  # host_header = \"logstash.example.com\"\n\n  ## Timeout for HTTP requests\n  # timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "bind",
		"Description": " Read BIND nameserver XML statistics",
		"SampleConfig": "[[inputs.bind]]\n  ## An array of BIND XML statistics URI to gather stats.\n  ## Default is \"http://localhost:8053/xml/v3\".\n  # urls = [\"http://localhost:8053/xml/v3\"]\n  # gather_memory_contexts = false\n  # gather_views = false\n\n  ## Timeout for http requests made by bind nameserver\n  # timeout = \"4s\"\n"
	},
	{
		"Name": "bond",
		"Description": " Collect bond interface status, slaves statuses and failures count",
		"SampleConfig": "[[inputs.bond]]\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n\n  ## Sets 'sys' directory path\n  ## If not specified, then default is /sys\n  # host_sys = \"/sys\"\n\n  ## By default, telegraf gather stats for all bond interfaces\n  ## Setting interfaces will restrict the stats to the specified\n  ## bond interfaces.\n  # bond_interfaces = [\"bond0\"]\n\n  ## Tries to collect additional bond details from /sys/class/net/{bond}\n  ## currently only useful for LACP (mode 4) bonds\n  # collect_sys_details = false\n"
	},
	{
		"Name": "burrow",
		"Description": " Collect Kafka topics and consumers status from Burrow HTTP API.",
		"SampleConfig": "[[inputs.burrow]]\n  ## Burrow API endpoints in format \"schema://host:port\".\n  ## Default is \"http://localhost:8000\".\n  servers = [\"http://localhost:8000\"]\n\n  ## Override Burrow API prefix.\n  ## Useful when Burrow is behind reverse-proxy.\n  # api_prefix = \"/v3/kafka\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Limit per-server concurrent connections.\n  ## Useful in case of large number of topics or consumer groups.\n  # concurrent_connections = 20\n\n  ## Filter clusters, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # clusters_include = []\n  # clusters_exclude = []\n\n  ## Filter consumer groups, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # groups_include = []\n  # groups_exclude = []\n\n  ## Filter topics, default is no filtering.\n  ## Values can be specified as glob patterns.\n  # topics_include = []\n  # topics_exclude = []\n\n  ## Credentials for basic HTTP authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional SSL config\n  # ssl_ca = \"/etc/telegraf/ca.pem\"\n  # ssl_cert = \"/etc/telegraf/cert.pem\"\n  # ssl_key = \"/etc/telegraf/key.pem\"\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "cassandra",
		"Description": " Read Cassandra metrics through Jolokia",
		"SampleConfig": "[[inputs.cassandra]]\n  ## DEPRECATED: The cassandra plugin has been deprecated.  Please use the\n  ## jolokia2 plugin instead.\n  ##\n  ## see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/jolokia2\n\n  context = \"/jolokia/read\"\n  ## List of cassandra servers exposing jolokia read service\n  servers = [\"myuser:mypassword@10.10.10.1:8778\",\"10.10.10.2:8778\",\":8778\"]\n  ## List of metrics collected on above servers\n  ## Each metric consists of a jmx path.\n  ## This will collect all heap memory usage metrics from the jvm and\n  ## ReadLatency metrics for all keyspaces and tables.\n  ## \"type=Table\" in the query works with Cassandra3.0. Older versions might\n  ## need to use \"type=ColumnFamily\"\n  metrics  = [\n    \"/java.lang:type=Memory/HeapMemoryUsage\",\n    \"/org.apache.cassandra.metrics:type=Table,keyspace=*,scope=*,name=ReadLatency\"\n  ]\n"
	},
	{
		"Name": "ceph",
		"Description": " Collects performance metrics from the MON, OSD, MDS and RGW nodes in a Ceph storage cluster.",
		"SampleConfig": "[[inputs.ceph]]\n  ## This is the recommended interval to poll.  Too frequent and you will lose\n  ## data points due to timeouts during rebalancing and recovery\n  interval = '1m'\n\n  ## All configuration values are optional, defaults are shown below\n\n  ## location of ceph binary\n  ceph_binary = \"/usr/bin/ceph\"\n\n  ## directory in which to look for socket files\n  socket_dir = \"/var/run/ceph\"\n\n  ## prefix of MON and OSD socket files, used to determine socket type\n  mon_prefix = \"ceph-mon\"\n  osd_prefix = \"ceph-osd\"\n  mds_prefix = \"ceph-mds\"\n  rgw_prefix = \"ceph-client\"\n\n  ## suffix used to identify socket files\n  socket_suffix = \"asok\"\n\n  ## Ceph user to authenticate as, ceph will search for the corresponding keyring\n  ## e.g. client.admin.keyring in /etc/ceph, or the explicit path defined in the\n  ## client section of ceph.conf for example:\n  ##\n  ##     [client.telegraf]\n  ##         keyring = /etc/ceph/client.telegraf.keyring\n  ##\n  ## Consult the ceph documentation for more detail on keyring generation.\n  ceph_user = \"client.admin\"\n\n  ## Ceph configuration to use to locate the cluster\n  ceph_config = \"/etc/ceph/ceph.conf\"\n\n  ## Whether to gather statistics via the admin socket\n  gather_admin_socket_stats = true\n\n  ## Whether to gather statistics via ceph commands, requires ceph_user and ceph_config\n  ## to be specified\n  gather_cluster_stats = false\n"
	},
	{
		"Name": "cgroup",
		"Description": " Read specific statistics per cgroup",
		"SampleConfig": "[[inputs.cgroup]]\n  ## Directories in which to look for files, globs are supported.\n  ## Consider restricting paths to the set of cgroups you really\n  ## want to monitor if you have a large number of cgroups, to avoid\n  ## any cardinality issues.\n  # paths = [\n  #   \"/sys/fs/cgroup/memory\",\n  #   \"/sys/fs/cgroup/memory/child1\",\n  #   \"/sys/fs/cgroup/memory/child2/*\",\n  # ]\n  ## cgroup stat fields, as file names, globs are supported.\n  ## these file names are appended to each path from above.\n  # files = [\"memory.*usage*\", \"memory.limit_in_bytes\"]\n"
	},
	{
		"Name": "chrony",
		"Description": " Get standard chrony metrics, requires chronyc executable.",
		"SampleConfig": "[[inputs.chrony]]\n  ## If true, chronyc tries to perform a DNS lookup for the time server.\n  # dns_lookup = false\n"
	},
	{
		"Name": "cisco_telemetry_mdt",
		"Description": " Cisco model-driven telemetry (MDT) input plugin for IOS XR, IOS XE and NX-OS platforms",
		"SampleConfig": "[[inputs.cisco_telemetry_mdt]]\n ## Telemetry transport can be \"tcp\" or \"grpc\".  TLS is only supported when\n ## using the grpc transport.\n transport = \"grpc\"\n\n ## Address and port to host telemetry listener\n service_address = \":57000\"\n\n ## Grpc Maximum Message Size, default is 4MB, increase the size.\n max_msg_size = 4000000\n\n ## Enable TLS; grpc transport only.\n # tls_cert = \"/etc/telegraf/cert.pem\"\n # tls_key = \"/etc/telegraf/key.pem\"\n\n ## Enable TLS client authentication and define allowed CA certificates; grpc\n ##  transport only.\n # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n ## Define (for certain nested telemetry measurements with embedded tags) which fields are tags\n # embedded_tags = [\"Cisco-IOS-XR-qos-ma-oper:qos/interface-table/interface/input/service-policy-names/service-policy-instance/statistics/class-stats/class-name\"]\n\n ## Define aliases to map telemetry encoding paths to simple measurement names\n [inputs.cisco_telemetry_mdt.aliases]\n   ifstats = \"ietf-interfaces:interfaces-state/interface/statistics\"\n ## Define Property Xformation, please refer README and https://pubhub.devnetcloud.com/media/dme-docs-9-3-3/docs/appendix/ for Model details.\n [inputs.cisco_telemetry_mdt.dmes]\n#    Global Property Xformation.\n#    prop1 = \"uint64 to int\"\n#    prop2 = \"uint64 to string\"\n#    prop3 = \"string to uint64\"\n#    prop4 = \"string to int64\"\n#    prop5 = \"string to float64\"\n#    auto-prop-xfrom = \"auto-float-xfrom\" #Xform any property which is string, and has float number to type float64\n#    Per Path property xformation, Name is telemetry configuration under sensor-group, path configuration \"WORD         Distinguished Name\"\n#    Per Path configuration is better as it avoid property collision issue of types.\n#    dnpath = '{\"Name\": \"show ip route summary\",\"prop\": [{\"Key\": \"routes\",\"Value\": \"string\"}, {\"Key\": \"best-paths\",\"Value\": \"string\"}]}'\n#    dnpath2 = '{\"Name\": \"show processes cpu\",\"prop\": [{\"Key\": \"kernel_percent\",\"Value\": \"float\"}, {\"Key\": \"idle_percent\",\"Value\": \"float\"}, {\"Key\": \"process\",\"Value\": \"string\"}, {\"Key\": \"user_percent\",\"Value\": \"float\"}, {\"Key\": \"onesec\",\"Value\": \"float\"}]}'\n#    dnpath3 = '{\"Name\": \"show processes memory physical\",\"prop\": [{\"Key\": \"processname\",\"Value\": \"string\"}]}'\n"
	},
	{
		"Name": "clickhouse",
		"Description": " Read metrics from one or many ClickHouse servers",
		"SampleConfig": "[[inputs.clickhouse]]\n  ## Username for authorization on ClickHouse server\n  ## example: username = \"default\"\n  username = \"default\"\n\n  ## Password for authorization on ClickHouse server\n  ## example: password = \"super_secret\"\n\n  ## HTTP(s) timeout while getting metrics values\n  ## The timeout includes connection time, any redirects, and reading the response body.\n  ##   example: timeout = 1s\n  # timeout = 5s\n\n  ## List of servers for metrics scraping\n  ## metrics scrape via HTTP(s) clickhouse interface\n  ## https://clickhouse.tech/docs/en/interfaces/http/\n  ##    example: servers = [\"http://127.0.0.1:8123\",\"https://custom-server.mdb.yandexcloud.net\"]\n  servers         = [\"http://127.0.0.1:8123\"]\n\n  ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers available in the cluster\n  ## with using same \"user:password\" described in \"user\" and \"password\" parameters\n  ## and get this server hostname list from \"system.clusters\" table\n  ## see\n  ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n  ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n  ##    example: auto_discovery = false\n  # auto_discovery = true\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n  ## please use only full cluster names here, regexp and glob filters is not allowed\n  ## for \"/etc/clickhouse-server/config.d/remote.xml\"\n  ## \u003cyandex\u003e\n  ##  \u003cremote_servers\u003e\n  ##    \u003cmy-own-cluster\u003e\n  ##        \u003cshard\u003e\n  ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n  ##          \u003creplica\u003e\u003chost\u003eclickhouse-ru-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n  ##        \u003c/shard\u003e\n  ##        \u003cshard\u003e\n  ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-1.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n  ##          \u003creplica\u003e\u003chost\u003eclickhouse-eu-2.local\u003c/host\u003e\u003cport\u003e9000\u003c/port\u003e\u003c/replica\u003e\n  ##        \u003c/shard\u003e\n  ##    \u003c/my-onw-cluster\u003e\n  ##  \u003c/remote_servers\u003e\n  ##\n  ## \u003c/yandex\u003e\n  ##\n  ## example: cluster_include = [\"my-own-cluster\"]\n  # cluster_include = []\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  ## when this filter present then \"WHERE cluster NOT IN (...)\" filter will apply\n  ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n  # cluster_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "cloud_pubsub",
		"Description": " Read metrics from Google PubSub",
		"SampleConfig": "[[inputs.cloud_pubsub]]\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub subscription.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub subscription to ingest metrics from.\n  subscription = \"my-subscription\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. Number of seconds to wait before attempting to restart the\n  ## PubSub subscription receiver after an unexpected error.\n  ## If the streaming pull for a PubSub Subscription fails (receiver),\n  ## the agent attempts to restart receiving messages after this many seconds.\n  # retry_delay_seconds = 5\n\n  ## Optional. Maximum byte length of a message to consume.\n  ## Larger messages are dropped with an error. If less than 0 or unspecified,\n  ## treated as no limit.\n  # max_message_len = 1000000\n\n  ## Optional. Maximum messages to read from PubSub that have not been written\n  ## to an output. Defaults to %d.\n  ## For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message contains 10 metrics and the output\n  ## metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## The following are optional Subscription ReceiveSettings in PubSub.\n  ## Read more about these values:\n  ## https://godoc.org/cloud.google.com/go/pubsub#ReceiveSettings\n\n  ## Optional. Maximum number of seconds for which a PubSub subscription\n  ## should auto-extend the PubSub ACK deadline for each message. If less than\n  ## 0, auto-extension is disabled.\n  # max_extension = 0\n\n  ## Optional. Maximum number of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_messages = 0\n\n  ## Optional. Maximum size in bytes of unprocessed messages in PubSub\n  ## (unacknowledged but not yet expired in PubSub).\n  ## A value of 0 is treated as the default PubSub value.\n  ## Negative values will be treated as unlimited.\n  # max_outstanding_bytes = 0\n\n  ## Optional. Max number of goroutines a PubSub Subscription receiver can spawn\n  ## to pull messages from PubSub concurrently. This limit applies to each\n  ## subscription separately and is treated as the PubSub default if less than\n  ## 1. Note this setting does not limit the number of messages that can be\n  ## processed concurrently (use \"max_outstanding_messages\" instead).\n  # max_receiver_go_routines = 0\n\n  ## Optional. If true, Telegraf will attempt to base64 decode the\n  ## PubSub message data before parsing. Many GCP services that\n  ## output JSON to Google PubSub base64-encode the JSON payload.\n  # base64_data = false\n"
	},
	{
		"Name": "cloud_pubsub_push",
		"Description": " Google Cloud Pub/Sub Push HTTP listener",
		"SampleConfig": "[[inputs.cloud_pubsub_push]]\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Application secret to verify messages originate from Cloud Pub/Sub\n  # token = \"\"\n\n  ## Path to listen to.\n  # path = \"/\"\n\n  ## Maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## Maximum duration before timing out write of the response. This should be set to a value\n  ## large enough that you can send at least 'metric_batch_size' number of messages within the\n  ## duration.\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,00 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Whether to add the pubsub metadata, such as message attributes and subscription as a tag.\n  # add_meta = false\n\n  ## Optional. Maximum messages to read from PubSub that have not been written\n  ## to an output. Defaults to 1000.\n  ## For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message contains 10 metrics and the output\n  ## metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "cloudwatch",
		"Description": " Pull Metric Statistics from Amazon CloudWatch",
		"SampleConfig": "[[inputs.cloudwatch]]\n  ## Amazon Region\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # web_identity_token_file = \"\"\n  # role_session_name = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n  # http_proxy_url = \"http://localhost:8888\"\n\n  # The minimum period for Cloudwatch metrics is 1 minute (60s). However not all\n  # metrics are made available to the 1 minute period. Some are collected at\n  # 3 minute, 5 minute, or larger intervals. See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n  # Note that if a period is configured that is smaller than the minimum for a\n  # particular metric, that metric will not be returned by the Cloudwatch API\n  # and will not be collected by Telegraf.\n  #\n  ## Requested CloudWatch aggregation Period (required - must be a multiple of 60s)\n  period = \"5m\"\n\n  ## Collection Delay (required - must account for metrics availability via CloudWatch API)\n  delay = \"5m\"\n\n  ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"5m\"\n\n  ## Recommended if \"delay\" and \"period\" are both within 3 hours of request time. Invalid values will be ignored.\n  ## Recently Active feature will only poll for CloudWatch ListMetrics values that occurred within the last 3 Hours.\n  ## If enabled, it will reduce total API usage of the CloudWatch ListMetrics API and require less memory to retain.\n  ## Do not enable if \"period\" or \"delay\" is longer than 3 hours, as it will not return data more than 3 hours old.\n  ## See https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_ListMetrics.html\n  #recently_active = \"PT3H\"\n\n  ## Configure the TTL for the internal cache of metrics.\n  # cache_ttl = \"1h\"\n\n  ## Metric Statistic Namespaces (required)\n  namespaces = [\"AWS/ELB\"]\n  # A single metric statistic namespace that will be appended to namespaces on startup\n  # namespace = \"AWS/ELB\"\n\n  ## Maximum requests per second. Note that the global default AWS rate limit is\n  ## 50 reqs/sec, so if you define multiple namespaces, these should add up to a\n  ## maximum of 50.\n  ## See http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_limits.html\n  # ratelimit = 25\n\n  ## Timeout for http requests made by the cloudwatch client.\n  # timeout = \"5s\"\n\n  ## Namespace-wide statistic filters. These allow fewer queries to be made to\n  ## cloudwatch.\n  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n  # statistic_exclude = []\n\n  ## Metrics to Pull\n  ## Defaults to all Metrics in Namespace if nothing is provided\n  ## Refreshes Namespace available metrics every 1h\n  #[[inputs.cloudwatch.metrics]]\n  #  names = [\"Latency\", \"RequestCount\"]\n  #\n  #  ## Statistic filters for Metric.  These allow for retrieving specific\n  #  ## statistics for an individual metric.\n  #  # statistic_include = [ \"average\", \"sum\", \"minimum\", \"maximum\", sample_count\" ]\n  #  # statistic_exclude = []\n  #\n  #  ## Dimension filters for Metric.  All dimensions defined for the metric names\n  #  ## must be specified in order to retrieve the metric statistics.\n  #  ## 'value' has wildcard / 'glob' matching support such as 'p-*'.\n  #  [[inputs.cloudwatch.metrics.dimensions]]\n  #    name = \"LoadBalancerName\"\n  #    value = \"p-example\"\n"
	},
	{
		"Name": "conntrack",
		"Description": " Collects conntrack stats from the configured directories and files.",
		"SampleConfig": "[[inputs.conntrack]]\n  ## The following defaults would work with multiple versions of conntrack.\n  ## Note the nf_ and ip_ filename prefixes are mutually exclusive across\n  ## kernel versions, as are the directory locations.\n\n  ## Superset of filenames to look for within the conntrack dirs.\n  ## Missing files will be ignored.\n  files = [\"ip_conntrack_count\",\"ip_conntrack_max\",\n          \"nf_conntrack_count\",\"nf_conntrack_max\"]\n\n  ## Directories to search within for the conntrack files above.\n  ## Missing directories will be ignored.\n  dirs = [\"/proc/sys/net/ipv4/netfilter\",\"/proc/sys/net/netfilter\"]\n"
	},
	{
		"Name": "consul",
		"Description": " Gather health check statuses from services registered in Consul",
		"SampleConfig": "[[inputs.consul]]\n  ## Consul server address\n  # address = \"localhost:8500\"\n\n  ## URI scheme for the Consul server, one of \"http\", \"https\"\n  # scheme = \"http\"\n\n  ## Metric version controls the mapping from Consul metrics into\n  ## Telegraf metrics. Version 2 moved all fields with string values\n  ## to tags.\n  ##\n  ##   example: metric_version = 1; deprecated in 1.16\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n  ## ACL token used in every request\n  # token = \"\"\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Data center to query the health checks from\n  # datacenter = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n  ## Consul checks' tag splitting\n  # When tags are formatted like \"key:value\" with \":\" as a delimiter then\n  # they will be splitted and reported as proper key:value in Telegraf\n  # tag_delimiter = \":\"\n"
	},
	{
		"Name": "consul_agent",
		"Description": " Read metrics from the Consul Agent API",
		"SampleConfig": "[[inputs.consul_agent]]\n  ## URL for the Consul agent\n  # url = \"http://127.0.0.1:8500\"\n\n  ## Use auth token for authorization.\n  ## If both are set, an error is thrown.\n  ## If both are empty, no token will be used.\n  # token_file = \"/path/to/auth/token\"\n  ## OR\n  # token = \"a1234567-40c7-9048-7bae-378687048181\"\n\n  ## Set timeout (default 5 seconds)\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n"
	},
	{
		"Name": "couchbase",
		"Description": " Read per-node and per-bucket metrics from Couchbase",
		"SampleConfig": "[[inputs.couchbase]]\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    http://couchbase-0.example.com/\n  ##    http://admin:secret@couchbase-0.example.com:8091/\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no protocol is specified, HTTP is used.\n  ## If no port is specified, 8091 is used.\n  servers = [\"http://localhost:8091\"]\n\n  ## Filter bucket fields to include only here.\n  # bucket_stats_included = [\"quota_percent_used\", \"ops_per_sec\", \"disk_fetches\", \"item_count\", \"disk_used\", \"data_used\", \"mem_used\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification (defaults to false)\n  ## If set to false, tls_cert and tls_key are required\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "couchdb",
		"Description": " Read CouchDB Stats from one or more servers",
		"SampleConfig": "[[inputs.couchdb]]\n  ## Works with CouchDB stats endpoints out of the box\n  ## Multiple Hosts from which to read CouchDB stats:\n  hosts = [\"http://localhost:8086/_stats\"]\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"telegraf\"\n  # basic_password = \"p@ssw0rd\"\n"
	},
	{
		"Name": "cpu",
		"Description": " Read metrics about cpu usage",
		"SampleConfig": "[[inputs.cpu]]\n  ## Whether to report per-cpu stats or not\n  percpu = true\n  ## Whether to report total system cpu stats or not\n  totalcpu = true\n  ## If true, collect raw CPU time metrics\n  collect_cpu_time = false\n  ## If true, compute and report the sum of all non-idle CPU states\n  report_active = false\n  ## If true and the info is available then add core_id and physical_id tags\n  core_tags = false\n"
	},
	{
		"Name": "csgo",
		"Description": " Fetch metrics from a CSGO SRCDS",
		"SampleConfig": "[[inputs.csgo]]\n  ## Specify servers using the following format:\n  ##    servers = [\n  ##      [\"ip1:port1\", \"rcon_password1\"],\n  ##      [\"ip2:port2\", \"rcon_password2\"],\n  ##    ]\n  #\n  ## If no servers are specified, no data will be collected\n  servers = []\n"
	},
	{
		"Name": "dcos",
		"Description": " Input plugin for DC/OS metrics",
		"SampleConfig": "[[inputs.dcos]]\n  ## The DC/OS cluster URL.\n  cluster_url = \"https://dcos-master-1\"\n\n  ## The ID of the service account.\n  service_account_id = \"telegraf\"\n  ## The private key file for the service account.\n  service_account_private_key = \"/etc/telegraf/telegraf-sa-key.pem\"\n\n  ## Path containing login token.  If set, will read on every gather.\n  # token_file = \"/home/dcos/.dcos/token\"\n\n  ## In all filter options if both include and exclude are empty all items\n  ## will be collected.  Arrays may contain glob patterns.\n  ##\n  ## Node IDs to collect metrics from.  If a node is excluded, no metrics will\n  ## be collected for its containers or apps.\n  # node_include = []\n  # node_exclude = []\n  ## Container IDs to collect container metrics from.\n  # container_include = []\n  # container_exclude = []\n  ## Container IDs to collect app metrics from.\n  # app_include = []\n  # app_exclude = []\n\n  ## Maximum concurrent connections to the cluster.\n  # max_connections = 10\n  ## Maximum time to receive a response from cluster.\n  # response_timeout = \"20s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n\n  ## Recommended filtering to reduce series cardinality.\n  # [inputs.dcos.tagdrop]\n  #   path = [\"/var/lib/mesos/slave/slaves/*\"]\n"
	},
	{
		"Name": "directory_monitor",
		"Description": " Ingests files in a directory and then moves them to a target directory.",
		"SampleConfig": "[[inputs.directory_monitor]]\n  ## The directory to monitor and read files from.\n  directory = \"\"\n  #\n  ## The directory to move finished files to.\n  finished_directory = \"\"\n  #\n  ## The directory to move files to upon file error.\n  ## If not provided, erroring files will stay in the monitored directory.\n  # error_directory = \"\"\n  #\n  ## The amount of time a file is allowed to sit in the directory before it is picked up.\n  ## This time can generally be low but if you choose to have a very large file written to the directory and it's potentially slow,\n  ## set this higher so that the plugin will wait until the file is fully copied to the directory.\n  # directory_duration_threshold = \"50ms\"\n  #\n  ## A list of the only file names to monitor, if necessary. Supports regex. If left blank, all files are ingested.\n  # files_to_monitor = [\"^.*\\.csv\"]\n  #\n  ## A list of files to ignore, if necessary. Supports regex.\n  # files_to_ignore = [\".DS_Store\"]\n  #\n  ## Maximum lines of the file to process that have not yet be written by the\n  ## output. For best throughput set to the size of the output's metric_buffer_limit.\n  ## Warning: setting this number higher than the output's metric_buffer_limit can cause dropped metrics.\n  # max_buffered_metrics = 10000\n  #\n  ## The maximum amount of file paths to queue up for processing at once, before waiting until files are processed to find more files.\n  ## Lowering this value will result in *slightly* less memory use, with a potential sacrifice in speed efficiency, if absolutely necessary.\n  # file_queue_size = 100000\n  #\n  ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n  ## to disable. Cautious when file name variation is high, this can increase the cardinality\n  ## significantly. Read more about cardinality here:\n  ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n  # file_tag = \"\"\n  #\n  ## The dataformat to be read from the files.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  ## NOTE: We currently only support parsing newline-delimited JSON. See the format here: https://github.com/ndjson/ndjson-spec\n  data_format = \"influx\"\n"
	},
	{
		"Name": "disk",
		"Description": " Read metrics about disk usage by mount point",
		"SampleConfig": "[[inputs.disk]]\n  ## By default stats will be gathered for all mount points.\n  ## Set mount_points will restrict the stats to only the specified mount points.\n  # mount_points = [\"/\"]\n\n  ## Ignore mount points by filesystem type.\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n  ## Ignore mount points by mount options.\n  ## The 'mount' command reports options of all mounts in parathesis.\n  ## Bind mounts can be ignored with the special 'bind' option.\n  # ignore_mount_opts = []\n"
	},
	{
		"Name": "diskio",
		"Description": " Read metrics about disk IO by device",
		"SampleConfig": "[[inputs.diskio]]\n  ## By default, telegraf will gather stats for all devices including\n  ## disk partitions.\n  ## Setting devices will restrict the stats to the specified devices.\n  # devices = [\"sda\", \"sdb\", \"vd*\"]\n  ## Uncomment the following line if you need disk serial numbers.\n  # skip_serial_number = false\n  #\n  ## On systems which support it, device metadata can be added in the form of\n  ## tags.\n  ## Currently only Linux is supported via udev properties. You can view\n  ## available properties for a device by running:\n  ## 'udevadm info -q property -n /dev/sda'\n  ## Note: Most, but not all, udev properties can be accessed this way. Properties\n  ## that are currently inaccessible include DEVTYPE, DEVNAME, and DEVPATH.\n  # device_tags = [\"ID_FS_TYPE\", \"ID_FS_USAGE\"]\n  #\n  ## Using the same metadata source as device_tags, you can also customize the\n  ## name of the device via templates.\n  ## The 'name_templates' parameter is a list of templates to try and apply to\n  ## the device. The template may contain variables in the form of '$PROPERTY' or\n  ## '${PROPERTY}'. The first template which does not contain any variables not\n  ## present for the device is used as the device name tag.\n  ## The typical use case is for LVM volumes, to get the VG/LV name instead of\n  ## the near-meaningless DM-0 name.\n  # name_templates = [\"$ID_FS_LABEL\",\"$DM_VG_NAME/$DM_LV_NAME\"]\n"
	},
	{
		"Name": "disque",
		"Description": " Read metrics from one or many disque servers",
		"SampleConfig": "[[inputs.disque]]\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port and password.\n  ## ie disque://localhost, disque://10.10.3.33:18832, 10.0.0.1:10000, etc.\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost\"]\n"
	},
	{
		"Name": "dmcache",
		"Description": " Provide a native collection for dmsetup based statistics for dm-cache",
		"SampleConfig": "[[inputs.dmcache]]\n  ## Whether to report per-device stats or not\n  per_device = true\n"
	},
	{
		"Name": "dns_query",
		"Description": " Query given DNS server and gives statistics",
		"SampleConfig": "[[inputs.dns_query]]\n  ## servers to query\n  servers = [\"8.8.8.8\"]\n\n  ## Network is the network protocol name.\n  # network = \"udp\"\n\n  ## Domains or subdomains to query.\n  # domains = [\".\"]\n\n  ## Query record type.\n  ## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n  # record_type = \"A\"\n\n  ## Dns server port.\n  # port = 53\n\n  ## Query timeout in seconds.\n  # timeout = 2\n"
	},
	{
		"Name": "docker",
		"Description": " Read metrics about docker containers",
		"SampleConfig": "[[inputs.docker]]\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  endpoint = \"unix:///var/run/docker.sock\"\n\n  ## Set to true to collect Swarm metrics(desired_replicas, running_replicas)\n  ## Note: configure this in one of the manager nodes in a Swarm cluster.\n  ## configuring in multiple Swarm managers results in duplication of metrics.\n  gather_services = false\n\n  ## Only collect metrics for these containers. Values will be appended to\n  ## container_name_include.\n  ## Deprecated (1.4.0), use container_name_include\n  container_names = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Containers to include and exclude. Collect all if empty. Globs accepted.\n  container_name_include = []\n  container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  ## example: container_state_include = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  ## example: container_state_exclude = [\"created\", \"restarting\", \"running\", \"removing\", \"paused\", \"exited\", \"dead\"]\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## Timeout for docker list, info, and stats commands\n  timeout = \"5s\"\n\n  ## Whether to report for each container per-device blkio (8:0, 8:1...),\n  ## network (eth0, eth1, ...) and cpu (cpu0, cpu1, ...) stats or not.\n  ## Usage of this setting is discouraged since it will be deprecated in favor of 'perdevice_include'.\n  ## Default value is 'true' for backwards compatibility, please set it to 'false' so that 'perdevice_include' setting\n  ## is honored.\n  perdevice = true\n\n  ## Specifies for which classes a per-device metric should be issued\n  ## Possible values are 'cpu' (cpu0, cpu1, ...), 'blkio' (8:0, 8:1, ...) and 'network' (eth0, eth1, ...)\n  ## Please note that this setting has no effect if 'perdevice' is set to 'true'\n  # perdevice_include = [\"cpu\"]\n\n  ## Whether to report for each container total blkio and network stats or not.\n  ## Usage of this setting is discouraged since it will be deprecated in favor of 'total_include'.\n  ## Default value is 'false' for backwards compatibility, please set it to 'true' so that 'total_include' setting\n  ## is honored.\n  total = false\n\n  ## Specifies for which classes a total metric should be issued. Total is an aggregated of the 'perdevice' values.\n  ## Possible values are 'cpu', 'blkio' and 'network'\n  ## Total 'cpu' is reported directly by Docker daemon, and 'network' and 'blkio' totals are aggregated by this plugin.\n  ## Please note that this setting has no effect if 'total' is set to 'false'\n  # total_include = [\"cpu\", \"blkio\", \"network\"]\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  docker_label_include = []\n  docker_label_exclude = []\n\n  ## Which environment variables should we use as a tag\n  tag_env = [\"JAVA_HOME\", \"HEAP_SIZE\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "docker_log",
		"Description": " Read logging output from the Docker engine",
		"SampleConfig": "[[inputs.docker_log]]\n  ## Docker Endpoint\n  ##   To use TCP, set endpoint = \"tcp://[ip]:[port]\"\n  ##   To use environment variables (ie, docker-machine), set endpoint = \"ENV\"\n  # endpoint = \"unix:///var/run/docker.sock\"\n\n  ## When true, container logs are read from the beginning; otherwise\n  ## reading begins at the end of the log.\n  # from_beginning = false\n\n  ## Timeout for Docker API calls.\n  # timeout = \"5s\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"running\" state will be captured.\n  # container_state_include = []\n  # container_state_exclude = []\n\n  ## docker labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  # docker_label_include = []\n  # docker_label_exclude = []\n\n  ## Set the source tag for the metrics to the container ID hostname, eg first 12 chars\n  source_tag = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "dovecot",
		"Description": " Read metrics about dovecot servers",
		"SampleConfig": "[[inputs.dovecot]]\n  ## specify dovecot servers via an address:port list\n  ##  e.g.\n  ##    localhost:24242\n  ## or as an UDS socket\n  ##  e.g.\n  ##    /var/run/dovecot/old-stats\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"localhost:24242\"]\n\n  ## Type is one of \"user\", \"domain\", \"ip\", or \"global\"\n  type = \"global\"\n\n  ## Wildcard matches like \"*.com\". An empty string \"\" is same as \"*\"\n  ## If type = \"ip\" filters should be \u003cIP/network\u003e\n  filters = [\"\"]\n"
	},
	{
		"Name": "dpdk",
		"Description": " Reads metrics from DPDK applications using v2 telemetry interface.",
		"SampleConfig": "[[inputs.dpdk]]\n  ## Path to DPDK telemetry socket. This shall point to v2 version of DPDK telemetry interface.\n  # socket_path = \"/var/run/dpdk/rte/dpdk_telemetry.v2\"\n\n  ## Duration that defines how long the connected socket client will wait for a response before terminating connection.\n  ## This includes both writing to and reading from socket. Since it's local socket access\n  ## to a fast packet processing application, the timeout should be sufficient for most users.\n  ## Setting the value to 0 disables the timeout (not recommended)\n  # socket_access_timeout = \"200ms\"\n\n  ## Enables telemetry data collection for selected device types.\n  ## Adding \"ethdev\" enables collection of telemetry from DPDK NICs (stats, xstats, link_status).\n  ## Adding \"rawdev\" enables collection of telemetry from DPDK Raw Devices (xstats).\n  # device_types = [\"ethdev\"]\n\n  ## List of custom, application-specific telemetry commands to query\n  ## The list of available commands depend on the application deployed. Applications can register their own commands\n  ##   via telemetry library API http://doc.dpdk.org/guides/prog_guide/telemetry_lib.html#registering-commands\n  ## For e.g. L3 Forwarding with Power Management Sample Application this could be:\n  ##   additional_commands = [\"/l3fwd-power/stats\"]\n  # additional_commands = []\n\n  ## Allows turning off collecting data for individual \"ethdev\" commands.\n  ## Remove \"/ethdev/link_status\" from list to start getting link status metrics.\n  [inputs.dpdk.ethdev]\n    exclude_commands = [\"/ethdev/link_status\"]\n\n  ## When running multiple instances of the plugin it's recommended to add a unique tag to each instance to identify\n  ## metrics exposed by an instance of DPDK application. This is useful when multiple DPDK apps run on a single host.\n  ##  [inputs.dpdk.tags]\n  ##    dpdk_instance = \"my-fwd-app\"\n"
	},
	{
		"Name": "ecs",
		"Description": " Read metrics about ECS containers",
		"SampleConfig": "[[inputs.ecs]]\n  ## ECS metadata url.\n  ## Metadata v2 API is used if set explicitly. Otherwise,\n  ## v3 metadata endpoint API is used if available.\n  # endpoint_url = \"\"\n\n  ## Containers to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all containers\n  # container_name_include = []\n  # container_name_exclude = []\n\n  ## Container states to include and exclude. Globs accepted.\n  ## When empty only containers in the \"RUNNING\" state will be captured.\n  ## Possible values are \"NONE\", \"PULLED\", \"CREATED\", \"RUNNING\",\n  ## \"RESOURCES_PROVISIONED\", \"STOPPED\".\n  # container_status_include = []\n  # container_status_exclude = []\n\n  ## ecs labels to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all labels as tags\n  ecs_label_include = [ \"com.amazonaws.ecs.*\" ]\n  ecs_label_exclude = []\n\n  ## Timeout for queries.\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "elasticsearch",
		"Description": " Read stats from one or more Elasticsearch servers or clusters",
		"SampleConfig": "[[inputs.elasticsearch]]\n  ## specify a list of one or more Elasticsearch servers\n  ## you can add username and password to your url to use basic authentication:\n  ## servers = [\"http://user:pass@localhost:9200\"]\n  servers = [\"http://localhost:9200\"]\n\n  ## Timeout for HTTP requests to the elastic search server(s)\n  http_timeout = \"5s\"\n\n  ## When local is true (the default), the node will read only its own stats.\n  ## Set local to false when you want to read the node stats from all nodes\n  ## of the cluster.\n  local = true\n\n  ## Set cluster_health to true when you want to obtain cluster health stats\n  cluster_health = false\n\n  ## Adjust cluster_health_level when you want to obtain detailed health stats\n  ## The options are\n  ##  - indices (default)\n  ##  - cluster\n  # cluster_health_level = \"indices\"\n\n  ## Set cluster_stats to true when you want to obtain cluster stats.\n  cluster_stats = false\n\n  ## Only gather cluster_stats from the master node. To work this require local = true\n  cluster_stats_only_from_master = true\n\n  ## Indices to collect; can be one or more indices names or _all\n  ## Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.\n  indices_include = [\"_all\"]\n\n  ## One of \"shards\", \"cluster\", \"indices\"\n  ## Currently only \"shards\" is implemented\n  indices_level = \"shards\"\n\n  ## node_stats is a list of sub-stats that you want to have gathered. Valid options\n  ## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n  ## \"breaker\". Per default, all stats are gathered.\n  # node_stats = [\"jvm\", \"http\"]\n\n  ## HTTP Basic Authentication username and password.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Sets the number of most recent indices to return for indices that are configured with a date-stamped suffix.\n  ## Each 'indices_include' entry ending with a wildcard (*) or glob matching pattern will group together all indices that match it, and \n  ## sort them by the date or number after the wildcard. Metrics then are gathered for only the 'num_most_recent_indices' amount of most \n  ## recent indices.\n  # num_most_recent_indices = 0\n"
	},
	{
		"Name": "elasticsearch_query",
		"Description": " Derive metrics from aggregating Elasticsearch query results",
		"SampleConfig": "[[inputs.elasticsearch_query]]\n  ## The full HTTP endpoint URL for your Elasticsearch instance\n  ## Multiple urls can be specified as part of the same cluster,\n  ## this means that only ONE of the urls will be written to each interval.\n  urls = [ \"http://node1.es.example.com:9200\" ] # required.\n\n  ## Elasticsearch client timeout, defaults to \"5s\".\n  # timeout = \"5s\"\n\n  ## Set to true to ask Elasticsearch a list of all cluster nodes,\n  ## thus it is not necessary to list all nodes in the urls config option\n  # enable_sniffer = false\n\n  ## Set the interval to check if the Elasticsearch nodes are available\n  ## This option is only used if enable_sniffer is also set (0s to disable it)\n  # health_check_interval = \"10s\"\n\n  ## HTTP basic authentication details (eg. when using x-pack)\n  # username = \"telegraf\"\n  # password = \"mypassword\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  [[inputs.elasticsearch_query.aggregation]]\n    ## measurement name for the results of the aggregation query\n    measurement_name = \"measurement\"\n\n    ## Elasticsearch indexes to query (accept wildcards).\n    index = \"index-*\"\n\n    ## The date/time field in the Elasticsearch index (mandatory).\n    date_field = \"@timestamp\"\n\n    ## If the field used for the date/time field in Elasticsearch is also using\n    ## a custom date/time format it may be required to provide the format to\n    ## correctly parse the field.\n    ##\n    ## If using one of the built in elasticsearch formats this is not required.\n    # date_field_custom_format = \"\"\n\n    ## Time window to query (eg. \"1m\" to query documents from last minute).\n    ## Normally should be set to same as collection interval\n    query_period = \"1m\"\n\n    ## Lucene query to filter results\n    # filter_query = \"*\"\n\n    ## Fields to aggregate values (must be numeric fields)\n    # metric_fields = [\"metric\"]\n\n    ## Aggregation function to use on the metric fields\n    ## Must be set if 'metric_fields' is set\n    ## Valid values are: avg, sum, min, max, sum\n    # metric_function = \"avg\"\n\n    ## Fields to be used as tags\n    ## Must be text, non-analyzed fields. Metric aggregations are performed per tag\n    # tags = [\"field.keyword\", \"field2.keyword\"]\n\n    ## Set to true to not ignore documents when the tag(s) above are missing\n    # include_missing_tag = false\n\n    ## String value of the tag when the tag does not exist\n    ## Used when include_missing_tag is true\n    # missing_tag_value = \"null\"\n"
	},
	{
		"Name": "ethtool",
		"Description": " Returns ethtool statistics for given interfaces",
		"SampleConfig": "[[inputs.ethtool]]\n  ## List of interfaces to pull metrics for\n  # interface_include = [\"eth0\"]\n\n  ## List of interfaces to ignore when pulling metrics.\n  # interface_exclude = [\"eth1\"]\n\n  ## Some drivers declare statistics with extra whitespace, different spacing,\n  ## and mix cases. This list, when enabled, can be used to clean the keys.\n  ## Here are the current possible normalizations:\n  ##  * snakecase: converts fooBarBaz to foo_bar_baz\n  ##  * trim: removes leading and trailing whitespace\n  ##  * lower: changes all capitalized letters to lowercase\n  ##  * underscore: replaces spaces with underscores\n  # normalize_keys = [\"snakecase\", \"trim\", \"lower\", \"underscore\"]\n"
	},
	{
		"Name": "eventhub_consumer",
		"Description": " Azure Event Hubs service input plugin",
		"SampleConfig": "[[inputs.eventhub_consumer]]\n  ## The default behavior is to create a new Event Hub client from environment variables.\n  ## This requires one of the following sets of environment variables to be set:\n  ##\n  ## 1) Expected Environment Variables:\n  ##    - \"EVENTHUB_CONNECTION_STRING\"\n  ##\n  ## 2) Expected Environment Variables:\n  ##    - \"EVENTHUB_NAMESPACE\"\n  ##    - \"EVENTHUB_NAME\"\n  ##    - \"EVENTHUB_KEY_NAME\"\n  ##    - \"EVENTHUB_KEY_VALUE\"\n\n  ## 3) Expected Environment Variables:\n  ##    - \"EVENTHUB_NAMESPACE\"\n  ##    - \"EVENTHUB_NAME\"\n  ##    - \"AZURE_TENANT_ID\"\n  ##    - \"AZURE_CLIENT_ID\"\n  ##    - \"AZURE_CLIENT_SECRET\"\n\n  ## Uncommenting the option below will create an Event Hub client based solely on the connection string.\n  ## This can either be the associated environment variable or hard coded directly.\n  ## If this option is uncommented, environment variables will be ignored.\n  ## Connection string should contain EventHubName (EntityPath)\n  # connection_string = \"\"\n\n  ## Set persistence directory to a valid folder to use a file persister instead of an in-memory persister\n  # persistence_dir = \"\"\n\n  ## Change the default consumer group\n  # consumer_group = \"\"\n\n  ## By default the event hub receives all messages present on the broker, alternative modes can be set below.\n  ## The timestamp should be in https://github.com/toml-lang/toml#offset-date-time format (RFC 3339).\n  ## The 3 options below only apply if no valid persister is read from memory or file (e.g. first run).\n  # from_timestamp =\n  # latest = true\n\n  ## Set a custom prefetch count for the receiver(s)\n  # prefetch_count = 1000\n\n  ## Add an epoch to the receiver(s)\n  # epoch = 0\n\n  ## Change to set a custom user agent, \"telegraf\" is used by default\n  # user_agent = \"telegraf\"\n\n  ## To consume from a specific partition, set the partition_ids option.\n  ## An empty array will result in receiving from all partitions.\n  # partition_ids = [\"0\",\"1\"]\n\n  ## Max undelivered messages\n  # max_undelivered_messages = 1000\n\n  ## Set either option below to true to use a system property as timestamp.\n  ## You have the choice between EnqueuedTime and IoTHubEnqueuedTime.\n  ## It is recommended to use this setting when the data itself has no timestamp.\n  # enqueued_time_as_ts = true\n  # iot_hub_enqueued_time_as_ts = true\n\n  ## Tags or fields to create from keys present in the application property bag.\n  ## These could for example be set by message enrichments in Azure IoT Hub.\n  # application_property_tags = []\n  # application_property_fields = []\n\n  ## Tag or field name to use for metadata\n  ## By default all metadata is disabled\n  # sequence_number_field = \"SequenceNumber\"\n  # enqueued_time_field = \"EnqueuedTime\"\n  # offset_field = \"Offset\"\n  # partition_id_tag = \"PartitionID\"\n  # partition_key_tag = \"PartitionKey\"\n  # iot_hub_device_connection_id_tag = \"IoTHubDeviceConnectionID\"\n  # iot_hub_auth_generation_id_tag = \"IoTHubAuthGenerationID\"\n  # iot_hub_connection_auth_method_tag = \"IoTHubConnectionAuthMethod\"\n  # iot_hub_connection_module_id_tag = \"IoTHubConnectionModuleID\"\n  # iot_hub_enqueued_time_field = \"IoTHubEnqueuedTime\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "example",
		"Description": " This is an example plugin",
		"SampleConfig": "[[inputs.example]]\n  example_option = \"example_value\"\n"
	},
	{
		"Name": "exec",
		"Description": " Read metrics from one or more commands that can output to stdout",
		"SampleConfig": "[[inputs.exec]]\n  ## Commands array\n  commands = [\n    \"/tmp/test.sh\",\n    \"/usr/bin/mycollector --foo=bar\",\n    \"/tmp/collect_*.sh\"\n  ]\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Timeout for each command to complete.\n  timeout = \"5s\"\n\n  ## measurement name suffix (for separating different commands)\n  name_suffix = \"_mycollector\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "execd",
		"Description": " Run executable as long-running input plugin",
		"SampleConfig": "[[inputs.execd]]\n  ## One program to run as daemon.\n  ## NOTE: process and each argument should each be their own string\n  command = [\"telegraf-smartctl\", \"-d\", \"/dev/sda\"]\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Define how the process is signaled on each collection interval.\n  ## Valid values are:\n  ##   \"none\"    : Do not signal anything. (Recommended for service inputs)\n  ##               The process must output metrics by itself.\n  ##   \"STDIN\"   : Send a newline on STDIN. (Recommended for gather inputs)\n  ##   \"SIGHUP\"  : Send a HUP signal. Not available on Windows. (not recommended)\n  ##   \"SIGUSR1\" : Send a USR1 signal. Not available on Windows.\n  ##   \"SIGUSR2\" : Send a USR2 signal. Not available on Windows.\n  signal = \"none\"\n\n  ## Delay before the process is restarted after an unexpected termination\n  restart_delay = \"10s\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "fail2ban",
		"Description": " Read metrics from fail2ban.",
		"SampleConfig": "[[inputs.fail2ban]]\n  ## Use sudo to run fail2ban-client\n  use_sudo = false\n"
	},
	{
		"Name": "fibaro",
		"Description": " Read devices value(s) from a Fibaro controller",
		"SampleConfig": "[[inputs.fibaro]]\n  ## Required Fibaro controller address/hostname.\n  ## Note: at the time of writing this plugin, Fibaro only implemented http - no https available\n  url = \"http://\u003ccontroller\u003e:80\"\n\n  ## Required credentials to access the API (http://\u003ccontroller/api/\u003ccomponent\u003e)\n  username = \"\u003cusername\u003e\"\n  password = \"\u003cpassword\u003e\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "file",
		"Description": " Parse a complete file each interval",
		"SampleConfig": "[[inputs.file]]\n  ## Files to parse each interval.  Accept standard unix glob matching rules,\n  ## as well as ** to match recursive files and directories.\n  files = [\"/tmp/metrics.out\"]\n\n  ## Character encoding to use when interpreting the file contents.  Invalid\n  ## characters are replaced using the unicode replacement character.  When set\n  ## to the empty string the data is not decoded to text.\n  ##   ex: character_encoding = \"utf-8\"\n  ##       character_encoding = \"utf-16le\"\n  ##       character_encoding = \"utf-16be\"\n  ##       character_encoding = \"\"\n  # character_encoding = \"\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n\n  ## Name a tag containing the name of the file the data was parsed from.  Leave empty\n  ## to disable. Cautious when file name variation is high, this can increase the cardinality\n  ## significantly. Read more about cardinality here:\n  ## https://docs.influxdata.com/influxdb/cloud/reference/glossary/#series-cardinality\n  # file_tag = \"\"\n"
	},
	{
		"Name": "filecount",
		"Description": " Count files in a directory",
		"SampleConfig": "[[inputs.filecount]]\n  ## Directories to gather stats about.\n  ## This accept standard unit glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**    -\u003e recursively find all directories in /var/log and count files in each directories\n  ##   /var/log/*/*   -\u003e find all directories with a parent dir in /var/log and count files in each directories\n  ##   /var/log       -\u003e count all files in /var/log and all of its subdirectories\n  directories = [\"/var/cache/apt\", \"/tmp\"]\n\n  ## Only count files that match the name pattern. Defaults to \"*\".\n  name = \"*\"\n\n  ## Count files in subdirectories. Defaults to true.\n  recursive = true\n\n  ## Only count regular files. Defaults to true.\n  regular_only = true\n\n  ## Follow all symlinks while walking the directory tree. Defaults to false.\n  follow_symlinks = false\n\n  ## Only count files that are at least this size. If size is\n  ## a negative number, only count files that are smaller than the\n  ## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n  ## Without quotes and units, interpreted as size in bytes.\n  size = \"0B\"\n\n  ## Only count files that have not been touched for at least this\n  ## duration. If mtime is negative, only count files that have been\n  ## touched in this duration. Defaults to \"0s\".\n  mtime = \"0s\"\n"
	},
	{
		"Name": "filestat",
		"Description": " Read stats about given file(s)",
		"SampleConfig": "[[inputs.filestat]]\n  ## Files to gather stats about.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". See https://github.com/gobwas/glob.\n  files = [\"/etc/telegraf/telegraf.conf\", \"/var/log/**.log\"]\n\n  ## If true, read the entire file and calculate an md5 checksum.\n  md5 = false\n"
	},
	{
		"Name": "fireboard",
		"Description": " Read real time temps from fireboard.io servers",
		"SampleConfig": "[[inputs.fireboard]]\n  ## Specify auth token for your account\n  auth_token = \"invalidAuthToken\"\n  ## You can override the fireboard server URL if necessary\n  # url = https://fireboard.io/api/v1/devices.json\n  ## You can set a different http_timeout if you need to\n  ## You should set a string using an number and time indicator\n  ## for example \"12s\" for 12 seconds.\n  # http_timeout = \"4s\"\n"
	},
	{
		"Name": "fluentd",
		"Description": " Read metrics exposed by fluentd in_monitor plugin",
		"SampleConfig": "[[inputs.fluentd]]\n  ## This plugin reads information exposed by fluentd (using /api/plugins.json endpoint).\n  ##\n  ## Endpoint:\n  ## - only one URI is allowed\n  ## - https is not supported\n  endpoint = \"http://localhost:24220/api/plugins.json\"\n\n  ## Define which plugins have to be excluded (based on \"type\" field - e.g. monitor_agent)\n  exclude = [\n    \"monitor_agent\",\n    \"dummy\",\n  ]\n"
	},
	{
		"Name": "github",
		"Description": " Gather repository information from GitHub hosted repositories.",
		"SampleConfig": "[[inputs.github]]\n  ## List of repositories to monitor\n  repositories = [\n    \"influxdata/telegraf\",\n    \"influxdata/influxdb\"\n  ]\n\n  ## Github API access token.  Unauthenticated requests are limited to 60 per hour.\n  # access_token = \"\"\n\n  ## Github API enterprise url. Github Enterprise accounts must specify their base url.\n  # enterprise_base_url = \"\"\n\n  ## Timeout for HTTP requests.\n  # http_timeout = \"5s\"\n\n  ## List of additional fields to query.\n  ## NOTE: Getting those fields might involve issuing additional API-calls, so please\n  ##       make sure you do not exceed the rate-limit of GitHub.\n  ##\n  ## Available fields are:\n  ##  - pull-requests -- number of open and closed pull requests (2 API-calls per repository)\n  # additional_fields = []\n"
	},
	{
		"Name": "gnmi",
		"Description": " gNMI telemetry input plugin",
		"SampleConfig": "[[inputs.gnmi]]\n  ## Address and port of the gNMI GRPC server\n  addresses = [\"10.49.234.114:57777\"]\n\n  ## define credentials\n  username = \"cisco\"\n  password = \"cisco\"\n\n  ## gNMI encoding requested (one of: \"proto\", \"json\", \"json_ietf\", \"bytes\")\n  # encoding = \"proto\"\n\n  ## redial in case of failures after\n  redial = \"10s\"\n\n  ## enable client-side TLS and define CA to authenticate the device\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # insecure_skip_verify = true\n\n  ## define client-side TLS certificate \u0026 key to authenticate to the device\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## gNMI subscription prefix (optional, can usually be left empty)\n  ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n  # origin = \"\"\n  # prefix = \"\"\n  # target = \"\"\n\n  ## Define additional aliases to map telemetry encoding paths to simple measurement names\n  # [inputs.gnmi.aliases]\n  #   ifcounters = \"openconfig:/interfaces/interface/state/counters\"\n\n  [[inputs.gnmi.subscription]]\n    ## Name of the measurement that will be emitted\n    name = \"ifcounters\"\n\n    ## Origin and path of the subscription\n    ## See: https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md#222-paths\n    ##\n    ## origin usually refers to a (YANG) data model implemented by the device\n    ## and path to a specific substructure inside it that should be subscribed to (similar to an XPath)\n    ## YANG models can be found e.g. here: https://github.com/YangModels/yang/tree/master/vendor/cisco/xr\n    origin = \"openconfig-interfaces\"\n    path = \"/interfaces/interface/state/counters\"\n\n    # Subscription mode (one of: \"target_defined\", \"sample\", \"on_change\") and interval\n    subscription_mode = \"sample\"\n    sample_interval = \"10s\"\n\n    ## Suppress redundant transmissions when measured values are unchanged\n    # suppress_redundant = false\n\n    ## If suppression is enabled, send updates at least every X seconds anyway\n    # heartbeat_interval = \"60s\"\n\n  #[[inputs.gnmi.subscription]]\n    # name = \"descr\"\n    # origin = \"openconfig-interfaces\"\n    # path = \"/interfaces/interface/state/description\"\n    # subscription_mode = \"on_change\"\n\n    ## If tag_only is set, the subscription in question will be utilized to maintain a map of\n    ## tags to apply to other measurements emitted by the plugin, by matching path keys\n    ## All fields from the tag-only subscription will be applied as tags to other readings,\n    ## in the format \u003cname\u003e_\u003cfieldBase\u003e.\n    # tag_only = true\n"
	},
	{
		"Name": "graylog",
		"Description": " Read flattened metrics from one or more GrayLog HTTP endpoints",
		"SampleConfig": "[[inputs.graylog]]\n  ## API endpoint, currently supported API:\n  ##\n  ##   - multiple  (e.g. http://\u003chost\u003e:9000/api/system/metrics/multiple)\n  ##   - namespace (e.g. http://\u003chost\u003e:9000/api/system/metrics/namespace/{namespace})\n  ##\n  ## For namespace endpoint, the metrics array will be ignored for that call.\n  ## Endpoint can contain namespace and multiple type calls.\n  ##\n  ## Please check http://[graylog-server-ip]:9000/api/api-browser for full list\n  ## of endpoints\n  servers = [\n    \"http://[graylog-server-ip]:9000/api/system/metrics/multiple\",\n  ]\n\n  ## Set timeout (default 5 seconds)\n  # timeout = \"5s\"\n\n  ## Metrics list\n  ## List of metrics can be found on Graylog webservice documentation.\n  ## Or by hitting the web service api at:\n  ##   http://[graylog-host]:9000/api/system/metrics\n  metrics = [\n    \"jvm.cl.loaded\",\n    \"jvm.memory.pools.Metaspace.committed\"\n  ]\n\n  ## Username and password\n  username = \"\"\n  password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "haproxy",
		"Description": " Read metrics of HAProxy, via socket or HTTP stats page",
		"SampleConfig": "[[inputs.haproxy]]\n  ## An array of address to gather stats about. Specify an ip on hostname\n  ## with optional port. ie localhost, 10.10.3.33:1936, etc.\n  ## Make sure you specify the complete path to the stats endpoint\n  ## including the protocol, ie http://10.10.3.33:1936/haproxy?stats\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## If no servers are specified, then default to 127.0.0.1:1936/haproxy?stats\n  servers = [\"http://myhaproxy.com:1936/haproxy?stats\"]\n\n  ## You can also use local socket with standard wildcard globbing.\n  ## Server address not starting with 'http' will be treated as a possible\n  ## socket, so both examples below are valid.\n  # servers = [\"socket:/run/haproxy/admin.sock\", \"/run/haproxy/*.sock\"]\n\n  ## By default, some of the fields are renamed from what haproxy calls them.\n  ## Setting this option to true results in the plugin keeping the original\n  ## field names.\n  # keep_field_names = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "hddtemp",
		"Description": " Monitor disks' temperatures using hddtemp",
		"SampleConfig": "[[inputs.hddtemp]]\n  ## By default, telegraf gathers temps data from all disks detected by the\n  ## hddtemp.\n  ##\n  ## Only collect temps from the selected disks.\n  ##\n  ## A * as the device name will return the temperature values of all disks.\n  ##\n  # address = \"127.0.0.1:7634\"\n  # devices = [\"sda\", \"*\"]\n"
	},
	{
		"Name": "http",
		"Description": " Read formatted metrics from one or more HTTP endpoints",
		"SampleConfig": "[[inputs.http]]\n  ## One or more URLs from which to read formatted metrics\n  urls = [\n    \"http://localhost/metrics\"\n  ]\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP entity-body to send with POST/PUT requests.\n  # body = \"\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Optional file with Bearer token\n  ## file content is added as an Authorization header\n  # bearer_token = \"/path/to/file\"\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## OAuth2 Client Credentials. The options 'client_id', 'client_secret', and 'token_url' are required to use OAuth2.\n  # client_id = \"clientid\"\n  # client_secret = \"secret\"\n  # token_url = \"https://indentityprovider/oauth2/v1/token\"\n  # scopes = [\"urn:opc:idm:__myscopes__\"]\n\n  ## HTTP Proxy support\n  # http_proxy_url = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional Cookie authentication\n  # cookie_auth_url = \"https://localhost/authMe\"\n  # cookie_auth_method = \"POST\"\n  # cookie_auth_username = \"username\"\n  # cookie_auth_password = \"pa$$word\"\n  # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n  # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n  ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n  # cookie_auth_renewal = \"5m\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## List of success status codes\n  # success_status_codes = [200]\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n"
	},
	{
		"Name": "http_listener_v2",
		"Description": " Generic HTTP write listener",
		"SampleConfig": "[[inputs.http_listener_v2]]\n  ## Address and port to host HTTP listener on\n  service_address = \":8080\"\n\n  ## Paths to listen to.\n  # paths = [\"/telegraf\"]\n\n  ## Save path as http_listener_v2_path tag if set to true\n  # path_tag = false\n\n  ## HTTP methods to accept.\n  # methods = [\"POST\", \"PUT\"]\n\n  ## maximum duration before timing out read of the request\n  # read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  # write_timeout = \"10s\"\n\n  ## Maximum allowed http request body size in bytes.\n  ## 0 means to use the default of 524,288,000 bytes (500 mebibytes)\n  # max_body_size = \"500MB\"\n\n  ## Part of the request to consume.  Available options are \"body\" and\n  ## \"query\".\n  # data_source = \"body\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n  ## Optional setting to map http headers into tags\n  ## If the http header is not present on the request, no corresponding tag will be added\n  ## If multiple instances of the http header are present, only the first value will be used\n  # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "http_response",
		"Description": " HTTP/HTTPS request given an address a method and a timeout",
		"SampleConfig": "[[inputs.http_response]]\n  ## List of urls to query.\n  # urls = [\"http://localhost\"]\n\n  ## Set http_proxy (telegraf uses the system wide proxy settings if it's is not set)\n  # http_proxy = \"http://localhost:8888\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## HTTP Request Method\n  # method = \"GET\"\n\n  ## Whether to follow redirects from the server (defaults to false)\n  # follow_redirects = false\n\n  ## Optional file with Bearer token\n  ## file content is added as an Authorization header\n  # bearer_token = \"/path/to/file\"\n\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional HTTP Request Body\n  # body = '''\n  # {'fake':'data'}\n  # '''\n\n  ## Optional name of the field that will contain the body of the response.\n  ## By default it is set to an empty String indicating that the body's content won't be added\n  # response_body_field = ''\n\n  ## Maximum allowed HTTP response body size in bytes.\n  ## 0 means to use the default of 32MiB.\n  ## If the response body size exceeds this limit a \"body_read_error\" will be raised\n  # response_body_max_size = \"32MiB\"\n\n  ## Optional substring or regex match in body of the response (case sensitive)\n  # response_string_match = \"\\\"service_status\\\": \\\"up\\\"\"\n  # response_string_match = \"ok\"\n  # response_string_match = \"\\\".*_status\\\".?:.?\\\"up\\\"\"\n\n  ## Expected response status code.\n  ## The status code of the response is compared to this value. If they match, the field\n  ## \"response_status_code_match\" will be 1, otherwise it will be 0. If the\n  ## expected status code is 0, the check is disabled and the field won't be added.\n  # response_status_code = 0\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n  ## Use the given name as the SNI server name on each URL\n  # tls_server_name = \"\"\n\n  ## HTTP Request Headers (all values must be strings)\n  # [inputs.http_response.headers]\n  #   Host = \"github.com\"\n\n  ## Optional setting to map response http headers into tags\n  ## If the http header is not present on the request, no corresponding tag will be added\n  ## If multiple instances of the http header are present, only the first value will be used\n  # http_header_tags = {\"HTTP_HEADER\" = \"TAG_NAME\"}\n\n  ## Interface to use when dialing an address\n  # interface = \"eth0\"\n"
	},
	{
		"Name": "httpjson",
		"Description": " Read flattened metrics from one or more JSON HTTP endpoints",
		"SampleConfig": "[[inputs.httpjson]]\n  ## NOTE This plugin only reads numerical measurements, strings and booleans\n  ## will be ignored.\n\n  ## Name for the service being polled.  Will be appended to the name of the\n  ## measurement e.g. \"httpjson_webserver_stats\".\n  ##\n  ## Deprecated (1.3.0): Use name_override, name_suffix, name_prefix instead.\n  name = \"webserver_stats\"\n\n  ## URL of each server in the service's cluster\n  servers = [\n    \"http://localhost:9999/stats/\",\n    \"http://localhost:9998/stats/\",\n  ]\n  ## Set response_timeout (default 5 seconds)\n  response_timeout = \"5s\"\n\n  ## HTTP method to use: GET or POST (case-sensitive)\n  method = \"GET\"\n\n  ## Tags to extract from top-level of JSON server response.\n  # tag_keys = [\n  #   \"my_tag_1\",\n  #   \"my_tag_2\"\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Request Parameters (all values must be strings).  For \"GET\" requests, data\n  ## will be included in the query.  For \"POST\" requests, data will be included\n  ## in the request body as \"x-www-form-urlencoded\".\n  # [inputs.httpjson.parameters]\n  #   event_type = \"cpu_spike\"\n  #   threshold = \"0.75\"\n\n  ## HTTP Request Headers (all values must be strings).\n  # [inputs.httpjson.headers]\n  #   X-Auth-Token = \"my-xauth-token\"\n  #   apiVersion = \"v1\"\n"
	},
	{
		"Name": "hugepages",
		"Description": " Gathers huge pages measurements.",
		"SampleConfig": "[[inputs.hugepages]]\n  ## Supported huge page types:\n  ##   - \"root\" - based on root huge page control directory: /sys/kernel/mm/hugepages\n  ##   - \"per_node\" - based on per NUMA node directories: /sys/devices/system/node/node[0-9]*/hugepages\n  ##   - \"meminfo\" - based on /proc/meminfo file\n  # types = [\"root\", \"per_node\"]\n"
	},
	{
		"Name": "icinga2",
		"Description": " Gather Icinga2 status",
		"SampleConfig": "[[inputs.icinga2]]\n  ## Required Icinga2 server address\n  # server = \"https://localhost:5665\"\n  \n  ## Required Icinga2 object type (\"services\" or \"hosts\")\n  # object_type = \"services\"\n\n  ## Credentials for basic HTTP authentication\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Maximum time to receive response.\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n"
	},
	{
		"Name": "infiniband",
		"Description": " Gets counters from all InfiniBand cards and ports installed",
		"SampleConfig": "[[inputs.infiniband]]\n  # no configuration\n"
	},
	{
		"Name": "influxdb",
		"Description": " Read InfluxDB-formatted JSON metrics from one or more HTTP endpoints",
		"SampleConfig": "[[inputs.influxdb]]\n  ## Works with InfluxDB debug endpoints out of the box,\n  ## but other services can use this format too.\n  ## See the influxdb plugin's README for more details.\n\n  ## Multiple URLs from which to read InfluxDB-formatted JSON\n  ## Default is \"http://localhost:8086/debug/vars\".\n  urls = [\n    \"http://localhost:8086/debug/vars\"\n  ]\n\n  ## Username and password to send using HTTP Basic Authentication.\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## http request \u0026 header timeout\n  timeout = \"5s\"\n"
	},
	{
		"Name": "influxdb_listener",
		"Description": " Accept metrics over InfluxDB 1.x HTTP API",
		"SampleConfig": "[[inputs.influxdb_listener]]\n  ## Address and port to host HTTP listener on\n  service_address = \":8186\"\n\n  ## maximum duration before timing out read of the request\n  read_timeout = \"10s\"\n  ## maximum duration before timing out write of the response\n  write_timeout = \"10s\"\n\n  ## Maximum allowed HTTP request body size in bytes.\n  ## 0 means to use the default of 32MiB.\n  max_body_size = 0\n\n  ## Maximum line size allowed to be sent in bytes.\n  ##   deprecated in 1.14; parser now handles lines of unlimited length and option is ignored\n  # max_line_size = 0\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  tls_cert = \"/etc/telegraf/cert.pem\"\n  tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional tag name used to store the database name.\n  ## If the write has a database in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  ## If you have a tag that is the same as the one specified below, and supply a database,\n  ## the tag will be overwritten with the database supplied.\n  # database_tag = \"\"\n\n  ## If set the retention policy specified in the write query will be added as\n  ## the value of this tag name.\n  # retention_policy_tag = \"\"\n\n  ## Optional username and password to accept for HTTP basic authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # basic_username = \"foobar\"\n  # basic_password = \"barfoo\"\n\n  ## Influx line protocol parser\n  ## 'internal' is the default. 'upstream' is a newer parser that is faster\n  ## and more memory efficient.\n  # parser_type = \"internal\"\n"
	},
	{
		"Name": "influxdb_v2_listener",
		"Description": " Accept metrics over InfluxDB 2.x HTTP API",
		"SampleConfig": "[[inputs.influxdb_v2_listener]]\n  ## Address and port to host InfluxDB listener on\n  ## (Double check the port. Could be 9999 if using OSS Beta)\n  service_address = \":8086\"\n\n  ## Maximum allowed HTTP request body size in bytes.\n  ## 0 means to use the default of 32MiB.\n  # max_body_size = \"32MiB\"\n\n  ## Optional tag to determine the bucket.\n  ## If the write has a bucket in the query string then it will be kept in this tag name.\n  ## This tag can be used in downstream outputs.\n  ## The default value of nothing means it will be off and the database will not be recorded.\n  # bucket_tag = \"\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Add service certificate and key\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Optional token to accept for HTTP authentication.\n  ## You probably want to make sure you have TLS configured above for this.\n  # token = \"some-long-shared-secret-token\"\n\n  ## Influx line protocol parser\n  ## 'internal' is the default. 'upstream' is a newer parser that is faster\n  ## and more memory efficient.\n  # parser_type = \"internal\"\n"
	},
	{
		"Name": "intel_pmu",
		"Description": " Intel Performance Monitoring Unit plugin exposes Intel PMU metrics available through Linux Perf subsystem",
		"SampleConfig": "[[inputs.intel_pmu]]\n  ## List of filesystem locations of JSON files that contain PMU event definitions.\n  event_definitions = [\"/var/cache/pmu/GenuineIntel-6-55-4-core.json\", \"/var/cache/pmu/GenuineIntel-6-55-4-uncore.json\"]\n  \n  ## List of core events measurement entities. There can be more than one core_events sections.\n  [[inputs.intel_pmu.core_events]]\n    ## List of events to be counted. Event names shall match names from event_definitions files.\n    ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n    ## If absent, all core events from provided event_definitions are counted skipping unresolvable ones.\n    events = [\"INST_RETIRED.ANY\", \"CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k\"]\n\n    ## Limits the counting of events to core numbers specified.\n    ## If absent, events are counted on all cores.\n    ## Single \"0\", multiple \"0,1,2\" and range \"0-2\" notation is supported for each array element.\n    ##   example: cores = [\"0,2\", \"4\", \"12-16\"]\n    cores = [\"0\"]\n\n    ## Indicator that plugin shall attempt to run core_events.events as a single perf group.\n    ## If absent or set to false, each event is counted individually. Defaults to false.\n    ## This limits the number of events that can be measured to a maximum of available hardware counters per core.\n    ## Could vary depending on type of event, use of fixed counters.\n    # perf_group = false\n\n    ## Optionally set a custom tag value that will be added to every measurement within this events group.\n    ## Can be applied to any group of events, unrelated to perf_group setting.\n    # events_tag = \"\"\n\n  ## List of uncore event measurement entities. There can be more than one uncore_events sections.\n  [[inputs.intel_pmu.uncore_events]]\n    ## List of events to be counted. Event names shall match names from event_definitions files.\n    ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n    ## If absent, all uncore events from provided event_definitions are counted skipping unresolvable ones.\n    events = [\"UNC_CHA_CLOCKTICKS\", \"UNC_CHA_TOR_OCCUPANCY.IA_MISS\"]\n\n    ## Limits the counting of events to specified sockets.\n    ## If absent, events are counted on all sockets.\n    ## Single \"0\", multiple \"0,1\" and range \"0-1\" notation is supported for each array element.\n    ##   example: sockets = [\"0-2\"]\n    sockets = [\"0\"]\n\n    ## Indicator that plugin shall provide an aggregated value for multiple units of same type distributed in an uncore.\n    ## If absent or set to false, events for each unit are exposed as separate metric. Defaults to false.\n    # aggregate_uncore_units = false\n\n    ## Optionally set a custom tag value that will be added to every measurement within this events group.\n    # events_tag = \"\"\n"
	},
	{
		"Name": "intel_powerstat",
		"Description": " Intel PowerStat plugin enables monitoring of platform metrics (power, TDP) and per-CPU metrics like temperature, power and utilization.",
		"SampleConfig": "[[inputs.intel_powerstat]]\n  ## The user can choose which package metrics are monitored by the plugin with the package_metrics setting:\n  ## - The default, will collect \"current_power_consumption\", \"current_dram_power_consumption\" and \"thermal_design_power\"\n  ## - Setting this value to an empty array means no package metrics will be collected\n  ## - Finally, a user can specify individual metrics to capture from the supported options list\n  ## Supported options:\n  ##   \"current_power_consumption\", \"current_dram_power_consumption\", \"thermal_design_power\", \"max_turbo_frequency\", \"uncore_frequency\"\n  # package_metrics = [\"current_power_consumption\", \"current_dram_power_consumption\", \"thermal_design_power\"]\n\n  ## The user can choose which per-CPU metrics are monitored by the plugin in cpu_metrics array.\n  ## Empty or missing array means no per-CPU specific metrics will be collected by the plugin.\n  ## Supported options:\n  ##   \"cpu_frequency\", \"cpu_c0_state_residency\", \"cpu_c1_state_residency\", \"cpu_c6_state_residency\", \"cpu_busy_cycles\", \"cpu_temperature\", \"cpu_busy_frequency\"\n  ## ATTENTION: cpu_busy_cycles option is DEPRECATED - superseded by cpu_c0_state_residency\n  # cpu_metrics = []\n"
	},
	{
		"Name": "intel_rdt",
		"Description": " Read Intel RDT metrics",
		"SampleConfig": "[[inputs.intel_rdt]]\n  ## Optionally set sampling interval to Nx100ms. \n  ## This value is propagated to pqos tool. Interval format is defined by pqos itself.\n  ## If not provided or provided 0, will be set to 10 = 10x100ms = 1s.\n  # sampling_interval = \"10\"\n \n  ## Optionally specify the path to pqos executable. \n  ## If not provided, auto discovery will be performed.\n  # pqos_path = \"/usr/local/bin/pqos\"\n\n  ## Optionally specify if IPC and LLC_Misses metrics shouldn't be propagated.\n  ## If not provided, default value is false.\n  # shortened_metrics = false\n\n  ## Specify the list of groups of CPU core(s) to be provided as pqos input. \n  ## Mandatory if processes aren't set and forbidden if processes are specified.\n  ## e.g. [\"0-3\", \"4,5,6\"] or [\"1-3,4\"]\n  # cores = [\"0-3\"]\n\n  ## Specify the list of processes for which Metrics will be collected.\n  ## Mandatory if cores aren't set and forbidden if cores are specified.\n  ## e.g. [\"qemu\", \"pmd\"]\n  # processes = [\"process\"]\n\n  ## Specify if the pqos process should be called with sudo.\n  ## Mandatory if the telegraf process does not run as root.\n  # use_sudo = false\n"
	},
	{
		"Name": "internal",
		"Description": " Collect statistics about itself",
		"SampleConfig": "[[inputs.internal]]\n  ## If true, collect telegraf memory stats.\n  # collect_memstats = true\n"
	},
	{
		"Name": "internet_speed",
		"Description": " Monitors internet speed using speedtest.net service",
		"SampleConfig": "[[inputs.internet_speed]]\n  ## This plugin downloads many MB of data each time it is run. As such\n  ## consider setting a higher interval for this plugin to reduce the\n  ## demand on your internet connection.\n  # interval = \"60m\"\n\n  ## Sets if runs file download test\n  # enable_file_download = false\n\n  ## Caches the closest server location\n  # cache = false\n"
	},
	{
		"Name": "interrupts",
		"Description": " This plugin gathers interrupts data from /proc/interrupts and /proc/softirqs.",
		"SampleConfig": "[[inputs.interrupts]]\n  ## When set to true, cpu metrics are tagged with the cpu.  Otherwise cpu is\n  ## stored as a field.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  # cpu_as_tag = false\n\n  ## To filter which IRQs to collect, make use of tagpass / tagdrop, i.e.\n  # [inputs.interrupts.tagdrop]\n  #   irq = [ \"NET_RX\", \"TASKLET\" ]\n"
	},
	{
		"Name": "ipmi_sensor",
		"Description": " Read metrics from the bare metal servers via IPMI",
		"SampleConfig": "[[inputs.ipmi_sensor]]\n  ## optionally specify the path to the ipmitool executable\n  # path = \"/usr/bin/ipmitool\"\n  ##\n  ## Setting 'use_sudo' to true will make use of sudo to run ipmitool.\n  ## Sudo must be configured to allow the telegraf user to run ipmitool\n  ## without a password.\n  # use_sudo = false\n  ##\n  ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR\n  # privilege = \"ADMINISTRATOR\"\n  ##\n  ## optionally specify one or more servers via a url matching\n  ##  [username[:password]@][protocol[(address)]]\n  ##  e.g.\n  ##    root:passwd@lan(127.0.0.1)\n  ##\n  ## if no servers are specified, local machine sensor stats will be queried\n  ##\n  # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n\n  ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"30s\"\n\n  ## Timeout for the ipmitool command to complete. Default is 20 seconds.\n  timeout = \"20s\"\n\n  ## Schema Version: (Optional, defaults to version 1)\n  metric_version = 2\n\n  ## Optionally provide the hex key for the IMPI connection.\n  # hex_key = \"\"\n\n  ## If ipmitool should use a cache\n  ## for me ipmitool runs about 2 to 10 times faster with cache enabled on HP G10 servers (when using ubuntu20.04)\n  ## the cache file may not work well for you if some sensors come up late\n  # use_cache = false\n\n  ## Path to the ipmitools cache file (defaults to OS temp dir)\n  ## The provided path must exist and must be writable\n  # cache_path = \"\"\n"
	},
	{
		"Name": "ipset",
		"Description": " Gather packets and bytes counters from Linux ipsets",
		"SampleConfig": "  [[inputs.ipset]]\n    ## By default, we only show sets which have already matched at least 1 packet.\n    ## set include_unmatched_sets = true to gather them all.\n    include_unmatched_sets = false\n    ## Adjust your sudo settings appropriately if using this option (\"sudo ipset save\")\n    ## You can avoid using sudo or root, by setting appropriate privileges for\n    ## the telegraf.service systemd service.\n    use_sudo = false\n    ## The default timeout of 1s for ipset execution can be overridden here:\n    # timeout = \"1s\"\n\n"
	},
	{
		"Name": "iptables",
		"Description": " Gather packets and bytes throughput from iptables",
		"SampleConfig": "[[inputs.iptables]]\n  ## iptables require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run iptables.\n  ## Users must configure sudo to allow telegraf user to run iptables with no password.\n  ## iptables can be restricted to only list command \"iptables -nvL\".\n  use_sudo = false\n  ## Setting 'use_lock' to true runs iptables with the \"-w\" option.\n  ## Adjust your sudo settings appropriately if using this option (\"iptables -w 5 -nvl\")\n  use_lock = false\n  ## Define an alternate executable, such as \"ip6tables\". Default is \"iptables\".\n  # binary = \"ip6tables\"\n  ## defines the table to monitor:\n  table = \"filter\"\n  ## defines the chains to monitor.\n  ## NOTE: iptables rules without a comment will not be monitored.\n  ## Read the plugin documentation for more information.\n  chains = [ \"INPUT\" ]\n"
	},
	{
		"Name": "ipvs",
		"Description": " Collect virtual and real server stats from Linux IPVS",
		"SampleConfig": "[[inputs.ipvs]]\n  # no configuration\n"
	},
	{
		"Name": "jenkins",
		"Description": " Read jobs and cluster metrics from Jenkins instances",
		"SampleConfig": "[[inputs.jenkins]]\n  ## The Jenkins URL in the format \"schema://host:port\"\n  url = \"http://my-jenkins-instance:8080\"\n  # username = \"admin\"\n  # password = \"admin\"\n\n  ## Set response_timeout\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use SSL but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional Max Job Build Age filter\n  ## Default 1 hour, ignore builds older than max_build_age\n  # max_build_age = \"1h\"\n\n  ## Optional Sub Job Depth filter\n  ## Jenkins can have unlimited layer of sub jobs\n  ## This config will limit the layers of pulling, default value 0 means\n  ## unlimited pulling until no more sub jobs\n  # max_subjob_depth = 0\n\n  ## Optional Sub Job Per Layer\n  ## In workflow-multibranch-plugin, each branch will be created as a sub job.\n  ## This config will limit to call only the lasted branches in each layer,\n  ## empty will use default value 10\n  # max_subjob_per_layer = 10\n\n  ## Jobs to include or exclude from gathering\n  ## When using both lists, job_exclude has priority.\n  ## Wildcards are supported: [ \"jobA/*\", \"jobB/subjob1/*\"]\n  # job_include = [ \"*\" ]\n  # job_exclude = [ ]\n\n  ## Nodes to include or exclude from gathering\n  ## When using both lists, node_exclude has priority.\n  # node_include = [ \"*\" ]\n  # node_exclude = [ ]\n\n  ## Worker pool for jenkins plugin only\n  ## Empty this field will use default value 5\n  # max_connections = 5\n"
	},
	{
		"Name": "jolokia",
		"Description": " Read JMX metrics through Jolokia",
		"SampleConfig": "[[inputs.jolokia]]\n  ## This is the context root used to compose the jolokia url\n  ## NOTE that Jolokia requires a trailing slash at the end of the context root\n  context = \"/jolokia/\"\n\n  ## This specifies the mode used\n  # mode = \"proxy\"\n  #\n  ## When in proxy mode this section is used to specify further\n  ## proxy address configurations.\n  ## Remember to change host address to fit your environment.\n  # [inputs.jolokia.proxy]\n  #   host = \"127.0.0.1\"\n  #   port = \"8080\"\n  \n  ## Optional http timeouts\n  ##\n  ## response_header_timeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # response_header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## List of servers exposing jolokia read service\n  [[inputs.jolokia.servers]]\n    name = \"as-server-01\"\n    host = \"127.0.0.1\"\n    port = \"8080\"\n    # username = \"myuser\"\n    # password = \"mypassword\"\n\n  ## List of metrics collected on above servers\n  ## Each metric consists in a name, a jmx path and either\n  ## a pass or drop slice attribute.\n  ## This collect all heap memory usage metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"heap_memory_usage\"\n    mbean  = \"java.lang:type=Memory\"\n    attribute = \"HeapMemoryUsage\"\n\n  ## This collect thread counts metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"thread_count\"\n    mbean  = \"java.lang:type=Threading\"\n    attribute = \"TotalStartedThreadCount,ThreadCount,DaemonThreadCount,PeakThreadCount\"\n\n  ## This collect number of class loaded/unloaded counts metrics.\n  [[inputs.jolokia.metrics]]\n    name = \"class_count\"\n    mbean  = \"java.lang:type=ClassLoading\"\n    attribute = \"LoadedClassCount,UnloadedClassCount,TotalLoadedClassCount\"\n"
	},
	{
		"Name": "jolokia2_agent",
		"Description": " Read JMX metrics from a Jolokia REST agent endpoint",
		"SampleConfig": "[[inputs.jolokia2_agent]]\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  # Add agents URLs to query\n  urls = [\"http://localhost:8080/jolokia\"]\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add metrics to read\n  [[inputs.jolokia2_agent.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n"
	},
	{
		"Name": "jolokia2_proxy",
		"Description": " Read JMX metrics from a Jolokia REST proxy endpoint",
		"SampleConfig": "[[inputs.jolokia2_proxy]]\n  # default_tag_prefix      = \"\"\n  # default_field_prefix    = \"\"\n  # default_field_separator = \".\"\n\n  ## Proxy agent\n  url = \"http://localhost:8080/jolokia\"\n  # username = \"\"\n  # password = \"\"\n  # response_timeout = \"5s\"\n\n  ## Optional TLS config\n  # tls_ca   = \"/var/private/ca.pem\"\n  # tls_cert = \"/var/private/client.pem\"\n  # tls_key  = \"/var/private/client-key.pem\"\n  # insecure_skip_verify = false\n\n  ## Add proxy targets to query\n  # default_target_username = \"\"\n  # default_target_password = \"\"\n  [[inputs.jolokia2_proxy.target]]\n    url = \"service:jmx:rmi:///jndi/rmi://targethost:9999/jmxrmi\"\n    # username = \"\"\n    # password = \"\"\n\n  ## Add metrics to read\n  [[inputs.jolokia2_proxy.metric]]\n    name  = \"java_runtime\"\n    mbean = \"java.lang:type=Runtime\"\n    paths = [\"Uptime\"]\n"
	},
	{
		"Name": "jti_openconfig_telemetry",
		"Description": " Subscribe and receive OpenConfig Telemetry data using JTI",
		"SampleConfig": "[[inputs.jti_openconfig_telemetry]]\n  ## List of device addresses to collect telemetry from\n  servers = [\"localhost:1883\"]\n\n  ## Authentication details. Username and password are must if device expects\n  ## authentication. Client ID must be unique when connecting from multiple instances\n  ## of telegraf to the same device\n  username = \"user\"\n  password = \"pass\"\n  client_id = \"telegraf\"\n\n  ## Frequency to get data\n  sample_frequency = \"1000ms\"\n\n  ## Sensors to subscribe for\n  ## A identifier for each sensor can be provided in path by separating with space\n  ## Else sensor path will be used as identifier\n  ## When identifier is used, we can provide a list of space separated sensors.\n  ## A single subscription will be created with all these sensors and data will\n  ## be saved to measurement with this identifier name\n  sensors = [\n   \"/interfaces/\",\n   \"collection /components/ /lldp\",\n  ]\n\n  ## We allow specifying sensor group level reporting rate. To do this, specify the\n  ## reporting rate in Duration at the beginning of sensor paths / collection\n  ## name. For entries without reporting rate, we use configured sample frequency\n  sensors = [\n   \"1000ms customReporting /interfaces /lldp\",\n   \"2000ms collection /components\",\n   \"/interfaces\",\n  ]\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Delay between retry attempts of failed RPC calls or streams. Defaults to 1000ms.\n  ## Failed streams/calls will not be retried if 0 is provided\n  retry_delay = \"1000ms\"\n\n  ## To treat all string values as tags, set this to true\n  str_as_tags = false\n"
	},
	{
		"Name": "kafka_consumer",
		"Description": " Read metrics from Kafka topics",
		"SampleConfig": "[[inputs.kafka_consumer]]\n  ## Kafka brokers.\n  brokers = [\"localhost:9092\"]\n\n  ## Topics to consume.\n  topics = [\"telegraf\"]\n\n  ## When set this tag will be added to all metrics with the topic as the value.\n  # topic_tag = \"\"\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Set the minimal supported Kafka version.  Setting this enables the use of new\n  ## Kafka features and APIs.  Must be 0.10.2.0 or greater.\n  ##   ex: version = \"1.1.0\"\n  # version = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## SASL authentication credentials.  These settings should typically be used\n  ## with TLS encryption enabled\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Optional SASL:\n  ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n  ## (defaults to PLAIN)\n  # sasl_mechanism = \"\"\n\n  ## used if sasl_mechanism is GSSAPI (experimental)\n  # sasl_gssapi_service_name = \"\"\n  # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n  # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n  # sasl_gssapi_kerberos_config_path = \"/\"\n  # sasl_gssapi_realm = \"realm\"\n  # sasl_gssapi_key_tab_path = \"\"\n  # sasl_gssapi_disable_pafxfast = false\n\n  ## used if sasl_mechanism is OAUTHBEARER (experimental)\n  # sasl_access_token = \"\"\n\n  ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n  # sasl_version = 1\n\n  # Disable Kafka metadata full fetch\n  # metadata_full = false\n\n  ## Name of the consumer group.\n  # consumer_group = \"telegraf_metrics_consumers\"\n\n  ## Compression codec represents the various compression codecs recognized by\n  ## Kafka in messages.\n  ##  0 : None\n  ##  1 : Gzip\n  ##  2 : Snappy\n  ##  3 : LZ4\n  ##  4 : ZSTD\n  # compression_codec = 0\n  ## Initial offset position; one of \"oldest\" or \"newest\".\n  # offset = \"oldest\"\n\n  ## Consumer group partition assignment strategy; one of \"range\", \"roundrobin\" or \"sticky\".\n  # balance_strategy = \"range\"\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 1000000\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Maximum amount of time the consumer should take to process messages. If\n  ## the debug log prints messages from sarama about 'abandoning subscription\n  ## to [topic] because consuming was taking too long', increase this value to\n  ## longer than the time taken by the output plugin(s).\n  ##\n  ## Note that the effective timeout could be between 'max_processing_time' and\n  ## '2 * max_processing_time'.\n  # max_processing_time = \"100ms\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "kafka_consumer_legacy",
		"Description": "# DEPRECATED: The 'kafka_consumer_legacy' plugin is deprecated in version 1.4.0, use 'inputs.kafka_consumer' instead, NOTE: 'kafka_consumer' only supports Kafka v0.8+.",
		"SampleConfig": "# Read metrics from Kafka topic(s)\n[[inputs.kafka_consumer_legacy]]\n  ## topic(s) to consume\n  topics = [\"telegraf\"]\n\n  ## an array of Zookeeper connection strings\n  zookeeper_peers = [\"localhost:2181\"]\n\n  ## Zookeeper Chroot\n  zookeeper_chroot = \"\"\n\n  ## the name of the consumer group\n  consumer_group = \"telegraf_metrics_consumers\"\n\n  ## Offset (must be either \"oldest\" or \"newest\")\n  offset = \"oldest\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Maximum length of a message to consume, in bytes (default 0/unlimited);\n  ## larger messages are dropped\n  max_message_len = 65536\n"
	},
	{
		"Name": "kapacitor",
		"Description": " Read Kapacitor-formatted JSON metrics from one or more HTTP endpoints",
		"SampleConfig": "[[inputs.kapacitor]]\n  ## Multiple URLs from which to read Kapacitor-formatted JSON\n  ## Default is \"http://localhost:9092/kapacitor/v1/debug/vars\".\n  urls = [\n    \"http://localhost:9092/kapacitor/v1/debug/vars\"\n  ]\n\n  ## Time limit for http requests\n  timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "kernel",
		"Description": " Get kernel statistics from /proc/stat",
		"SampleConfig": "[[inputs.kernel]]\n  # no configuration\n"
	},
	{
		"Name": "kernel_vmstat",
		"Description": " Get kernel statistics from /proc/vmstat",
		"SampleConfig": "[[inputs.kernel_vmstat]]\n  # no configuration\n"
	},
	{
		"Name": "kibana",
		"Description": " Read status information from one or more Kibana servers",
		"SampleConfig": "[[inputs.kibana]]\n  ## Specify a list of one or more Kibana servers\n  servers = [\"http://localhost:5601\"]\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "kinesis_consumer",
		"Description": " Configuration for the AWS Kinesis input.",
		"SampleConfig": "[[inputs.kinesis_consumer]]\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  # access_key = \"\"\n  # secret_key = \"\"\n  # token = \"\"\n  # role_arn = \"\"\n  # web_identity_token_file = \"\"\n  # role_session_name = \"\"\n  # profile = \"\"\n  # shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n\n  ## Shard iterator type (only 'TRIM_HORIZON' and 'LATEST' currently supported)\n  # shard_iterator_type = \"TRIM_HORIZON\"\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ##\n  ## The content encoding of the data from kinesis\n  ## If you are processing a cloudwatch logs kinesis stream then set this to \"gzip\"\n  ## as AWS compresses cloudwatch log data before it is sent to kinesis (aws\n  ## also base64 encodes the zip byte data before pushing to the stream.  The base64 decoding\n  ## is done automatically by the golang sdk, as data is read from kinesis)\n  ##\n  # content_encoding = \"identity\"\n\n  ## Optional\n  ## Configuration for a dynamodb checkpoint\n  [inputs.kinesis_consumer.checkpoint_dynamodb]\n    ## unique name for this consumer\n    app_name = \"default\"\n    table_name = \"default\"\n"
	},
	{
		"Name": "knx_listener",
		"Description": " Listener capable of handling KNX bus messages provided through a KNX-IP Interface.",
		"SampleConfig": "[[inputs.knx_listener]]\n  ## Type of KNX-IP interface.\n  ## Can be either \"tunnel\" or \"router\".\n  # service_type = \"tunnel\"\n\n  ## Address of the KNX-IP interface.\n  service_address = \"localhost:3671\"\n\n  ## Measurement definition(s)\n  # [[inputs.knx_listener.measurement]]\n  #   ## Name of the measurement\n  #   name = \"temperature\"\n  #   ## Datapoint-Type (DPT) of the KNX messages\n  #   dpt = \"9.001\"\n  #   ## List of Group-Addresses (GAs) assigned to the measurement\n  #   addresses = [\"5/5/1\"]\n\n  # [[inputs.knx_listener.measurement]]\n  #   name = \"illumination\"\n  #   dpt = \"9.004\"\n  #   addresses = [\"5/5/3\"]\n"
	},
	{
		"Name": "kube_inventory",
		"Description": " Read metrics from the Kubernetes api",
		"SampleConfig": "[[inputs.kube_inventory]]\n  ## URL for the Kubernetes API\n  url = \"https://127.0.0.1\"\n\n  ## Namespace to use. Set to \"\" to use all namespaces.\n  # namespace = \"default\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  ## If both of these are empty, we'll use the default serviceaccount:\n  ## at: /run/secrets/kubernetes.io/serviceaccount/token\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional Resources to exclude from gathering\n  ## Leave them with blank with try to gather everything available.\n  ## Values can be - \"daemonsets\", deployments\", \"endpoints\", \"ingress\", \"nodes\",\n  ## \"persistentvolumes\", \"persistentvolumeclaims\", \"pods\", \"services\", \"statefulsets\"\n  # resource_exclude = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## Optional Resources to include when gathering\n  ## Overrides resource_exclude if both set.\n  # resource_include = [ \"deployments\", \"nodes\", \"statefulsets\" ]\n\n  ## selectors to include and exclude as tags.  Globs accepted.\n  ## Note that an empty array for both will include all selectors as tags\n  ## selector_exclude overrides selector_include if both set.\n  # selector_include = []\n  # selector_exclude = [\"*\"]\n\n  ## Optional TLS Config\n  ## Trusted root certificates for server\n  # tls_ca = \"/path/to/cafile\"\n  ## Used for TLS client certificate authentication\n  # tls_cert = \"/path/to/certfile\"\n  ## Used for TLS client certificate authentication\n  # tls_key = \"/path/to/keyfile\"\n  ## Send the specified TLS server name via SNI\n  # tls_server_name = \"kubernetes.example.com\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Uncomment to remove deprecated metrics.\n  # fielddrop = [\"terminated_reason\"]\n"
	},
	{
		"Name": "kubernetes",
		"Description": " Read metrics from the kubernetes kubelet api",
		"SampleConfig": "[[inputs.kubernetes]]\n  ## URL for the kubelet\n  url = \"http://127.0.0.1:10255\"\n\n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  ## If both of these are empty, we'll use the default serviceaccount:\n  ## at: /run/secrets/kubernetes.io/serviceaccount/token\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n\n  ## Pod labels to be added as tags.  An empty array for both include and\n  ## exclude will include all labels.\n  # label_include = []\n  # label_exclude = [\"*\"]\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "lanz",
		"Description": " Read metrics off Arista LANZ, via socket",
		"SampleConfig": "[[inputs.lanz]]\n  ## URL to Arista LANZ endpoint\n  servers = [\n    \"tcp://switch1.int.example.com:50001\",\n    \"tcp://switch2.int.example.com:50001\",\n  ]\n"
	},
	{
		"Name": "leofs",
		"Description": " Read metrics from a LeoFS Server via SNMP",
		"SampleConfig": "[[inputs.leofs]]\n  ## An array of URLs of the form:\n  ##   host [ \":\" port]\n  servers = [\"127.0.0.1:4010\"]\n"
	},
	{
		"Name": "linux_sysctl_fs",
		"Description": " Provides Linux sysctl fs metrics",
		"SampleConfig": "[[inputs.linux_sysctl_fs]]\n  # no configuration\n"
	},
	{
		"Name": "logparser",
		"Description": "# DEPRECATED: The 'logparser' plugin is deprecated in version 1.15.0, use 'inputs.tail' with 'grok' data format instead.",
		"SampleConfig": "# Read metrics off Arista LANZ, via socket\n[[inputs.logparser]]\n  ## Log files to parse.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   /var/log/**.log     -\u003e recursively find all .log files in /var/log\n  ##   /var/log/*/*.log    -\u003e find all .log files with a parent dir in /var/log\n  ##   /var/log/apache.log -\u003e only tail the apache log file\n  files = [\"/var/log/apache/access.log\"]\n\n  ## Read files that currently exist from the beginning. Files that are created\n  ## while telegraf is running (and that match the \"files\" globs) will always\n  ## be read from the beginning.\n  from_beginning = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  # watch_method = \"inotify\"\n\n  ## Parse logstash-style \"grok\" patterns:\n  [inputs.logparser.grok]\n    ## This is a list of patterns to check the given log file(s) for.\n    ## Note that adding patterns here increases processing time. The most\n    ## efficient configuration is to have one pattern per logparser.\n    ## Other common built-in patterns are:\n    ##   %{COMMON_LOG_FORMAT}   (plain apache \u0026 nginx access logs)\n    ##   %{COMBINED_LOG_FORMAT} (access logs + referrer \u0026 agent)\n    patterns = [\"%{COMBINED_LOG_FORMAT}\"]\n\n    ## Name of the outputted measurement name.\n    measurement = \"apache_access_log\"\n\n    ## Full path(s) to custom pattern files.\n    custom_pattern_files = []\n\n    ## Custom patterns can also be defined here. Put one pattern per line.\n    custom_patterns = '''\n    '''\n\n    ## Timezone allows you to provide an override for timestamps that\n    ## don't already include an offset\n    ## e.g. 04/06/2016 12:41:45 data one two 5.43µs\n    ##\n    ## Default: \"\" which renders UTC\n    ## Options are as follows:\n    ##   1. Local             -- interpret based on machine localtime\n    ##   2. \"Canada/Eastern\"  -- Unix TZ values like those found in https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n    ##   3. UTC               -- or blank/unspecified, will return timestamp in UTC\n    # timezone = \"Canada/Eastern\"\n\n    ## When set to \"disable\", timestamp will not incremented if there is a\n    ## duplicate.\n    # unique_timestamp = \"auto\"\n"
	},
	{
		"Name": "logstash",
		"Description": " Read metrics exposed by Logstash",
		"SampleConfig": "[[inputs.logstash]]\n  ## The URL of the exposed Logstash API endpoint.\n  url = \"http://127.0.0.1:9600\"\n\n  ## Use Logstash 5 single pipeline API, set to true when monitoring\n  ## Logstash 5.\n  # single_pipeline = false\n\n  ## Enable optional collection components.  Can contain\n  ## \"pipelines\", \"process\", and \"jvm\".\n  # collect = [\"pipelines\", \"process\", \"jvm\"]\n\n  ## Timeout for HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials.\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Use TLS but skip chain \u0026 host verification.\n  # insecure_skip_verify = false\n\n  ## Optional HTTP headers.\n  # [inputs.logstash.headers]\n  #   \"X-Special-Header\" = \"Special-Value\"\n"
	},
	{
		"Name": "lustre2",
		"Description": " Read metrics from local Lustre service on OST, MDS",
		"SampleConfig": "[[inputs.lustre2]]\n  ## An array of /proc globs to search for Lustre stats\n  ## If not specified, the default will work on Lustre 2.5.x\n  ##\n  # ost_procfiles = [\n  #   \"/proc/fs/lustre/obdfilter/*/stats\",\n  #   \"/proc/fs/lustre/osd-ldiskfs/*/stats\",\n  #   \"/proc/fs/lustre/obdfilter/*/job_stats\",\n  #   \"/proc/fs/lustre/obdfilter/*/exports/*/stats\",\n  # ]\n  # mds_procfiles = [\n  #   \"/proc/fs/lustre/mdt/*/md_stats\",\n  #   \"/proc/fs/lustre/mdt/*/job_stats\",\n  #   \"/proc/fs/lustre/mdt/*/exports/*/stats\",\n  # ]\n"
	},
	{
		"Name": "lvm",
		"Description": " Read metrics about LVM physical volumes, volume groups, logical volumes.",
		"SampleConfig": "[[inputs.lvm]]\n  ## Use sudo to run LVM commands\n  use_sudo = false\n"
	},
	{
		"Name": "mailchimp",
		"Description": " Gathers metrics from the /3.0/reports MailChimp API",
		"SampleConfig": "[[inputs.mailchimp]]\n  ## MailChimp API key\n  ## get from https://admin.mailchimp.com/account/api/\n  api_key = \"\" # required\n\n  ## Reports for campaigns sent more than days_old ago will not be collected.\n  ## 0 means collect all and is the default value.\n  days_old = 0\n\n  ## Campaign ID to get, if empty gets all campaigns, this option overrides days_old\n  # campaign_id = \"\"\n"
	},
	{
		"Name": "marklogic",
		"Description": " Retrieves information on a specific host in a MarkLogic Cluster",
		"SampleConfig": "[[inputs.marklogic]]\n  ## Base URL of the MarkLogic HTTP Server.\n  url = \"http://localhost:8002\"\n\n  ## List of specific hostnames to retrieve information. At least (1) required.\n  # hosts = [\"hostname1\", \"hostname2\"]\n\n  ## Using HTTP Basic Authentication. Management API requires 'manage-user' role privileges\n  # username = \"myuser\"\n  # password = \"mypassword\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "mcrouter",
		"Description": " Read metrics from one or many mcrouter servers.",
		"SampleConfig": "[[inputs.mcrouter]]\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie tcp://localhost:11211, tcp://10.0.0.1:11211, etc.\n  servers = [\"tcp://localhost:11211\", \"unix:///var/run/mcrouter.sock\"]\n\n  ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "mdstat",
		"Description": " Get kernel statistics from /proc/mdstat",
		"SampleConfig": "[[inputs.mdstat]]\n  ## Sets file path\n  ## If not specified, then default is /proc/mdstat\n  # file_name = \"/proc/mdstat\"\n"
	},
	{
		"Name": "mem",
		"Description": " Read metrics about memory usage",
		"SampleConfig": "[[inputs.mem]]\n  # no configuration\n"
	},
	{
		"Name": "memcached",
		"Description": " Read metrics from one or many memcached servers.",
		"SampleConfig": "[[inputs.memcached]]\n  # An array of address to gather stats about. Specify an ip on hostname\n  # with optional port. ie localhost, 10.0.0.1:11211, etc.\n  servers = [\"localhost:11211\"]\n  # An array of unix memcached sockets to gather stats about.\n  # unix_sockets = [\"/var/run/memcached.sock\"]\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n"
	},
	{
		"Name": "mesos",
		"Description": " Telegraf plugin for gathering metrics from N Mesos masters",
		"SampleConfig": "[[inputs.mesos]]\n  ## Timeout, in ms.\n  timeout = 100\n\n  ## A list of Mesos masters.\n  masters = [\"http://localhost:5050\"]\n\n  ## Master metrics groups to be collected, by default, all enabled.\n  master_collections = [\n    \"resources\",\n    \"master\",\n    \"system\",\n    \"agents\",\n    \"frameworks\",\n    \"framework_offers\",\n    \"tasks\",\n    \"messages\",\n    \"evqueue\",\n    \"registrar\",\n    \"allocator\",\n  ]\n\n  ## A list of Mesos slaves, default is []\n  # slaves = []\n\n  ## Slave metrics groups to be collected, by default, all enabled.\n  # slave_collections = [\n  #   \"resources\",\n  #   \"agent\",\n  #   \"system\",\n  #   \"executors\",\n  #   \"tasks\",\n  #   \"messages\",\n  # ]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "minecraft",
		"Description": " Collects scores from a Minecraft server's scoreboard using the RCON protocol",
		"SampleConfig": "[[inputs.minecraft]]\n  ## Address of the Minecraft server.\n  # server = \"localhost\"\n\n  ## Server RCON Port.\n  # port = \"25575\"\n\n  ## Server RCON Password.\n  password = \"\"\n\n  ## Uncomment to remove deprecated metric components.\n  # tagdrop = [\"server\"]\n"
	},
	{
		"Name": "mock",
		"Description": " Generate metrics for test and demonstration purposes",
		"SampleConfig": "[[inputs.mock]]\n  ## Set the metric name to use for reporting\n  metric_name = \"mock\"\n\n  ## Optional string key-value pairs of tags to add to all metrics\n  # [inputs.mock.tags]\n  # \"key\" = \"value\"\n\n  ## One or more mock data fields *must* be defined.\n  ##\n  ## [[inputs.mock.constant]]\n  ##   name = \"constant\"\n  ##   value = value_of_any_type\n  ## [[inputs.mock.random]]\n  ##   name = \"rand\"\n  ##   min = 1.0\n  ##   max = 6.0\n  ## [[inputs.mock.sine_wave]]\n  ##   name = \"wave\"\n  ##   amplitude = 1.0\n  ##   period = 0.5\n  ## [[inputs.mock.step]]\n  ##   name = \"plus_one\"\n  ##   start = 0.0\n  ##   step = 1.0\n  ## [[inputs.mock.stock]]\n  ##   name = \"abc\"\n  ##   price = 50.00\n  ##   volatility = 0.2\n"
	},
	{
		"Name": "modbus",
		"Description": " Retrieve data from MODBUS slave devices",
		"SampleConfig": "[[inputs.modbus]]\n  ## Connection Configuration\n  ##\n  ## The plugin supports connections to PLCs via MODBUS/TCP, RTU over TCP, ASCII over TCP or\n  ## via serial line communication in binary (RTU) or readable (ASCII) encoding\n  ##\n  ## Device name\n  name = \"Device\"\n\n  ## Slave ID - addresses a MODBUS device on the bus\n  ## Range: 0 - 255 [0 = broadcast; 248 - 255 = reserved]\n  slave_id = 1\n\n  ## Timeout for each request\n  timeout = \"1s\"\n\n  ## Maximum number of retries and the time to wait between retries\n  ## when a slave-device is busy.\n  # busy_retries = 0\n  # busy_retries_wait = \"100ms\"\n\n  # TCP - connect via Modbus/TCP\n  controller = \"tcp://localhost:502\"\n\n  ## Serial (RS485; RS232)\n  # controller = \"file:///dev/ttyUSB0\"\n  # baud_rate = 9600\n  # data_bits = 8\n  # parity = \"N\"\n  # stop_bits = 1\n\n  ## For Modbus over TCP you can choose between \"TCP\", \"RTUoverTCP\" and \"ASCIIoverTCP\"\n  ## default behaviour is \"TCP\" if the controller is TCP\n  ## For Serial you can choose between \"RTU\" and \"ASCII\"\n  # transmission_mode = \"RTU\"\n\n  ## Trace the connection to the modbus device as debug messages\n  ## Note: You have to enable telegraf's debug mode to see those messages!\n  # debug_connection = false\n\n  ## Define the configuration schema\n  ##  |---register -- define fields per register type in the original style (only supports one slave ID)\n  ##  |---request  -- define fields on a requests base\n  configuration_type = \"register\"\n\n  ## --- \"register\" configuration style ---\n\n  ## Measurements\n  ##\n\n  ## Digital Variables, Discrete Inputs and Coils\n  ## measurement - the (optional) measurement name, defaults to \"modbus\"\n  ## name        - the variable name\n  ## address     - variable address\n\n  discrete_inputs = [\n    { name = \"start\",          address = [0]},\n    { name = \"stop\",           address = [1]},\n    { name = \"reset\",          address = [2]},\n    { name = \"emergency_stop\", address = [3]},\n  ]\n  coils = [\n    { name = \"motor1_run\",     address = [0]},\n    { name = \"motor1_jog\",     address = [1]},\n    { name = \"motor1_stop\",    address = [2]},\n  ]\n\n  ## Analog Variables, Input Registers and Holding Registers\n  ## measurement - the (optional) measurement name, defaults to \"modbus\"\n  ## name        - the variable name\n  ## byte_order  - the ordering of bytes\n  ##  |---AB, ABCD   - Big Endian\n  ##  |---BA, DCBA   - Little Endian\n  ##  |---BADC       - Mid-Big Endian\n  ##  |---CDAB       - Mid-Little Endian\n  ## data_type   - INT16, UINT16, INT32, UINT32, INT64, UINT64,\n  ##               FLOAT32-IEEE, FLOAT64-IEEE (the IEEE 754 binary representation)\n  ##               FLOAT32, FIXED, UFIXED (fixed-point representation on input)\n  ## scale       - the final numeric variable representation\n  ## address     - variable address\n\n  holding_registers = [\n    { name = \"power_factor\", byte_order = \"AB\",   data_type = \"FIXED\", scale=0.01,  address = [8]},\n    { name = \"voltage\",      byte_order = \"AB\",   data_type = \"FIXED\", scale=0.1,   address = [0]},\n    { name = \"energy\",       byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [5,6]},\n    { name = \"current\",      byte_order = \"ABCD\", data_type = \"FIXED\", scale=0.001, address = [1,2]},\n    { name = \"frequency\",    byte_order = \"AB\",   data_type = \"UFIXED\", scale=0.1,  address = [7]},\n    { name = \"power\",        byte_order = \"ABCD\", data_type = \"UFIXED\", scale=0.1,  address = [3,4]},\n  ]\n  input_registers = [\n    { name = \"tank_level\",   byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [0]},\n    { name = \"tank_ph\",      byte_order = \"AB\",   data_type = \"INT16\",   scale=1.0,     address = [1]},\n    { name = \"pump1_speed\",  byte_order = \"ABCD\", data_type = \"INT32\",   scale=1.0,     address = [3,4]},\n  ]\n\n  ## --- \"request\" configuration style ---\n\n  ## Per request definition\n  ##\n\n  ## Define a request sent to the device\n  ## Multiple of those requests can be defined. Data will be collated into metrics at the end of data collection.\n  [[inputs.modbus.request]]\n    ## ID of the modbus slave device to query.\n    ## If you need to query multiple slave-devices, create several \"request\" definitions.\n    slave_id = 1\n\n    ## Byte order of the data.\n    ##  |---ABCD -- Big Endian (Motorola)\n    ##  |---DCBA -- Little Endian (Intel)\n    ##  |---BADC -- Big Endian with byte swap\n    ##  |---CDAB -- Little Endian with byte swap\n    byte_order = \"ABCD\"\n\n    ## Type of the register for the request\n    ## Can be \"coil\", \"discrete\", \"holding\" or \"input\"\n    register = \"coil\"\n\n    ## Name of the measurement.\n    ## Can be overriden by the individual field definitions. Defaults to \"modbus\"\n    # measurement = \"modbus\"\n\n    ## Field definitions\n    ## Analog Variables, Input Registers and Holding Registers\n    ## address        - address of the register to query. For coil and discrete inputs this is the bit address.\n    ## name *1        - field name\n    ## type *1,2      - type of the modbus field, can be INT16, UINT16, INT32, UINT32, INT64, UINT64 and\n    ##                  FLOAT32, FLOAT64 (IEEE 754 binary representation)\n    ## scale *1,2     - (optional) factor to scale the variable with\n    ## output *1,2    - (optional) type of resulting field, can be INT64, UINT64 or FLOAT64. Defaults to FLOAT64 if\n    ##                  \"scale\" is provided and to the input \"type\" class otherwise (i.e. INT* -\u003e INT64, etc).\n    ## measurement *1 - (optional) measurement name, defaults to the setting of the request\n    ## omit           - (optional) omit this field. Useful to leave out single values when querying many registers\n    ##                  with a single request. Defaults to \"false\".\n    ##\n    ## *1: Those fields are ignored if field is omitted (\"omit\"=true)\n    ##\n    ## *2: Thise fields are ignored for both \"coil\" and \"discrete\"-input type of registers. For those register types\n    ##     the fields are output as zero or one in UINT64 format by default.\n\n    ## Coil / discrete input example\n    fields = [\n      { address=0, name=\"motor1_run\"},\n      { address=1, name=\"jog\", measurement=\"motor\"},\n      { address=2, name=\"motor1_stop\", omit=true},\n      { address=3, name=\"motor1_overheating\"},\n    ]\n\n    [[inputs.modbus.request.tags]]\n      machine = \"impresser\"\n      location = \"main building\"\n\n  [[inputs.modbus.request]]\n    ## Holding example\n    ## All of those examples will result in FLOAT64 field outputs\n    slave_id = 1\n    byte_order = \"DCBA\"\n    register = \"holding\"\n    fields = [\n      { address=0, name=\"voltage\",      type=\"INT16\",   scale=0.1   },\n      { address=1, name=\"current\",      type=\"INT32\",   scale=0.001 },\n      { address=3, name=\"power\",        type=\"UINT32\",  omit=true   },\n      { address=5, name=\"energy\",       type=\"FLOAT32\", scale=0.001, measurement=\"W\" },\n      { address=7, name=\"frequency\",    type=\"UINT32\",  scale=0.1   },\n      { address=8, name=\"power_factor\", type=\"INT64\",   scale=0.01  },\n    ]\n\n    [[inputs.modbus.request.tags]]\n      machine = \"impresser\"\n      location = \"main building\"\n\n  [[inputs.modbus.request]]\n    ## Input example with type conversions\n    slave_id = 1\n    byte_order = \"ABCD\"\n    register = \"input\"\n    fields = [\n      { address=0, name=\"rpm\",         type=\"INT16\"                   },  # will result in INT64 field\n      { address=1, name=\"temperature\", type=\"INT16\", scale=0.1        },  # will result in FLOAT64 field\n      { address=2, name=\"force\",       type=\"INT32\", output=\"FLOAT64\" },  # will result in FLOAT64 field\n      { address=4, name=\"hours\",       type=\"UINT32\"                  },  # will result in UIN64 field\n    ]\n\n    [[inputs.modbus.request.tags]]\n      machine = \"impresser\"\n      location = \"main building\"\n\n  ## Enable workarounds required by some devices to work correctly\n  # [inputs.modbus.workarounds]\n    ## Pause between read requests sent to the device. This might be necessary for (slow) serial devices.\n    # pause_between_requests = \"0ms\"\n    ## Close the connection after every gather cycle. Usually the plugin closes the connection after a certain\n    ## idle-timeout, however, if you query a device with limited simultaneous connectivity (e.g. serial devices)\n    ## from multiple instances you might want to only stay connected during gather and disconnect afterwards.\n    # close_connection_after_gather = false\n"
	},
	{
		"Name": "mongodb",
		"Description": " Read metrics from one or many MongoDB servers",
		"SampleConfig": "[[inputs.mongodb]]\n  ## An array of URLs of the form:\n  ##   \"mongodb://\" [user \":\" pass \"@\"] host [ \":\" port]\n  ## For example:\n  ##   mongodb://user:auth_key@10.10.3.30:27017,\n  ##   mongodb://10.10.3.33:18832,\n  servers = [\"mongodb://127.0.0.1:27017/?connect=direct\"]\n\n  ## When true, collect cluster status.\n  ## Note that the query that counts jumbo chunks triggers a COLLSCAN, which\n  ## may have an impact on performance.\n  # gather_cluster_status = true\n\n  ## When true, collect per database stats\n  # gather_perdb_stats = false\n\n  ## When true, collect per collection stats\n  # gather_col_stats = false\n\n  ## When true, collect usage statistics for each collection\n  ## (insert, update, queries, remove, getmore, commands etc...).\n  # gather_top_stat = false\n\n  ## List of db where collections stats are collected\n  ## If empty, all db are concerned\n  # col_stats_dbs = [\"local\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "monit",
		"Description": " Read metrics and status information about processes managed by Monit",
		"SampleConfig": "[[inputs.monit]]\n  ## Monit HTTPD address\n  address = \"http://127.0.0.1:2812\"\n\n  ## Username and Password for Monit\n  # username = \"\"\n  # password = \"\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "mqtt_consumer",
		"Description": " Read metrics from MQTT topic(s)",
		"SampleConfig": "[[inputs.mqtt_consumer]]\n  ## Broker URLs for the MQTT server or cluster.  To connect to multiple\n  ## clusters or standalone servers, use a separate plugin instance.\n  ##   example: servers = [\"tcp://localhost:1883\"]\n  ##            servers = [\"ssl://localhost:1883\"]\n  ##            servers = [\"ws://localhost:1883\"]\n  servers = [\"tcp://127.0.0.1:1883\"]\n\n  ## Topics that will be subscribed to.\n  topics = [\n    \"telegraf/host01/cpu\",\n    \"telegraf/+/mem\",\n    \"sensors/#\",\n  ]\n\n  ## The message topic will be stored in a tag specified by this value.  If set\n  ## to the empty string no topic tag will be created.\n  # topic_tag = \"topic\"\n\n  ## QoS policy for messages\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  ##\n  ## When using a QoS of 1 or 2, you should enable persistent_session to allow\n  ## resuming unacknowledged messages.\n  # qos = 0\n\n  ## Connection timeout for initial connection in seconds\n  # connection_timeout = \"30s\"\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Persistent session disables clearing of the client session on connection.\n  ## In order for this option to work you must also set client_id to identify\n  ## the client.  To receive messages that arrived while the client is offline,\n  ## also set the qos option to 1 or 2 and don't forget to also set the QoS when\n  ## publishing.\n  # persistent_session = false\n\n  ## If unset, a random client ID will be generated.\n  # client_id = \"\"\n\n  ## Username and password to connect MQTT server.\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Enable extracting tag values from MQTT topics\n  ## _ denotes an ignored entry in the topic path\n  # [[inputs.mqtt_consumer.topic_parsing]]\n  #   topic = \"\"\n  #   measurement = \"\"\n  #   tags = \"\"\n  #   fields = \"\"\n  ## Value supported is int, float, unit\n  #   [[inputs.mqtt_consumer.topic.types]]\n  #      key = type\n"
	},
	{
		"Name": "multifile",
		"Description": " Aggregates the contents of multiple files into a single point",
		"SampleConfig": "[[inputs.multifile]]\n  ## Base directory where telegraf will look for files.\n  ## Omit this option to use absolute paths.\n  base_dir = \"/sys/bus/i2c/devices/1-0076/iio:device0\"\n\n  ## If true discard all data when a single file can't be read.\n  ## Else, Telegraf omits the field generated from this file.\n  # fail_early = true\n\n  ## Files to parse each interval.\n  [[inputs.multifile.file]]\n    file = \"in_pressure_input\"\n    dest = \"pressure\"\n    conversion = \"float\"\n  [[inputs.multifile.file]]\n    file = \"in_temp_input\"\n    dest = \"temperature\"\n    conversion = \"float(3)\"\n  [[inputs.multifile.file]]\n    file = \"in_humidityrelative_input\"\n    dest = \"humidityrelative\"\n    conversion = \"float(3)\"\n"
	},
	{
		"Name": "mysql",
		"Description": " Read metrics from one or many mysql servers",
		"SampleConfig": "[[inputs.mysql]]\n  ## specify servers via a url matching:\n  ##  [username[:password]@][protocol[(address)]]/[?tls=[true|false|skip-verify|custom]]\n  ##  see https://github.com/go-sql-driver/mysql#dsn-data-source-name\n  ##  e.g.\n  ##    servers = [\"user:passwd@tcp(127.0.0.1:3306)/?tls=false\"]\n  ##    servers = [\"user@tcp(127.0.0.1:3306)/?tls=false\"]\n  #\n  ## If no servers are specified, then localhost is used as the host.\n  servers = [\"tcp(127.0.0.1:3306)/\"]\n\n  ## Selects the metric output format.\n  ##\n  ## This option exists to maintain backwards compatibility, if you have\n  ## existing metrics do not set or change this value until you are ready to\n  ## migrate to the new format.\n  ##\n  ## If you do not have existing metrics from this plugin set to the latest\n  ## version.\n  ##\n  ## Telegraf \u003e=1.6: metric_version = 2\n  ##           \u003c1.6: metric_version = 1 (or unset)\n  metric_version = 2\n\n  ## if the list is empty, then metrics are gathered from all database tables\n  # table_schema_databases = []\n\n  ## gather metrics from INFORMATION_SCHEMA.TABLES for databases provided above list\n  # gather_table_schema = false\n\n  ## gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST\n  # gather_process_list = false\n\n  ## gather user statistics from INFORMATION_SCHEMA.USER_STATISTICS\n  # gather_user_statistics = false\n\n  ## gather auto_increment columns and max values from information schema\n  # gather_info_schema_auto_inc = false\n\n  ## gather metrics from INFORMATION_SCHEMA.INNODB_METRICS\n  # gather_innodb_metrics = false\n\n  ## gather metrics from all channels from SHOW SLAVE STATUS command output\n  # gather_all_slave_channels = false\n\n  ## gather metrics from SHOW SLAVE STATUS command output\n  # gather_slave_status = false\n\n  ## use SHOW ALL SLAVES STATUS command output for MariaDB\n  # mariadb_dialect = false\n\n  ## gather metrics from SHOW BINARY LOGS command output\n  # gather_binary_logs = false\n\n  ## gather metrics from SHOW GLOBAL VARIABLES command output\n  # gather_global_variables = true\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE\n  # gather_table_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS\n  # gather_table_lock_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE\n  # gather_index_io_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS\n  # gather_event_waits = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME\n  # gather_file_events_stats = false\n\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST\n  # gather_perf_events_statements             = false\n  #\n  ## gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_ACCOUNT_BY_EVENT_NAME\n  # gather_perf_sum_per_acc_per_event         = false\n  #\n  ## list of events to be gathered for gather_perf_sum_per_acc_per_event\n  ## in case of empty list all events will be gathered\n  # perf_summary_events                       = []\n  #\n  # gather_perf_events_statements = false\n\n  ## the limits for metrics form perf_events_statements\n  # perf_events_statements_digest_text_limit = 120\n  # perf_events_statements_limit = 250\n  # perf_events_statements_time_limit = 86400\n\n  ## Some queries we may want to run less often (such as SHOW GLOBAL VARIABLES)\n  ##   example: interval_slow = \"30m\"\n  # interval_slow = \"\"\n\n  ## Optional TLS Config (will be used if tls=custom parameter specified in server uri)\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "nats",
		"Description": " Provides metrics about the state of a NATS server",
		"SampleConfig": "[[inputs.nats]]\n  ## The address of the monitoring endpoint of the NATS server\n  server = \"http://localhost:8222\"\n\n  ## Maximum time to receive response\n  # response_timeout = \"5s\"\n"
	},
	{
		"Name": "nats_consumer",
		"Description": " Read metrics from NATS subject(s)",
		"SampleConfig": "[[inputs.nats_consumer]]\n  ## urls of NATS servers\n  servers = [\"nats://localhost:4222\"]\n\n  ## subject(s) to consume\n  subjects = [\"telegraf\"]\n\n  ## name a queue group\n  queue_group = \"telegraf_consumers\"\n\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional NATS 2.0 and NATS NGS compatible user credentials\n  # credentials = \"/etc/telegraf/nats.creds\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Sets the limits for pending msgs and bytes for each subscription\n  ## These shouldn't need to be adjusted except in very high throughput scenarios\n  # pending_message_limit = 65536\n  # pending_bytes_limit = 67108864\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "neptune_apex",
		"Description": " Neptune Apex data collector",
		"SampleConfig": "[[inputs.neptune_apex]]\n  ## The Neptune Apex plugin reads the publicly available status.xml data from a local Apex.\n  ## Measurements will be logged under \"apex\".\n\n  ## The base URL of the local Apex(es). If you specify more than one server, they will\n  ## be differentiated by the \"source\" tag.\n  servers = [\n    \"http://apex.local\",\n  ]\n\n  ## The response_timeout specifies how long to wait for a reply from the Apex.\n  #response_timeout = \"5s\"\n\n"
	},
	{
		"Name": "net",
		"Description": " Gather metrics about network interfaces",
		"SampleConfig": "[[inputs.net]]\n  ## By default, telegraf gathers stats from any up interface (excluding loopback)\n  ## Setting interfaces will tell it to gather these explicit interfaces,\n  ## regardless of status. When specifying an interface, glob-style\n  ## patterns are also supported.\n  ##\n  # interfaces = [\"eth*\", \"enp0s[0-1]\", \"lo\"]\n  ##\n  ## On linux systems telegraf also collects protocol stats.\n  ## Setting ignore_protocol_stats to true will skip reporting of protocol metrics.\n  ##\n  # ignore_protocol_stats = false\n  ##\n"
	},
	{
		"Name": "net_response",
		"Description": " Collect response time of a TCP or UDP connection",
		"SampleConfig": "[[inputs.net_response]]\n  ## Protocol, must be \"tcp\" or \"udp\"\n  ## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n  ## a send/expect string pair (see below).\n  protocol = \"tcp\"\n  ## Server address (default localhost)\n  address = \"localhost:80\"\n\n  ## Set timeout\n  # timeout = \"1s\"\n\n  ## Set read timeout (only used if expecting a response)\n  # read_timeout = \"1s\"\n\n  ## The following options are required for UDP checks. For TCP, they are\n  ## optional. The plugin will send the given string to the server and then\n  ## expect to receive the given 'expect' string back.\n  ## string sent to the server\n  # send = \"ssh\"\n  ## expected string in answer\n  # expect = \"ssh\"\n\n  ## Uncomment to remove deprecated fields; recommended for new deploys\n  # fielddrop = [\"result_type\", \"string_found\"]\n"
	},
	{
		"Name": "netstat",
		"Description": " Read TCP metrics such as established, time wait and sockets counts.",
		"SampleConfig": "[[inputs.netstat]]\n  # no configuration\n"
	},
	{
		"Name": "nfsclient",
		"Description": " Read per-mount NFS client metrics from /proc/self/mountstats",
		"SampleConfig": "[[inputs.nfsclient]]\n  ## Read more low-level metrics (optional, defaults to false)\n  # fullstat = false\n\n  ## List of mounts to explictly include or exclude (optional)\n  ## The pattern (Go regexp) is matched against the mount point (not the\n  ## device being mounted).  If include_mounts is set, all mounts are ignored\n  ## unless present in the list. If a mount is listed in both include_mounts\n  ## and exclude_mounts, it is excluded.  Go regexp patterns can be used.\n  # include_mounts = []\n  # exclude_mounts = []\n\n  ## List of operations to include or exclude from collecting.  This applies\n  ## only when fullstat=true.  Symantics are similar to {include,exclude}_mounts:\n  ## the default is to collect everything; when include_operations is set, only\n  ## those OPs are collected; when exclude_operations is set, all are collected\n  ## except those listed.  If include and exclude are set, the OP is excluded.\n  ## See /proc/self/mountstats for a list of valid operations; note that\n  ## NFSv3 and NFSv4 have different lists.  While it is not possible to\n  ## have different include/exclude lists for NFSv3/4, unused elements\n  ## in the list should be okay.  It is possible to have different lists\n  ## for different mountpoints:  use mulitple [[input.nfsclient]] stanzas,\n  ## with their own lists.  See \"include_mounts\" above, and be careful of\n  ## duplicate metrics.\n  # include_operations = []\n  # exclude_operations = []\n"
	},
	{
		"Name": "nginx",
		"Description": " Read Nginx's basic status information (ngx_http_stub_status_module)",
		"SampleConfig": "[[inputs.nginx]]\n  ## An array of Nginx stub_status URI to gather stats.\n  urls = [\"http://localhost/server_status\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n"
	},
	{
		"Name": "nginx_plus",
		"Description": " Read Nginx Plus' advanced status information",
		"SampleConfig": "[[inputs.nginx_plus]]\n  ## An array of Nginx status URIs to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "nginx_plus_api",
		"Description": " Read Nginx Plus API advanced status information",
		"SampleConfig": "[[inputs.nginx_plus_api]]\n  ## An array of Nginx API URIs to gather stats.\n  urls = [\"http://localhost/api\"]\n  # Nginx API version, default: 3\n  # api_version = 3\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "nginx_sts",
		"Description": " Read Nginx virtual host traffic status module information (nginx-module-sts)",
		"SampleConfig": "[[inputs.nginx_sts]]\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "nginx_upstream_check",
		"Description": " Read nginx_upstream_check module status information (https://github.com/yaoweibin/nginx_upstream_check_module)",
		"SampleConfig": "[[inputs.nginx_upstream_check]]\n  ## An URL where Nginx Upstream check module is enabled\n  ## It should be set to return a JSON formatted response\n  url = \"http://127.0.0.1/status?format=json\"\n\n  ## HTTP method\n  # method = \"GET\"\n\n  ## Optional HTTP headers\n  # headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## Override HTTP \"Host\" header\n  # host_header = \"check.example.com\"\n\n  ## Timeout for HTTP requests\n  timeout = \"5s\"\n\n  ## Optional HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "nginx_vts",
		"Description": " Read Nginx virtual host traffic status module information (nginx-module-vts)",
		"SampleConfig": "[[inputs.nginx_vts]]\n  ## An array of ngx_http_status_module or status URI to gather stats.\n  urls = [\"http://localhost/status\"]\n\n  ## HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "nomad",
		"Description": " Read metrics from the Nomad API",
		"SampleConfig": "[[inputs.nomad]]\n  ## URL for the Nomad agent\n  # url = \"http://127.0.0.1:4646\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n"
	},
	{
		"Name": "nsd",
		"Description": " A plugin to collect stats from the NSD DNS resolver",
		"SampleConfig": "[[inputs.nsd]]\n  ## Address of server to connect to, optionally ':port'. Defaults to the\n  ## address in the nsd config file.\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the nsd-control binary can be overridden with:\n  # binary = \"/usr/sbin/nsd-control\"\n\n  ## The default location of the nsd config file can be overridden with:\n  # config_file = \"/etc/nsd/nsd.conf\"\n\n  ## The default timeout of 1s can be overridden with:\n  # timeout = \"1s\"\n"
	},
	{
		"Name": "nsq",
		"Description": " Read NSQ topic and channel statistics.",
		"SampleConfig": "[[inputs.nsq]]\n  ## An array of NSQD HTTP API endpoints\n  endpoints  = [\"http://localhost:4151\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "nsq_consumer",
		"Description": " Read metrics from NSQD topic(s)",
		"SampleConfig": "[[inputs.nsq_consumer]]\n  ## Server option still works but is deprecated, we just prepend it to the nsqd array.\n  # server = \"localhost:4150\"\n\n  ## An array representing the NSQD TCP HTTP Endpoints\n  nsqd = [\"localhost:4150\"]\n\n  ## An array representing the NSQLookupd HTTP Endpoints\n  nsqlookupd = [\"localhost:4161\"]\n  topic = \"telegraf\"\n  channel = \"consumer\"\n  max_in_flight = 100\n\n  ## Maximum messages to read from the broker that have not been written by an\n  ## output.  For best throughput set based on the number of metrics within\n  ## each message and the size of the output's metric_batch_size.\n  ##\n  ## For example, if each message from the queue contains 10 metrics and the\n  ## output metric_batch_size is 1000, setting this to 100 will ensure that a\n  ## full batch is collected and the write is triggered immediately without\n  ## waiting until the next flush_interval.\n  # max_undelivered_messages = 1000\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "nstat",
		"Description": " Collect kernel snmp counters and network interface statistics",
		"SampleConfig": "[[inputs.nstat]]\n  ## file paths for proc files. If empty default paths will be used:\n  ##    /proc/net/netstat, /proc/net/snmp, /proc/net/snmp6\n  ## These can also be overridden with env variables, see README.\n  proc_net_netstat = \"/proc/net/netstat\"\n  proc_net_snmp = \"/proc/net/snmp\"\n  proc_net_snmp6 = \"/proc/net/snmp6\"\n  ## dump metrics with 0 values too\n  dump_zeros       = true\n"
	},
	{
		"Name": "ntpq",
		"Description": " Get standard NTP query metrics, requires ntpq executable.",
		"SampleConfig": "[[inputs.ntpq]]\n  ## If false, set the -n ntpq flag. Can reduce metric gather time.\n  dns_lookup = true\n"
	},
	{
		"Name": "nvidia_smi",
		"Description": " Pulls statistics from nvidia GPUs attached to the host",
		"SampleConfig": "[[inputs.nvidia_smi]]\n  ## Optional: path to nvidia-smi binary, defaults \"/usr/bin/nvidia-smi\"\n  ## We will first try to locate the nvidia-smi binary with the explicitly specified value (or default value), \n  ## if it is not found, we will try to locate it on PATH(exec.LookPath), if it is still not found, an error will be returned\n  # bin_path = \"/usr/bin/nvidia-smi\"\n\n  ## Optional: timeout for GPU polling\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "opcua",
		"Description": " Retrieve data from OPCUA devices",
		"SampleConfig": "[[inputs.opcua]]\n  ## Metric name\n  # name = \"opcua\"\n  #\n  ## OPC UA Endpoint URL\n  # endpoint = \"opc.tcp://localhost:4840\"\n  #\n  ## Maximum time allowed to establish a connect to the endpoint.\n  # connect_timeout = \"10s\"\n  #\n  ## Maximum time allowed for a request over the estabilished connection.\n  # request_timeout = \"5s\"\n  #\n  ## Security policy, one of \"None\", \"Basic128Rsa15\", \"Basic256\",\n  ## \"Basic256Sha256\", or \"auto\"\n  # security_policy = \"auto\"\n  #\n  ## Security mode, one of \"None\", \"Sign\", \"SignAndEncrypt\", or \"auto\"\n  # security_mode = \"auto\"\n  #\n  ## Path to cert.pem. Required when security mode or policy isn't \"None\".\n  ## If cert path is not supplied, self-signed cert and key will be generated.\n  # certificate = \"/etc/telegraf/cert.pem\"\n  #\n  ## Path to private key.pem. Required when security mode or policy isn't \"None\".\n  ## If key path is not supplied, self-signed cert and key will be generated.\n  # private_key = \"/etc/telegraf/key.pem\"\n  #\n  ## Authentication Method, one of \"Certificate\", \"UserName\", or \"Anonymous\".  To\n  ## authenticate using a specific ID, select 'Certificate' or 'UserName'\n  # auth_method = \"Anonymous\"\n  #\n  ## Username. Required for auth_method = \"UserName\"\n  # username = \"\"\n  #\n  ## Password. Required for auth_method = \"UserName\"\n  # password = \"\"\n  #\n  ## Option to select the metric timestamp to use. Valid options are:\n  ##     \"gather\" -- uses the time of receiving the data in telegraf\n  ##     \"server\" -- uses the timestamp provided by the server\n  ##     \"source\" -- uses the timestamp provided by the source\n  # timestamp = \"gather\"\n  #\n  ## Node ID configuration\n  ## name              - field name to use in the output\n  ## namespace         - OPC UA namespace of the node (integer value 0 thru 3)\n  ## identifier_type   - OPC UA ID type (s=string, i=numeric, g=guid, b=opaque)\n  ## identifier        - OPC UA ID (tag as shown in opcua browser)\n  ## tags              - extra tags to be added to the output metric (optional)\n  ## Example:\n  ## {name=\"ProductUri\", namespace=\"0\", identifier_type=\"i\", identifier=\"2262\", tags=[[\"tag1\",\"value1\"],[\"tag2\",\"value2]]}\n  # nodes = [\n  #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #]\n  #\n  ## Node Group\n  ## Sets defaults for OPC UA namespace and ID type so they aren't required in\n  ## every node.  A group can also have a metric name that overrides the main\n  ## plugin metric name.\n  ##\n  ## Multiple node groups are allowed\n  #[[inputs.opcua.group]]\n  ## Group Metric name. Overrides the top level name.  If unset, the\n  ## top level name is used.\n  # name =\n  #\n  ## Group default namespace. If a node in the group doesn't set its\n  ## namespace, this is used.\n  # namespace =\n  #\n  ## Group default identifier type. If a node in the group doesn't set its\n  ## namespace, this is used.\n  # identifier_type =\n  #\n  ## Node ID Configuration.  Array of nodes with the same settings as above.\n  # nodes = [\n  #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #  {name=\"\", namespace=\"\", identifier_type=\"\", identifier=\"\"},\n  #]\n\n  ## Enable workarounds required by some devices to work correctly\n  # [inputs.opcua.workarounds]\n    ## Set additional valid status codes, StatusOK (0x0) is always considered valid\n    # additional_valid_status_codes = [\"0xC0\"]\n"
	},
	{
		"Name": "openldap",
		"Description": " OpenLDAP cn=Monitor plugin",
		"SampleConfig": "[[inputs.openldap]]\n  host = \"localhost\"\n  port = 389\n\n  # ldaps, starttls, or no encryption. default is an empty string, disabling all encryption.\n  # note that port will likely need to be changed to 636 for ldaps\n  # valid options: \"\" | \"starttls\" | \"ldaps\"\n  tls = \"\"\n\n  # skip peer certificate verification. Default is false.\n  insecure_skip_verify = false\n\n  # Path to PEM-encoded Root certificate to use to verify server certificate\n  tls_ca = \"/etc/ssl/certs.pem\"\n\n  # dn/password to bind with. If bind_dn is empty, an anonymous bind is performed.\n  bind_dn = \"\"\n  bind_password = \"\"\n\n  # reverse metric names so they sort more naturally\n  # Defaults to false if unset, but is set to true when generating a new config\n  reverse_metric_names = true\n"
	},
	{
		"Name": "openntpd",
		"Description": " Get standard NTP query metrics from OpenNTPD.",
		"SampleConfig": "[[inputs.openntpd]]\n  ## Run ntpctl binary with sudo.\n  # use_sudo = false\n\n  ## Location of the ntpctl binary.\n  # binary = \"/usr/sbin/ntpctl\"\n\n  ## Maximum time the ntpctl binary is allowed to run.\n  # timeout = \"5ms\"\n"
	},
	{
		"Name": "opensmtpd",
		"Description": " A plugin to collect stats from Opensmtpd - a validating, recursive, and caching DNS resolver",
		"SampleConfig": " [[inputs.opensmtpd]]\n   ## If running as a restricted user you can prepend sudo for additional access:\n   #use_sudo = false\n\n   ## The default location of the smtpctl binary can be overridden with:\n   binary = \"/usr/sbin/smtpctl\"\n\n   # The default timeout of 1s can be overridden with:\n   #timeout = \"1s\"\n"
	},
	{
		"Name": "openstack",
		"Description": " Collects performance metrics from OpenStack services",
		"SampleConfig": "[[inputs.openstack]]\n  ## The recommended interval to poll is '30m'\n\n  ## The identity endpoint to authenticate against and get the service catalog from.\n  authentication_endpoint = \"https://my.openstack.cloud:5000\"\n\n  ## The domain to authenticate against when using a V3 identity endpoint.\n  # domain = \"default\"\n\n  ## The project to authenticate as.\n  # project = \"admin\"\n\n  ## User authentication credentials. Must have admin rights.\n  username = \"admin\"\n  password = \"password\"\n\n  ## Available services are:\n  ## \"agents\", \"aggregates\", \"flavors\", \"hypervisors\", \"networks\", \"nova_services\",\n  ## \"ports\", \"projects\", \"servers\", \"services\", \"stacks\", \"storage_pools\", \"subnets\", \"volumes\"\n  # enabled_services = [\"services\", \"projects\", \"hypervisors\", \"flavors\", \"networks\", \"volumes\"]\n\n  ## Collect Server Diagnostics\n  # server_diagnotics = false\n\n  ## output secrets (such as adminPass(for server) and UserID(for volume)).\n  # output_secrets = false\n\n  ## Amount of time allowed to complete the HTTP(s) request.\n  # timeout = \"5s\"\n\n  ## HTTP Proxy support\n  # http_proxy_url = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Options for tags received from Openstack\n  # tag_prefix = \"openstack_tag_\"\n  # tag_value = \"true\"\n\n  ## Timestamp format for timestamp data recieved from Openstack.\n  ## If false format is unix nanoseconds.\n  # human_readable_timestamps = false\n\n  ## Measure Openstack call duration\n  # measure_openstack_requests = false\n"
	},
	{
		"Name": "opentelemetry",
		"Description": " Receive OpenTelemetry traces, metrics, and logs over gRPC",
		"SampleConfig": "[[inputs.opentelemetry]]\n  ## Override the default (0.0.0.0:4317) destination OpenTelemetry gRPC service\n  ## address:port\n  # service_address = \"0.0.0.0:4317\"\n\n  ## Override the default (5s) new connection timeout\n  # timeout = \"5s\"\n\n  ## Override the default (prometheus-v1) metrics schema.\n  ## Supports: \"prometheus-v1\", \"prometheus-v2\"\n  ## For more information about the alternatives, read the Prometheus input\n  ## plugin notes.\n  # metrics_schema = \"prometheus-v1\"\n\n  ## Optional TLS Config.\n  ## For advanced options: https://github.com/influxdata/telegraf/blob/v1.18.3/docs/TLS.md\n  ##\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n  ## Add service certificate and key.\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n"
	},
	{
		"Name": "openweathermap",
		"Description": " Read current weather and forecasts data from openweathermap.org",
		"SampleConfig": "[[inputs.openweathermap]]\n  ## OpenWeatherMap API key.\n  app_id = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n  ## City ID's to collect weather data from.\n  city_id = [\"5391959\"]\n\n  ## Language of the description field. Can be one of \"ar\", \"bg\",\n  ## \"ca\", \"cz\", \"de\", \"el\", \"en\", \"fa\", \"fi\", \"fr\", \"gl\", \"hr\", \"hu\",\n  ## \"it\", \"ja\", \"kr\", \"la\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\",\n  ## \"se\", \"sk\", \"sl\", \"es\", \"tr\", \"ua\", \"vi\", \"zh_cn\", \"zh_tw\"\n  # lang = \"en\"\n\n  ## APIs to fetch; can contain \"weather\" or \"forecast\".\n  fetch = [\"weather\", \"forecast\"]\n\n  ## OpenWeatherMap base URL\n  # base_url = \"https://api.openweathermap.org/\"\n\n  ## Timeout for HTTP response.\n  # response_timeout = \"5s\"\n\n  ## Preferred unit system for temperature and wind speed. Can be one of\n  ## \"metric\", \"imperial\", or \"standard\".\n  # units = \"metric\"\n\n  ## Query interval; OpenWeatherMap weather data is updated every 10\n  ## minutes.\n  interval = \"10m\"\n"
	},
	{
		"Name": "passenger",
		"Description": " Read metrics of passenger using passenger-status",
		"SampleConfig": "[[inputs.passenger]]\n  ## Path of passenger-status.\n  ##\n  ## Plugin gather metric via parsing XML output of passenger-status\n  ## More information about the tool:\n  ##   https://www.phusionpassenger.com/library/admin/apache/overall_status_report.html\n  ##\n  ## If no path is specified, then the plugin simply execute passenger-status\n  ## hopefully it can be found in your PATH\n  command = \"passenger-status -v --show=xml\"\n"
	},
	{
		"Name": "pf",
		"Description": " Gather counters from PF",
		"SampleConfig": "[[inputs.pf]]\n  ## PF require root access on most systems.\n  ## Setting 'use_sudo' to true will make use of sudo to run pfctl.\n  ## Users must configure sudo to allow telegraf user to run pfctl with no password.\n  ## pfctl can be restricted to only list command \"pfctl -s info\".\n  use_sudo = false\n"
	},
	{
		"Name": "pgbouncer",
		"Description": " Read metrics from one or many pgbouncer servers",
		"SampleConfig": "[[inputs.pgbouncer]]\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@host:port[/dbname]\\\n  ##       ?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost port=5432 user=pqgotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  address = \"host=localhost user=pgbouncer sslmode=disable\"\n"
	},
	{
		"Name": "phpfpm",
		"Description": " Read metrics of phpfpm, via HTTP status page or socket",
		"SampleConfig": "[[inputs.phpfpm]]\n  ## An array of addresses to gather stats about. Specify an ip or hostname\n  ## with optional port and path\n  ##\n  ## Plugin can be configured in three modes (either can be used):\n  ##   - http: the URL must start with http:// or https://, ie:\n  ##       \"http://localhost/status\"\n  ##       \"http://192.168.130.1/status?full\"\n  ##\n  ##   - unixsocket: path to fpm socket, ie:\n  ##       \"/var/run/php5-fpm.sock\"\n  ##      or using a custom fpm status path:\n  ##       \"/var/run/php5-fpm.sock:fpm-custom-status-path\"\n  ##      glob patterns are also supported:\n  ##       \"/var/run/php*.sock\"\n  ##\n  ##   - fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n  ##       \"fcgi://10.0.0.12:9000/status\"\n  ##       \"cgi://10.0.10.12:9001/status\"\n  ##\n  ## Example of multiple gathering from local socket and remote host\n  ## urls = [\"http://192.168.1.20/status\", \"/tmp/fpm.sock\"]\n  urls = [\"http://localhost/status\"]\n\n  ## Duration allowed to complete HTTP requests.\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "ping",
		"Description": " Ping given url(s) and return statistics",
		"SampleConfig": "[[inputs.ping]]\n  ## Hosts to send ping packets to.\n  urls = [\"example.org\"]\n\n  ## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n  ## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n  ## the plugin will send pings directly.\n  ##\n  ## While the default is \"exec\" for backwards compatibility, new deployments\n  ## are encouraged to use the \"native\" method for improved compatibility and\n  ## performance.\n  # method = \"exec\"\n\n  ## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n  ## option of the ping command.\n  # count = 1\n\n  ## Time to wait between sending ping packets in seconds.  Operates like the\n  ## \"-i\" option of the ping command.\n  # ping_interval = 1.0\n\n  ## If set, the time to wait for a ping response in seconds.  Operates like\n  ## the \"-W\" option of the ping command.\n  # timeout = 1.0\n\n  ## If set, the total ping deadline, in seconds.  Operates like the -w option\n  ## of the ping command.\n  # deadline = 10\n\n  ## Interface or source address to send ping from.  Operates like the -I or -S\n  ## option of the ping command.\n  # interface = \"\"\n\n  ## Percentiles to calculate. This only works with the native method.\n  # percentiles = [50, 95, 99]\n\n  ## Specify the ping executable binary.\n  # binary = \"ping\"\n\n  ## Arguments for ping command. When arguments is not empty, the command from\n  ## the binary option will be used and other options (ping_interval, timeout,\n  ## etc) will be ignored.\n  # arguments = [\"-c\", \"3\"]\n\n  ## Use only IPv6 addresses when resolving a hostname.\n  # ipv6 = false\n\n  ## Number of data bytes to be sent. Corresponds to the \"-s\"\n  ## option of the ping command. This only works with the native method.\n  # size = 56\n"
	},
	{
		"Name": "postfix",
		"Description": " Measure postfix queue statistics",
		"SampleConfig": "[[inputs.postfix]]\n  ## Postfix queue directory. If not provided, telegraf will try to use\n  ## 'postconf -h queue_directory' to determine it.\n  # queue_directory = \"/var/spool/postfix\"\n"
	},
	{
		"Name": "postgresql",
		"Description": " Read metrics from one or many postgresql servers",
		"SampleConfig": "[[inputs.postgresql]]\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  ## Without the dbname parameter, the driver will default to a database\n  ## with the same name as the user. This dbname is just for instantiating a\n  ## connection with the server and doesn't restrict the databases we are trying\n  ## to grab metrics for.\n  ##\n  address = \"host=localhost user=postgres sslmode=disable\"\n  ## A custom name for the database that will be used as the \"server\" tag in the\n  ## measurement output. If not specified, a default one generated from\n  ## the connection address is used.\n  # outputaddress = \"db01\"\n\n  ## connection configuration.\n  ## maxlifetime - specify the maximum lifetime of a connection.\n  ## default is forever (0s)\n  # max_lifetime = \"0s\"\n\n  ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the 'databases' option.\n  # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the 'ignored_databases' option.\n  # databases = [\"app_production\", \"testing\"]\n\n  ## Whether to use prepared statements when connecting to the database.\n  ## This should be set to false when connecting through a PgBouncer instance\n  ## with pool_mode set to transaction.\n  prepared_statements = true\n"
	},
	{
		"Name": "postgresql_extensible",
		"Description": " Read metrics from one or many postgresql servers",
		"SampleConfig": "[[inputs.postgresql_extensible]]\n  # specify address via a url matching:\n  # postgres://[pqgotest[:password]]@host:port[/dbname]?sslmode=...\n  # or a simple string:\n  #   host=localhost port=5432 user=pqgotest password=... sslmode=... dbname=app_production\n  #\n  # All connection parameters are optional.\n  # Without the dbname parameter, the driver will default to a database\n  # with the same name as the user. This dbname is just for instantiating a\n  # connection with the server and doesn't restrict the databases we are trying\n  # to grab metrics for.\n  #\n  address = \"host=localhost user=postgres sslmode=disable\"\n\n  ## A list of databases to pull metrics about.\n  ## deprecated in 1.22.3; use the sqlquery option to specify database to use\n  # databases = [\"app_production\", \"testing\"]\n\n  ## Whether to use prepared statements when connecting to the database.\n  ## This should be set to false when connecting through a PgBouncer instance\n  ## with pool_mode set to transaction.\n  prepared_statements = true\n\n  # Define the toml config where the sql queries are stored\n  # The script option can be used to specify the .sql file path.\n  # If script and sqlquery options specified at same time, sqlquery will be used\n  #\n  # the tagvalue field is used to define custom tags (separated by comas).\n  # the query is expected to return columns which match the names of the\n  # defined tags. The values in these columns must be of a string-type,\n  # a number-type or a blob-type.\n  #\n  # The timestamp field is used to override the data points timestamp value. By\n  # default, all rows inserted with current time. By setting a timestamp column,\n  # the row will be inserted with that column's value.\n  #\n  # Structure :\n  # [[inputs.postgresql_extensible.query]]\n  #   sqlquery string\n  #   version string\n  #   withdbname boolean\n  #   tagvalue string (coma separated)\n  #   timestamp string\n  [[inputs.postgresql_extensible.query]]\n    sqlquery=\"SELECT * FROM pg_stat_database where datname\"\n    version=901\n    withdbname=false\n    tagvalue=\"\"\n  [[inputs.postgresql_extensible.query]]\n    script=\"your_sql-filepath.sql\"\n    version=901\n    withdbname=false\n    tagvalue=\"\"\n"
	},
	{
		"Name": "powerdns",
		"Description": " Read metrics from one or many PowerDNS servers",
		"SampleConfig": "[[inputs.powerdns]]\n  # An array of sockets to gather stats about.\n  # Specify a path to unix socket.\n  #\n  # If no servers are specified, then '/var/run/pdns.controlsocket' is used as the path.\n  unix_sockets = [\"/var/run/pdns.controlsocket\"]\n"
	},
	{
		"Name": "powerdns_recursor",
		"Description": " Read metrics from one or many PowerDNS Recursor servers",
		"SampleConfig": "[[inputs.powerdns_recursor]]\n  ## Path to the Recursor control socket.\n  unix_sockets = [\"/var/run/pdns_recursor.controlsocket\"]\n\n  ## Directory to create receive socket.  This default is likely not writable,\n  ## please reference the full plugin documentation for a recommended setup.\n  # socket_dir = \"/var/run/\"\n  ## Socket permissions for the receive socket.\n  # socket_mode = \"0666\"\n"
	},
	{
		"Name": "processes",
		"Description": " Get the number of processes and group them by status",
		"SampleConfig": "[[inputs.processes]]\n  # no configuration\n"
	},
	{
		"Name": "procstat",
		"Description": " Monitor process cpu and memory usage",
		"SampleConfig": "[[inputs.procstat]]\n  ## PID file to monitor process\n  pid_file = \"/var/run/nginx.pid\"\n  ## executable name (ie, pgrep \u003cexe\u003e)\n  # exe = \"nginx\"\n  ## pattern as argument for pgrep (ie, pgrep -f \u003cpattern\u003e)\n  # pattern = \"nginx\"\n  ## user as argument for pgrep (ie, pgrep -u \u003cuser\u003e)\n  # user = \"nginx\"\n  ## Systemd unit name, supports globs when include_systemd_children is set to true\n  # systemd_unit = \"nginx.service\"\n  # include_systemd_children = false\n  ## CGroup name or path, supports globs\n  # cgroup = \"systemd/system.slice/nginx.service\"\n\n  ## Windows service name\n  # win_service = \"\"\n\n  ## override for process_name\n  ## This is optional; default is sourced from /proc/\u003cpid\u003e/status\n  # process_name = \"bar\"\n\n  ## Field name prefix\n  # prefix = \"\"\n\n  ## When true add the full cmdline as a tag.\n  # cmdline_tag = false\n\n  ## Mode to use when calculating CPU usage. Can be one of 'solaris' or 'irix'.\n  # mode = \"irix\"\n\n  ## Add the PID as a tag instead of as a field.  When collecting multiple\n  ## processes with otherwise matching tags this setting should be enabled to\n  ## ensure each process has a unique identity.\n  ##\n  ## Enabling this option may result in a large number of series, especially\n  ## when processes have a short lifetime.\n  # pid_tag = false\n\n  ## Method to use when finding process IDs.  Can be one of 'pgrep', or\n  ## 'native'.  The pgrep finder calls the pgrep executable in the PATH while\n  ## the native finder performs the search directly in a manor dependent on the\n  ## platform.  Default is 'pgrep'\n  # pid_finder = \"pgrep\"\n"
	},
	{
		"Name": "prometheus",
		"Description": " Read metrics from one or many prometheus clients",
		"SampleConfig": "[[inputs.prometheus]]\n  ## An array of urls to scrape metrics from.\n  urls = [\"http://localhost:9100/metrics\"]\n  \n  ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.\n  ## See \"Metric Format Configuration\" in plugins/inputs/prometheus/README.md for details.\n  ## Valid options: 1, 2\n  # metric_version = 1\n  \n  ## Url tag name (tag containing scrapped url. optional, default is \"url\")\n  # url_tag = \"url\"\n  \n  ## Whether the timestamp of the scraped metrics will be ignored.\n  ## If set to true, the gather time will be used.\n  # ignore_timestamp = false\n  \n  ## An array of Kubernetes services to scrape metrics from.\n  # kubernetes_services = [\"http://my-service-dns.my-namespace:9100/metrics\"]\n  \n  ## Kubernetes config file to create client from.\n  # kube_config = \"/path/to/kubernetes.config\"\n  \n  ## Scrape Kubernetes pods for the following prometheus annotations:\n  ## - prometheus.io/scrape: Enable scraping for this pod\n  ## - prometheus.io/scheme: If the metrics endpoint is secured then you will need to\n  ##     set this to 'https' \u0026 most likely set the tls config.\n  ## - prometheus.io/path: If the metrics path is not /metrics, define it with this annotation.\n  ## - prometheus.io/port: If port is not 9102 use this annotation\n  # monitor_kubernetes_pods = true\n  \n  ## Get the list of pods to scrape with either the scope of\n  ## - cluster: the kubernetes watch api (default, no need to specify)\n  ## - node: the local cadvisor api; for scalability. Note that the config node_ip or the environment variable NODE_IP must be set to the host IP.\n  # pod_scrape_scope = \"cluster\"\n  \n  ## Only for node scrape scope: node IP of the node that telegraf is running on.\n  ## Either this config or the environment variable NODE_IP must be set.\n  # node_ip = \"10.180.1.1\"\n \n  ## Only for node scrape scope: interval in seconds for how often to get updated pod list for scraping.\n  ## Default is 60 seconds.\n  # pod_scrape_interval = 60\n  \n  ## Restricts Kubernetes monitoring to a single namespace\n  ##   ex: monitor_kubernetes_pods_namespace = \"default\"\n  # monitor_kubernetes_pods_namespace = \"\"\n  # label selector to target pods which have the label\n  # kubernetes_label_selector = \"env=dev,app=nginx\"\n  # field selector to target pods\n  # eg. To scrape pods on a specific node\n  # kubernetes_field_selector = \"spec.nodeName=$HOSTNAME\"\n\n  # cache refresh interval to set the interval for re-sync of pods list. \n  # Default is 60 minutes.\n  # cache_refresh_interval = 60\n\n  ## Scrape Services available in Consul Catalog\n  # [inputs.prometheus.consul]\n  #   enabled = true\n  #   agent = \"http://localhost:8500\"\n  #   query_interval = \"5m\"\n\n  #   [[inputs.prometheus.consul.query]]\n  #     name = \"a service name\"\n  #     tag = \"a service tag\"\n  #     url = 'http://{{if ne .ServiceAddress \"\"}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}'\n  #     [inputs.prometheus.consul.query.tags]\n  #       host = \"{{.Node}}\"\n  \n  ## Use bearer token for authorization. ('bearer_token' takes priority)\n  # bearer_token = \"/path/to/bearer/token\"\n  ## OR\n  # bearer_token_string = \"abc_123\"\n  \n  ## HTTP Basic Authentication username and password. ('bearer_token' and\n  ## 'bearer_token_string' take priority)\n  # username = \"\"\n  # password = \"\"\n  \n  ## Specify timeout duration for slower prometheus clients (default is 3s)\n  # response_timeout = \"3s\"\n  \n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n  \n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "proxmox",
		"Description": " Provides metrics from Proxmox nodes (Proxmox Virtual Environment \u003e 6.2).",
		"SampleConfig": "[[inputs.proxmox]]\n  ## API connection configuration. The API token was introduced in Proxmox v6.2. Required permissions for user and token: PVEAuditor role on /.\n  base_url = \"https://localhost:8006/api2/json\"\n  api_token = \"USER@REALM!TOKENID=UUID\"\n  ## Node name, defaults to OS hostname\n  # node_name = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  insecure_skip_verify = false\n\n  # HTTP response timeout (default: 5s)\n  response_timeout = \"5s\"\n"
	},
	{
		"Name": "puppetagent",
		"Description": " Reads last_run_summary.yaml file and converts to measurements",
		"SampleConfig": "[[inputs.puppetagent]]\n  ## Location of puppet last run summary file\n  location = \"/var/lib/puppet/state/last_run_summary.yaml\"\n"
	},
	{
		"Name": "rabbitmq",
		"Description": " Reads metrics from RabbitMQ servers via the Management Plugin",
		"SampleConfig": "[[inputs.rabbitmq]]\n  ## Management Plugin url. (default: http://localhost:15672)\n  # url = \"http://localhost:15672\"\n  ## Tag added to rabbitmq_overview series; deprecated: use tags\n  # name = \"rmq-server-1\"\n  ## Credentials\n  # username = \"guest\"\n  # password = \"guest\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional request timeouts\n  ##\n  ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## A list of nodes to gather as the rabbitmq_node measurement. If not\n  ## specified, metrics for all nodes are gathered.\n  # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n  ## A list of queues to gather as the rabbitmq_queue measurement. If not\n  ## specified, metrics for all queues are gathered.\n  ## Deprecated in 1.6: Use queue_name_include instead.\n  # queues = [\"telegraf\"]\n\n  ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n  ## specified, metrics for all exchanges are gathered.\n  # exchanges = [\"telegraf\"]\n\n  ## Metrics to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all metrics\n  ## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n  # metric_include = []\n  # metric_exclude = []\n\n  ## Queues to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all queues\n  # queue_name_include = []\n  # queue_name_exclude = []\n\n  ## Federation upstreams to include and exclude specified as an array of glob\n  ## pattern strings.  Federation links can also be limited by the queue and\n  ## exchange filters.\n  # federation_upstream_include = []\n  # federation_upstream_exclude = []\n"
	},
	{
		"Name": "raindrops",
		"Description": " Read raindrops stats (raindrops - real-time stats for preforking Rack servers)",
		"SampleConfig": "[[inputs.raindrops]]\n  ## An array of raindrops middleware URI to gather stats.\n  urls = [\"http://localhost:8080/_raindrops\"]\n"
	},
	{
		"Name": "ras",
		"Description": " RAS plugin exposes counter metrics for Machine Check Errors provided by RASDaemon (sqlite3 output is required).",
		"SampleConfig": "[[inputs.ras]]\n  ## Optional path to RASDaemon sqlite3 database.\n  ## Default: /var/lib/rasdaemon/ras-mc_event.db\n  # db_path = \"\"\n"
	},
	{
		"Name": "ravendb",
		"Description": " Reads metrics from RavenDB servers via the Monitoring Endpoints",
		"SampleConfig": "[[inputs.ravendb]]\n  ## Node URL and port that RavenDB is listening on. By default,\n  ## attempts to connect securely over HTTPS, however, if the user\n  ## is running a local unsecure development cluster users can use\n  ## HTTP via a URL like \"http://localhost:8080\"\n  url = \"https://localhost:4433\"\n\n  ## RavenDB X509 client certificate setup\n  # tls_cert = \"/etc/telegraf/raven.crt\"\n  # tls_key = \"/etc/telegraf/raven.key\"\n\n  ## Optional request timeout\n  ##\n  ## Timeout, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request and\n  ## time limit for requests made by this client\n  # timeout = \"5s\"\n\n  ## List of statistics which are collected\n  # At least one is required\n  # Allowed values: server, databases, indexes, collections\n  #\n  # stats_include = [\"server\", \"databases\", \"indexes\", \"collections\"]\n\n  ## List of db where database stats are collected\n  ## If empty, all db are concerned\n  # db_stats_dbs = []\n\n  ## List of db where index status are collected\n  ## If empty, all indexes from all db are concerned\n  # index_stats_dbs = []\n\n  ## List of db where collection status are collected\n  ## If empty, all collections from all db are concerned\n  # collection_stats_dbs = []\n"
	},
	{
		"Name": "redfish",
		"Description": " Read CPU, Fans, Powersupply and Voltage metrics of hardware server through redfish APIs",
		"SampleConfig": "[[inputs.redfish]]\n  ## Redfish API Base URL.\n  address = \"https://127.0.0.1:5000\"\n\n  ## Credentials for the Redfish API.\n  username = \"root\"\n  password = \"password123456\"\n\n  ## System Id to collect data for in Redfish APIs.\n  computer_system_id=\"System.Embedded.1\"\n\n  ## Amount of time allowed to complete the HTTP request\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "redis",
		"Description": " Read metrics from one or many redis servers",
		"SampleConfig": "[[inputs.redis]]\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:6379\n  ##    tcp://:password@192.168.99.100\n  ##    unix:///var/run/redis.sock\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 6379 is used\n  servers = [\"tcp://localhost:6379\"]\n\n  ## Optional. Specify redis commands to retrieve values\n  # [[inputs.redis.commands]]\n  #   # The command to run where each argument is a separate element\n  #   command = [\"get\", \"sample-key\"]\n  #   # The field to store the result in\n  #   field = \"sample-key-value\"\n  #   # The type of the result\n  #   # Can be \"string\", \"integer\", or \"float\"\n  #   type = \"string\"\n\n  ## specify server password\n  # password = \"s#cr@t%\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n"
	},
	{
		"Name": "redis_sentinel",
		"Description": " Read metrics from one or many redis-sentinel servers",
		"SampleConfig": "[[inputs.redis_sentinel]]\n  ## specify servers via a url matching:\n  ##  [protocol://][:password]@address[:port]\n  ##  e.g.\n  ##    tcp://localhost:26379\n  ##    tcp://:password@192.168.99.100\n  ##    unix:///var/run/redis-sentinel.sock\n  ##\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 26379 is used\n  # servers = [\"tcp://localhost:26379\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = true\n"
	},
	{
		"Name": "rethinkdb",
		"Description": " Read metrics from one or many RethinkDB servers",
		"SampleConfig": "[[inputs.rethinkdb]]\n  ## An array of URI to gather stats about. Specify an ip or hostname\n  ## with optional port add password. ie,\n  ##   rethinkdb://user:auth_key@10.10.3.30:28105,\n  ##   rethinkdb://10.10.3.33:18832,\n  ##   10.0.0.1:10000, etc.\n  servers = [\"127.0.0.1:28015\"]\n\n  ## If you use actual rethinkdb of \u003e 2.3.0 with username/password authorization,\n  ## protocol have to be named \"rethinkdb2\" - it will use 1_0 H.\n  # servers = [\"rethinkdb2://username:password@127.0.0.1:28015\"]\n\n  ## If you use older versions of rethinkdb (\u003c2.2) with auth_key, protocol\n  ## have to be named \"rethinkdb\".\n  # servers = [\"rethinkdb://username:auth_key@127.0.0.1:28015\"]\n"
	},
	{
		"Name": "riak",
		"Description": " Read metrics one or many Riak servers",
		"SampleConfig": "[[inputs.riak]]\n  # Specify a list of one or more riak http servers\n  servers = [\"http://localhost:8098\"]\n"
	},
	{
		"Name": "riemann_listener",
		"Description": " Riemann protobuff listener",
		"SampleConfig": "[[inputs.rimann_listener]]\n  ## URL to listen on\n  ## Default is \"tcp://:5555\"\n  #  service_address = \"tcp://:8094\"\n  #  service_address = \"tcp://127.0.0.1:http\"\n  #  service_address = \"tcp4://:8094\"\n  #  service_address = \"tcp6://:8094\"\n  #  service_address = \"tcp6://[2001:db8::1]:8094\"\n\n  ## Maximum number of concurrent connections.\n  ## 0 (default) is unlimited.\n  #  max_connections = 1024\n  ## Read timeout.\n  ## 0 (default) is unlimited.\n  #  read_timeout = \"30s\"\n  ## Optional TLS configuration.\n  #  tls_cert = \"/etc/telegraf/cert.pem\"\n  #  tls_key  = \"/etc/telegraf/key.pem\"\n  ## Enables client authentication if set.\n  #  tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n  ## Maximum socket buffer size (in bytes when no unit specified).\n  #  read_buffer_size = \"64KiB\"\n  ## Period between keep alive probes.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  #  keep_alive_period = \"5m\"\n"
	},
	{
		"Name": "salesforce",
		"Description": " Read API usage and limits for a Salesforce organisation",
		"SampleConfig": "[[inputs.salesforce]]\n  ## specify your credentials\n  ##\n  username = \"your_username\"\n  password = \"your_password\"\n  ##\n  ## (optional) security token\n  # security_token = \"your_security_token\"\n  ##\n  ## (optional) environment type (sandbox or production)\n  ## default is: production\n  ##\n  # environment = \"production\"\n  ##\n  ## (optional) API version (default: \"39.0\")\n  ##\n  # version = \"39.0\"\n"
	},
	{
		"Name": "sensors",
		"Description": " Monitor sensors, requires lm-sensors package",
		"SampleConfig": "[[inputs.sensors]]\n  ## Remove numbers from field names.\n  ## If true, a field name like 'temp1_input' will be changed to 'temp_input'.\n  # remove_numbers = true\n\n  ## Timeout is the maximum amount of time that the sensors command can run.\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "sflow",
		"Description": " SFlow V5 Protocol Listener",
		"SampleConfig": "[[inputs.sflow]]\n  ## Address to listen for sFlow packets.\n  ##   example: service_address = \"udp://:6343\"\n  ##            service_address = \"udp4://:6343\"\n  ##            service_address = \"udp6://:6343\"\n  service_address = \"udp://:6343\"\n\n  ## Set the size of the operating system's receive buffer.\n  ##   example: read_buffer_size = \"64KiB\"\n  # read_buffer_size = \"\"\n"
	},
	{
		"Name": "slab",
		"Description": " Get slab statistics from procfs",
		"SampleConfig": "[[inputs.slab]]\n  # no configuration - please see the plugin's README for steps to configure\n  # sudo properly\n"
	},
	{
		"Name": "smart",
		"Description": " Read metrics from storage devices supporting S.M.A.R.T.",
		"SampleConfig": "[[inputs.smart]]\n    ## Optionally specify the path to the smartctl executable\n    # path_smartctl = \"/usr/bin/smartctl\"\n  \n    ## Optionally specify the path to the nvme-cli executable\n    # path_nvme = \"/usr/bin/nvme\"\n  \n    ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n    ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n    ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n    # enable_extensions = [\"auto-on\"]\n  \n    ## On most platforms used cli utilities requires root access.\n    ## Setting 'use_sudo' to true will make use of sudo to run smartctl or nvme-cli.\n    ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli\n    ## without a password.\n    # use_sudo = false\n  \n    ## Skip checking disks in this power mode. Defaults to\n    ## \"standby\" to not wake up disks that have stopped rotating.\n    ## See --nocheck in the man pages for smartctl.\n    ## smartctl version 5.41 and 5.42 have faulty detection of\n    ## power mode and might require changing this value to\n    ## \"never\" depending on your disks.\n    # nocheck = \"standby\"\n  \n    ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n    ## information from each drive into the 'smart_attribute' measurement.\n    # attributes = false\n  \n    ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n    # excludes = [ \"/dev/pass6\" ]\n  \n    ## Optionally specify devices and device type, if unset\n    ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n    ## and all found will be included except for the excluded in excludes.\n    # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n  \n    ## Timeout for the cli command to complete.\n    # timeout = \"30s\"\n    \n    ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n    ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n    ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n    ## SMART data - one individual array drive at the time. In such case please set this configuration option\n    ## to \"sequential\" to get readings for all drives.\n    ## valid options: concurrent, sequential\n    # read_method = \"concurrent\"\n"
	},
	{
		"Name": "snmp",
		"Description": " Retrieves SNMP values from remote agents",
		"SampleConfig": "[[inputs.snmp]]\n  ## Agent addresses to retrieve values from.\n  ##   format:  agents = [\"\u003cscheme://\u003e\u003chostname\u003e:\u003cport\u003e\"]\n  ##   scheme:  optional, either udp, udp4, udp6, tcp, tcp4, tcp6.\n  ##            default is udp\n  ##   port:    optional\n  ##   example: agents = [\"udp://127.0.0.1:161\"]\n  ##            agents = [\"tcp://127.0.0.1:161\"]\n  ##            agents = [\"udp4://v4only-snmp-agent\"]\n  agents = [\"udp://127.0.0.1:161\"]\n\n  ## Timeout for each request.\n  # timeout = \"5s\"\n\n  ## SNMP version; can be 1, 2, or 3.\n  # version = 2\n\n  ## Path to mib files\n  ## Used by the gosmi translator.\n  ## To add paths when translating with netsnmp, use the MIBDIRS environment variable\n  # path = [\"/usr/share/snmp/mibs\"]\n\n  ## SNMP community string.\n  # community = \"public\"\n\n  ## Agent host tag\n  # agent_host_tag = \"agent_host\"\n\n  ## Number of retries to attempt.\n  # retries = 3\n\n  ## The GETBULK max-repetitions parameter.\n  # max_repetitions = 10\n\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\", \"SHA224\", \"SHA256\", \"SHA384\", \"SHA512\" or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Context Name.\n  # context_name = \"\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\", or \"\".\n  ### Protocols \"AES192\", \"AES192\", \"AES256\", and \"AES256C\" require the underlying net-snmp tools\n  ### to be compiled with --enable-blumenthal-aes (http://www.net-snmp.org/docs/INSTALL.html)\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n\n  ## Add fields and tables defining the variables you wish to collect.  This\n  ## example collects the system uptime and interface variables.  Reference the\n  ## full plugin documentation for configuration details.\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysUpTime.0\"\n    name = \"uptime\"\n\n  [[inputs.snmp.field]]\n    oid = \"RFC1213-MIB::sysName.0\"\n    name = \"source\"\n    is_tag = true\n\n  [[inputs.snmp.table]]\n    oid = \"IF-MIB::ifTable\"\n    name = \"interface\"\n    inherit_tags = [\"source\"]\n\n    [[inputs.snmp.table.field]]\n      oid = \"IF-MIB::ifDescr\"\n      name = \"ifDescr\"\n      is_tag = true\n"
	},
	{
		"Name": "snmp_legacy",
		"Description": " DEPRECATED! PLEASE USE inputs.snmp INSTEAD.",
		"SampleConfig": "[[inputs.snmp_legacy]]\n  ## Use 'oids.txt' file to translate oids to names\n  ## To generate 'oids.txt' you need to run:\n  ##   snmptranslate -m all -Tz -On | sed -e 's/\"//g' \u003e /tmp/oids.txt\n  ## Or if you have an other MIB folder with custom MIBs\n  ##   snmptranslate -M /mycustommibfolder -Tz -On -m all | sed -e 's/\"//g' \u003e oids.txt\n  snmptranslate_file = \"/tmp/oids.txt\"\n  [[inputs.snmp.host]]\n    address = \"192.168.2.2:161\"\n    # SNMP community\n    community = \"public\" # default public\n    # SNMP version (1, 2 or 3)\n    # Version 3 not supported yet\n    version = 2 # default 2\n    # SNMP response timeout\n    timeout = 2.0 # default 2.0\n    # SNMP request retries\n    retries = 2 # default 2\n    # Which get/bulk do you want to collect for this host\n    collect = [\"mybulk\", \"sysservices\", \"sysdescr\"]\n    # Simple list of OIDs to get, in addition to \"collect\"\n    get_oids = []\n  [[inputs.snmp.host]]\n    address = \"192.168.2.3:161\"\n    community = \"public\"\n    version = 2\n    timeout = 2.0\n    retries = 2\n    collect = [\"mybulk\"]\n    get_oids = [\n        \"ifNumber\",\n        \".1.3.6.1.2.1.1.3.0\",\n    ]\n  [[inputs.snmp.get]]\n    name = \"ifnumber\"\n    oid = \"ifNumber\"\n  [[inputs.snmp.get]]\n    name = \"interface_speed\"\n    oid = \"ifSpeed\"\n    instance = \"0\"\n  [[inputs.snmp.get]]\n    name = \"sysuptime\"\n    oid = \".1.3.6.1.2.1.1.3.0\"\n    unit = \"second\"\n  [[inputs.snmp.bulk]]\n    name = \"mybulk\"\n    max_repetition = 127\n    oid = \".1.3.6.1.2.1.1\"\n  [[inputs.snmp.bulk]]\n    name = \"ifoutoctets\"\n    max_repetition = 127\n    oid = \"ifOutOctets\"\n  [[inputs.snmp.host]]\n    address = \"192.168.2.13:161\"\n    #address = \"127.0.0.1:161\"\n    community = \"public\"\n    version = 2\n    timeout = 2.0\n    retries = 2\n    #collect = [\"mybulk\", \"sysservices\", \"sysdescr\", \"systype\"]\n    collect = [\"sysuptime\" ]\n    [[inputs.snmp.host.table]]\n      name = \"iftable3\"\n      include_instances = [\"enp5s0\", \"eth1\"]\n  # SNMP TABLEs\n  # table without mapping neither subtables\n  [[inputs.snmp.table]]\n    name = \"iftable1\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n  # table without mapping but with subtables\n  [[inputs.snmp.table]]\n    name = \"iftable2\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    sub_tables = [\".1.3.6.1.2.1.2.2.1.13\"]\n  # table with mapping but without subtables\n  [[inputs.snmp.table]]\n    name = \"iftable3\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    # if empty. get all instances\n    mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n    # if empty, get all subtables\n  # table with both mapping and subtables\n  [[inputs.snmp.table]]\n    name = \"iftable4\"\n    oid = \".1.3.6.1.2.1.31.1.1.1\"\n    # if empty get all instances\n    mapping_table = \".1.3.6.1.2.1.31.1.1.1.1\"\n    # if empty get all subtables\n    # sub_tables could be not \"real subtables\"\n    sub_tables=[\".1.3.6.1.2.1.2.2.1.13\", \"bytes_recv\", \"bytes_send\"]\n"
	},
	{
		"Name": "snmp_trap",
		"Description": " Receive SNMP traps",
		"SampleConfig": "[[inputs.snmp_trap]]\n  ## Transport, local address, and port to listen on.  Transport must\n  ## be \"udp://\".  Omit local address to listen on all interfaces.\n  ##   example: \"udp://127.0.0.1:1234\"\n  ##\n  ## Special permissions may be required to listen on a port less than\n  ## 1024.  See README.md for details\n  ##\n  # service_address = \"udp://:162\"\n  ##\n  ## Path to mib files\n  ## Used by the gosmi translator.\n  ## To add paths when translating with netsnmp, use the MIBDIRS environment variable\n  # path = [\"/usr/share/snmp/mibs\"]\n  ##\n  ## Deprecated in 1.20.0; no longer running snmptranslate\n  ## Timeout running snmptranslate command\n  # timeout = \"5s\"\n  ## Snmp version\n  # version = \"2c\"\n  ## SNMPv3 authentication and encryption options.\n  ##\n  ## Security Name.\n  # sec_name = \"myuser\"\n  ## Authentication protocol; one of \"MD5\", \"SHA\" or \"\".\n  # auth_protocol = \"MD5\"\n  ## Authentication password.\n  # auth_password = \"pass\"\n  ## Security Level; one of \"noAuthNoPriv\", \"authNoPriv\", or \"authPriv\".\n  # sec_level = \"authNoPriv\"\n  ## Privacy protocol used for encrypted messages; one of \"DES\", \"AES\", \"AES192\", \"AES192C\", \"AES256\", \"AES256C\" or \"\".\n  # priv_protocol = \"\"\n  ## Privacy password used for encrypted messages.\n  # priv_password = \"\"\n"
	},
	{
		"Name": "socket_listener",
		"Description": " Generic socket listener capable of handling multiple socket types.",
		"SampleConfig": "[[inputs.socket_listener]]\n  ## URL to listen on\n  # service_address = \"tcp://:8094\"\n  # service_address = \"tcp://127.0.0.1:http\"\n  # service_address = \"tcp4://:8094\"\n  # service_address = \"tcp6://:8094\"\n  # service_address = \"tcp6://[2001:db8::1]:8094\"\n  # service_address = \"udp://:8094\"\n  # service_address = \"udp4://:8094\"\n  # service_address = \"udp6://:8094\"\n  # service_address = \"unix:///tmp/telegraf.sock\"\n  # service_address = \"unixgram:///tmp/telegraf.sock\"\n\n  ## Change the file mode bits on unix sockets.  These permissions may not be\n  ## respected by some platforms, to safely restrict write permissions it is best\n  ## to place the socket into a directory that has previously been created\n  ## with the desired permissions.\n  ##   ex: socket_mode = \"777\"\n  # socket_mode = \"\"\n\n  ## Maximum number of concurrent connections.\n  ## Only applies to stream sockets (e.g. TCP).\n  ## 0 (default) is unlimited.\n  # max_connections = 1024\n\n  ## Read timeout.\n  ## Only applies to stream sockets (e.g. TCP).\n  ## 0 (default) is unlimited.\n  # read_timeout = \"30s\"\n\n  ## Optional TLS configuration.\n  ## Only applies to stream sockets (e.g. TCP).\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key  = \"/etc/telegraf/key.pem\"\n  ## Enables client authentication if set.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Maximum socket buffer size (in bytes when no unit specified).\n  ## For stream sockets, once the buffer fills up, the sender will start backing up.\n  ## For datagram sockets, once the buffer fills up, metrics will start dropping.\n  ## Defaults to the OS default.\n  # read_buffer_size = \"64KiB\"\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  # data_format = \"influx\"\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n"
	},
	{
		"Name": "socketstat",
		"Description": " Gather indicators from established connections, using iproute2's ss command.",
		"SampleConfig": "[[inputs.socketstat]]\n  ## ss can display information about tcp, udp, raw, unix, packet, dccp and sctp sockets\n  ## Specify here the types you want to gather\n  socket_types = [ \"tcp\", \"udp\" ]\n  ## The default timeout of 1s for ss execution can be overridden here:\n  # timeout = \"1s\"\n"
	},
	{
		"Name": "solr",
		"Description": " Read stats from one or more Solr servers or cores",
		"SampleConfig": "[[inputs.solr]]\n  ## specify a list of one or more Solr servers\n  servers = [\"http://localhost:8983\"]\n  ##\n  ## specify a list of one or more Solr cores (default - all)\n  # cores = [\"main\"]\n  ##\n  ## Optional HTTP Basic Auth Credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n"
	},
	{
		"Name": "sql",
		"Description": " Read metrics from SQL queries",
		"SampleConfig": "[[inputs.sql]]\n  ## Database Driver\n  ## See https://github.com/influxdata/telegraf/blob/master/docs/SQL_DRIVERS_INPUT.md for\n  ## a list of supported drivers.\n  driver = \"mysql\"\n\n  ## Data source name for connecting\n  ## The syntax and supported options depends on selected driver.\n  dsn = \"username:password@mysqlserver:3307/dbname?param=value\"\n\n  ## Timeout for any operation\n  ## Note that the timeout for queries is per query not per gather.\n  # timeout = \"5s\"\n\n  ## Connection time limits\n  ## By default the maximum idle time and maximum lifetime of a connection is unlimited, i.e. the connections\n  ## will not be closed automatically. If you specify a positive time, the connections will be closed after\n  ## idleing or existing for at least that amount of time, respectively.\n  # connection_max_idle_time = \"0s\"\n  # connection_max_life_time = \"0s\"\n\n  ## Connection count limits\n  ## By default the number of open connections is not limited and the number of maximum idle connections\n  ## will be inferred from the number of queries specified. If you specify a positive number for any of the\n  ## two options, connections will be closed when reaching the specified limit. The number of idle connections\n  ## will be clipped to the maximum number of connections limit if any.\n  # connection_max_open = 0\n  # connection_max_idle = auto\n\n  [[inputs.sql.query]]\n    ## Query to perform on the server\n    query=\"SELECT user,state,latency,score FROM Scoreboard WHERE application \u003e 0\"\n    ## Alternatively to specifying the query directly you can select a file here containing the SQL query.\n    ## Only one of 'query' and 'query_script' can be specified!\n    # query_script = \"/path/to/sql/script.sql\"\n\n    ## Name of the measurement\n    ## In case both measurement and 'measurement_col' are given, the latter takes precedence.\n    # measurement = \"sql\"\n\n    ## Column name containing the name of the measurement\n    ## If given, this will take precedence over the 'measurement' setting. In case a query result\n    ## does not contain the specified column, we fall-back to the 'measurement' setting.\n    # measurement_column = \"\"\n\n    ## Column name containing the time of the measurement\n    ## If ommited, the time of the query will be used.\n    # time_column = \"\"\n\n    ## Format of the time contained in 'time_col'\n    ## The time must be 'unix', 'unix_ms', 'unix_us', 'unix_ns', or a golang time format.\n    ## See https://golang.org/pkg/time/#Time.Format for details.\n    # time_format = \"unix\"\n\n    ## Column names containing tags\n    ## An empty include list will reject all columns and an empty exclude list will not exclude any column.\n    ## I.e. by default no columns will be returned as tag and the tags are empty.\n    # tag_columns_include = []\n    # tag_columns_exclude = []\n\n    ## Column names containing fields (explicit types)\n    ## Convert the given columns to the corresponding type. Explicit type conversions take precedence over\n    ## the automatic (driver-based) conversion below.\n    ## NOTE: Columns should not be specified for multiple types or the resulting type is undefined.\n    # field_columns_float = []\n    # field_columns_int = []\n    # field_columns_uint = []\n    # field_columns_bool = []\n    # field_columns_string = []\n\n    ## Column names containing fields (automatic types)\n    ## An empty include list is equivalent to '[*]' and all returned columns will be accepted. An empty\n    ## exclude list will not exclude any column. I.e. by default all columns will be returned as fields.\n    ## NOTE: We rely on the database driver to perform automatic datatype conversion.\n    # field_columns_include = []\n    # field_columns_exclude = []\n"
	},
	{
		"Name": "sqlserver",
		"Description": " Read metrics from Microsoft SQL Server",
		"SampleConfig": "[[inputs.sqlserver]]\n  ## Specify instances to monitor with a list of connection strings.\n  ## All connection parameters are optional.\n  ## By default, the host is localhost, listening on default port, TCP 1433.\n  ##   for Windows, the user is the currently running AD user (SSO).\n  ##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n  ##   parameters, in particular, tls connections can be created like so:\n  ##   \"encrypt=true;certificate=\u003ccert\u003e;hostNameInCertificate=\u003cSqlServer host fqdn\u003e\"\n  servers = [\n    \"Server=192.168.1.10;Port=1433;User Id=\u003cuser\u003e;Password=\u003cpw\u003e;app name=telegraf;log=1;\",\n  ]\n\n  ## Authentication method\n  ## valid methods: \"connection_string\", \"AAD\"\n  # auth_method = \"connection_string\"\n\n  ## \"database_type\" enables a specific set of queries depending on the database type. If specified, it replaces azuredb = true/false and query_version = 2\n  ## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.\n  ## Possible values for database_type are - \"SQLServer\" or \"AzureSQLDB\" or \"AzureSQLManagedInstance\" or \"AzureSQLPool\"\n\n  database_type = \"SQLServer\"\n\n  ## A list of queries to include. If not specified, all the below listed queries are used.\n  include_query = []\n\n  ## A list of queries to explicitly ignore.\n  exclude_query = [\"SQLServerAvailabilityReplicaStates\", \"SQLServerDatabaseReplicaStates\"]\n\n  ## Queries enabled by default for database_type = \"SQLServer\" are -\n  ## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,\n  ## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates,\n  ## SQLServerRecentBackups\n\n  ## Queries enabled by default for database_type = \"AzureSQLDB\" are -\n  ## AzureSQLDBResourceStats, AzureSQLDBResourceGovernance, AzureSQLDBWaitStats, AzureSQLDBDatabaseIO, AzureSQLDBServerProperties,\n  ## AzureSQLDBOsWaitstats, AzureSQLDBMemoryClerks, AzureSQLDBPerformanceCounters, AzureSQLDBRequests, AzureSQLDBSchedulers\n\n  ## Queries enabled by default for database_type = \"AzureSQLManagedInstance\" are -\n  ## AzureSQLMIResourceStats, AzureSQLMIResourceGovernance, AzureSQLMIDatabaseIO, AzureSQLMIServerProperties, AzureSQLMIOsWaitstats,\n  ## AzureSQLMIMemoryClerks, AzureSQLMIPerformanceCounters, AzureSQLMIRequests, AzureSQLMISchedulers\n\n  ## Queries enabled by default for database_type = \"AzureSQLPool\" are -\n  ## AzureSQLPoolResourceStats, AzureSQLPoolResourceGovernance, AzureSQLPoolDatabaseIO, AzureSQLPoolWaitStats,\n  ## AzureSQLPoolMemoryClerks, AzureSQLPoolPerformanceCounters, AzureSQLPoolSchedulers\n\n  ## Following are old config settings\n  ## You may use them only if you are using the earlier flavor of queries, however it is recommended to use\n  ## the new mechanism of identifying the database_type there by use it's corresponding queries\n\n  ## Optional parameter, setting this to 2 will use a new version\n  ## of the collection queries that break compatibility with the original\n  ## dashboards.\n  ## Version 2 - is compatible from SQL Server 2012 and later versions and also for SQL Azure DB\n  # query_version = 2\n\n  ## If you are using AzureDB, setting this to true will gather resource utilization metrics\n  # azuredb = false\n\n  ## Toggling this to true will emit an additional metric called \"sqlserver_telegraf_health\".\n  ## This metric tracks the count of attempted queries and successful queries for each SQL instance specified in \"servers\".\n  ## The purpose of this metric is to assist with identifying and diagnosing any connectivity or query issues.\n  ## This setting/metric is optional and is disabled by default.\n  # health_metric = false\n\n  ## Possible queries accross different versions of the collectors\n  ## Queries enabled by default for specific Database Type\n\n  ## database_type =  AzureSQLDB  by default collects the following queries\n  ## - AzureSQLDBWaitStats\n  ## - AzureSQLDBResourceStats\n  ## - AzureSQLDBResourceGovernance\n  ## - AzureSQLDBDatabaseIO\n  ## - AzureSQLDBServerProperties\n  ## - AzureSQLDBOsWaitstats\n  ## - AzureSQLDBMemoryClerks\n  ## - AzureSQLDBPerformanceCounters\n  ## - AzureSQLDBRequests\n  ## - AzureSQLDBSchedulers\n\n  ## database_type =  AzureSQLManagedInstance by default collects the following queries\n  ## - AzureSQLMIResourceStats\n  ## - AzureSQLMIResourceGovernance\n  ## - AzureSQLMIDatabaseIO\n  ## - AzureSQLMIServerProperties\n  ## - AzureSQLMIOsWaitstats\n  ## - AzureSQLMIMemoryClerks\n  ## - AzureSQLMIPerformanceCounters\n  ## - AzureSQLMIRequests\n  ## - AzureSQLMISchedulers\n\n  ## database_type =  AzureSQLPool by default collects the following queries\n  ## - AzureSQLPoolResourceStats\n  ## - AzureSQLPoolResourceGovernance\n  ## - AzureSQLPoolDatabaseIO\n  ## - AzureSQLPoolOsWaitStats,\n  ## - AzureSQLPoolMemoryClerks\n  ## - AzureSQLPoolPerformanceCounters\n  ## - AzureSQLPoolSchedulers\n\n  ## database_type =  SQLServer by default collects the following queries\n  ## - SQLServerPerformanceCounters\n  ## - SQLServerWaitStatsCategorized\n  ## - SQLServerDatabaseIO\n  ## - SQLServerProperties\n  ## - SQLServerMemoryClerks\n  ## - SQLServerSchedulers\n  ## - SQLServerRequests\n  ## - SQLServerVolumeSpace\n  ## - SQLServerCpu\n  ## - SQLServerRecentBackups\n  ## and following as optional (if mentioned in the include_query list)\n  ## - SQLServerAvailabilityReplicaStates\n  ## - SQLServerDatabaseReplicaStates\n\n  ## Version 2 by default collects the following queries\n  ## Version 2 is being deprecated, please consider using database_type.\n  ## - PerformanceCounters\n  ## - WaitStatsCategorized\n  ## - DatabaseIO\n  ## - ServerProperties\n  ## - MemoryClerk\n  ## - Schedulers\n  ## - SqlRequests\n  ## - VolumeSpace\n  ## - Cpu\n\n  ## Version 1 by default collects the following queries\n  ## Version 1 is deprecated, please consider using database_type.\n  ## - PerformanceCounters\n  ## - WaitStatsCategorized\n  ## - CPUHistory\n  ## - DatabaseIO\n  ## - DatabaseSize\n  ## - DatabaseStats\n  ## - DatabaseProperties\n  ## - MemoryClerk\n  ## - VolumeSpace\n  ## - PerformanceMetrics\n"
	},
	{
		"Name": "stackdriver",
		"Description": " Gather timeseries from Google Cloud Platform v3 monitoring API",
		"SampleConfig": "[[inputs.stackdriver]]\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## Include timeseries that start with the given metric type.\n  metric_type_prefix_include = [\n    \"compute.googleapis.com/\",\n  ]\n\n  ## Exclude timeseries that start with the given metric type.\n  # metric_type_prefix_exclude = []\n\n  ## Most metrics are updated no more than once per minute; it is recommended\n  ## to override the agent level interval with a value of 1m or greater.\n  interval = \"1m\"\n\n  ## Maximum number of API calls to make per second.  The quota for accounts\n  ## varies, it can be viewed on the API dashboard:\n  ##   https://cloud.google.com/monitoring/quotas#quotas_and_limits\n  # rate_limit = 14\n\n  ## The delay and window options control the number of points selected on\n  ## each gather.  When set, metrics are gathered between:\n  ##   start: now() - delay - window\n  ##   end:   now() - delay\n  #\n  ## Collection delay; if set too low metrics may not yet be available.\n  # delay = \"5m\"\n  #\n  ## If unset, the window will start at 1m and be updated dynamically to span\n  ## the time between calls (approximately the length of the plugin interval).\n  # window = \"1m\"\n\n  ## TTL for cached list of metric types.  This is the maximum amount of time\n  ## it may take to discover new metrics.\n  # cache_ttl = \"1h\"\n\n  ## If true, raw bucket counts are collected for distribution value types.\n  ## For a more lightweight collection, you may wish to disable and use\n  ## distribution_aggregation_aligners instead.\n  # gather_raw_distribution_buckets = true\n\n  ## Aggregate functions to be used for metrics whose value type is\n  ## distribution.  These aggregate values are recorded in in addition to raw\n  ## bucket counts; if they are enabled.\n  ##\n  ## For a list of aligner strings see:\n  ##   https://cloud.google.com/monitoring/api/ref_v3/rpc/google.monitoring.v3#aligner\n  # distribution_aggregation_aligners = [\n  #  \"ALIGN_PERCENTILE_99\",\n  #  \"ALIGN_PERCENTILE_95\",\n  #  \"ALIGN_PERCENTILE_50\",\n  # ]\n\n  ## Filters can be added to reduce the number of time series matched.  All\n  ## functions are supported: starts_with, ends_with, has_substring, and\n  ## one_of.  Only the '=' operator is supported.\n  ##\n  ## The logical operators when combining filters are defined statically using\n  ## the following values:\n  ##   filter ::= \u003cresource_labels\u003e {AND \u003cmetric_labels\u003e}\n  ##   resource_labels ::= \u003cresource_labels\u003e {OR \u003cresource_label\u003e}\n  ##   metric_labels ::= \u003cmetric_labels\u003e {OR \u003cmetric_label\u003e}\n  ##\n  ## For more details, see https://cloud.google.com/monitoring/api/v3/filters\n  #\n  ## Resource labels refine the time series selection with the following expression:\n  ##   resource.labels.\u003ckey\u003e = \u003cvalue\u003e\n  # [[inputs.stackdriver.filter.resource_labels]]\n  #   key = \"instance_name\"\n  #   value = 'starts_with(\"localhost\")'\n  #\n  ## Metric labels refine the time series selection with the following expression:\n  ##   metric.labels.\u003ckey\u003e = \u003cvalue\u003e\n  #  [[inputs.stackdriver.filter.metric_labels]]\n  #    key = \"device_name\"\n  #    value = 'one_of(\"sda\", \"sdb\")'\n"
	},
	{
		"Name": "statsd",
		"Description": " Statsd Server",
		"SampleConfig": "[[inputs.statsd]]\n  ## Protocol, must be \"tcp\", \"udp4\", \"udp6\" or \"udp\" (default=udp)\n  protocol = \"udp\"\n\n  ## MaxTCPConnection - applicable when protocol is set to tcp (default=250)\n  max_tcp_connections = 250\n\n  ## Enable TCP keep alive probes (default=false)\n  tcp_keep_alive = false\n\n  ## Specifies the keep-alive period for an active network connection.\n  ## Only applies to TCP sockets and will be ignored if tcp_keep_alive is false.\n  ## Defaults to the OS configuration.\n  # tcp_keep_alive_period = \"2h\"\n\n  ## Address and port to host UDP listener on\n  service_address = \":8125\"\n\n  ## The following configuration options control when telegraf clears it's cache\n  ## of previous values. If set to false, then telegraf will only clear it's\n  ## cache when the daemon is restarted.\n  ## Reset gauges every interval (default=true)\n  delete_gauges = true\n  ## Reset counters every interval (default=true)\n  delete_counters = true\n  ## Reset sets every interval (default=true)\n  delete_sets = true\n  ## Reset timings \u0026 histograms every interval (default=true)\n  delete_timings = true\n\n  ## Percentiles to calculate for timing \u0026 histogram stats.\n  percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0]\n\n  ## separator to use between elements of a statsd metric\n  metric_separator = \"_\"\n\n  ## Parses tags in the datadog statsd format\n  ## http://docs.datadoghq.com/guides/dogstatsd/\n  ## deprecated in 1.10; use datadog_extensions option instead\n  parse_data_dog_tags = false\n\n  ## Parses extensions to statsd in the datadog statsd format\n  ## currently supports metrics and datadog tags.\n  ## http://docs.datadoghq.com/guides/dogstatsd/\n  datadog_extensions = false\n\n  ## Parses distributions metric as specified in the datadog statsd format\n  ## https://docs.datadoghq.com/developers/metrics/types/?tab=distribution#definition\n  datadog_distributions = false\n\n  ## Statsd data translation templates, more info can be read here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/TEMPLATE_PATTERN.md\n  # templates = [\n  #     \"cpu.* measurement*\"\n  # ]\n\n  ## Number of UDP messages allowed to queue up, once filled,\n  ## the statsd server will start dropping packets\n  allowed_pending_messages = 10000\n\n  ## Number of timing/histogram values to track per-measurement in the\n  ## calculation of percentiles. Raising this limit increases the accuracy\n  ## of percentiles but also increases the memory usage and cpu time.\n  percentile_limit = 1000\n\n  ## Maximum socket buffer size in bytes, once the buffer fills up, metrics\n  ## will start dropping.  Defaults to the OS default.\n  # read_buffer_size = 65535\n\n  ## Max duration (TTL) for each metric to stay cached/reported without being updated.\n  # max_ttl = \"10h\"\n\n  ## Sanitize name method\n  ## By default, telegraf will pass names directly as they are received.\n  ## However, upstream statsd now does sanitization of names which can be\n  ## enabled by using the \"upstream\" method option. This option will a) replace\n  ## white space with '_', replace '/' with '-', and remove charachters not\n  ## matching 'a-zA-Z_\\-0-9\\.;='.\n  #sanitize_name_method = \"\"\n"
	},
	{
		"Name": "suricata",
		"Description": " Suricata stats and alerts plugin",
		"SampleConfig": "[[inputs.suricata]]\n  ## Data sink for Suricata stats log.\n  # This is expected to be a filename of a\n  # unix socket to be created for listening.\n  source = \"/var/run/suricata-stats.sock\"\n\n  # Delimiter for flattening field keys, e.g. subitem \"alert\" of \"detect\"\n  # becomes \"detect_alert\" when delimiter is \"_\".\n  delimiter = \"_\"\n\n  # Detect alert logs\n  alerts = false\n"
	},
	{
		"Name": "swap",
		"Description": " Read metrics about swap memory usage",
		"SampleConfig": "[[inputs.swap]]\n  # no configuration\n"
	},
	{
		"Name": "synproxy",
		"Description": " Get synproxy counter statistics from procfs",
		"SampleConfig": "[[inputs.synproxy]]\n  # no configuration\n"
	},
	{
		"Name": "syslog",
		"Description": "[[inputs.syslog]]",
		"SampleConfig": "  ## Protocol, address and port to host the syslog receiver.\n  ## If no host is specified, then localhost is used.\n  ## If no port is specified, 6514 is used (RFC5425#section-4.1).\n  ##   ex: server = \"tcp://localhost:6514\"\n  ##       server = \"udp://:6514\"\n  ##       server = \"unix:///var/run/telegraf-syslog.sock\"\n  server = \"tcp://:6514\"\n\n  ## TLS Config\n  # tls_allowed_cacerts = [\"/etc/telegraf/ca.pem\"]\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Period between keep alive probes.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  ## Only applies to stream sockets (e.g. TCP).\n  # keep_alive_period = \"5m\"\n\n  ## Maximum number of concurrent connections (default = 0).\n  ## 0 means unlimited.\n  ## Only applies to stream sockets (e.g. TCP).\n  # max_connections = 1024\n\n  ## Read timeout is the maximum time allowed for reading a single message (default = 5s).\n  ## 0 means unlimited.\n  # read_timeout = \"5s\"\n\n  ## The framing technique with which it is expected that messages are transported (default = \"octet-counting\").\n  ## Whether the messages come using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n  ## or the non-transparent framing technique (RFC6587#section-3.4.2).\n  ## Must be one of \"octect-counting\", \"non-transparent\".\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## Whether to parse in best effort mode or not (default = false).\n  ## By default best effort parsing is off.\n  # best_effort = false\n\n  ## The RFC standard to use for message parsing\n  ## By default RFC5424 is used. RFC3164 only supports UDP transport (no streaming support)\n  ## Must be one of \"RFC5424\", or \"RFC3164\".\n  # syslog_standard = \"RFC5424\"\n\n  ## Character to prepend to SD-PARAMs (default = \"_\").\n  ## A syslog message can contain multiple parameters and multiple identifiers within structured data section.\n  ## Eg., [id1 name1=\"val1\" name2=\"val2\"][id2 name1=\"val1\" nameA=\"valA\"]\n  ## For each combination a field is created.\n  ## Its name is created concatenating identifier, sdparam_separator, and parameter name.\n  # sdparam_separator = \"_\"\n"
	},
	{
		"Name": "sysstat",
		"Description": " Sysstat metrics collector",
		"SampleConfig": "[[inputs.sysstat]]\n  ## Path to the sadc command.\n  #\n  ## Common Defaults:\n  ##   Debian/Ubuntu: /usr/lib/sysstat/sadc\n  ##   Arch:          /usr/lib/sa/sadc\n  ##   RHEL/CentOS:   /usr/lib64/sa/sadc\n  sadc_path = \"/usr/lib/sa/sadc\" # required\n\n  ## Path to the sadf command, if it is not in PATH\n  # sadf_path = \"/usr/bin/sadf\"\n\n  ## Activities is a list of activities, that are passed as argument to the\n  ## sadc collector utility (e.g: DISK, SNMP etc...)\n  ## The more activities that are added, the more data is collected.\n  # activities = [\"DISK\"]\n\n  ## Group metrics to measurements.\n  ##\n  ## If group is false each metric will be prefixed with a description\n  ## and represents itself a measurement.\n  ##\n  ## If Group is true, corresponding metrics are grouped to a single measurement.\n  # group = true\n\n  ## Options for the sadf command. The values on the left represent the sadf options and\n  ## the values on the right their description (wich are used for grouping and prefixing metrics).\n  ##\n  ## Run 'sar -h' or 'man sar' to find out the supported options for your sysstat version.\n  [inputs.sysstat.options]\n    -C = \"cpu\"\n    -B = \"paging\"\n    -b = \"io\"\n    -d = \"disk\"             # requires DISK activity\n    \"-n ALL\" = \"network\"\n    \"-P ALL\" = \"per_cpu\"\n    -q = \"queue\"\n    -R = \"mem\"\n    -r = \"mem_util\"\n    -S = \"swap_util\"\n    -u = \"cpu_util\"\n    -v = \"inode\"\n    -W = \"swap\"\n    -w = \"task\"\n  # -H = \"hugepages\"        # only available for newer linux distributions\n  # \"-I ALL\" = \"interrupts\" # requires INT activity\n\n  ## Device tags can be used to add additional tags for devices. For example the configuration below\n  ## adds a tag vg with value rootvg for all metrics with sda devices.\n  # [[inputs.sysstat.device_tags.sda]]\n  #  vg = \"rootvg\"\n"
	},
	{
		"Name": "system",
		"Description": " Read metrics about system load \u0026 uptime",
		"SampleConfig": "[[inputs.system]]\n  # no configuration\n"
	},
	{
		"Name": "systemd_units",
		"Description": " Gather systemd units state",
		"SampleConfig": "[[inputs.systemd_units]]\n  ## Set timeout for systemctl execution\n  # timeout = \"1s\"\n  #\n  ## Filter for a specific unit type, default is \"service\", other possible\n  ## values are \"socket\", \"target\", \"device\", \"mount\", \"automount\", \"swap\",\n  ## \"timer\", \"path\", \"slice\" and \"scope \":\n  # unittype = \"service\"\n  #\n  ## Filter for a specific pattern, default is \"\" (i.e. all), other possible\n  ## values are valid pattern for systemctl, e.g. \"a*\" for all units with\n  ## names starting with \"a\"\n  # pattern = \"\"\n  ## pattern = \"telegraf* influxdb*\"\n  ## pattern = \"a*\"\n"
	},
	{
		"Name": "tail",
		"Description": " Parse the new lines appended to a file",
		"SampleConfig": "[[inputs.tail]]\n  ## File names or a pattern to tail.\n  ## These accept standard unix glob matching rules, but with the addition of\n  ## ** as a \"super asterisk\". ie:\n  ##   \"/var/log/**.log\"  -\u003e recursively find all .log files in /var/log\n  ##   \"/var/log/*/*.log\" -\u003e find all .log files with a parent dir in /var/log\n  ##   \"/var/log/apache.log\" -\u003e just tail the apache log file\n  ##   \"/var/log/log[!1-2]*  -\u003e tail files without 1-2\n  ##   \"/var/log/log[^1-2]*  -\u003e identical behavior as above\n  ## See https://github.com/gobwas/glob for more examples\n  ##\n  files = [\"/var/mymetrics.out\"]\n\n  ## Read file from beginning.\n  # from_beginning = false\n\n  ## Whether file is a named pipe\n  # pipe = false\n\n  ## Method used to watch for file updates.  Can be either \"inotify\" or \"poll\".\n  # watch_method = \"inotify\"\n\n  ## Maximum lines of the file to process that have not yet be written by the\n  ## output.  For best throughput set based on the number of metrics on each\n  ## line and the size of the output's metric_batch_size.\n  # max_undelivered_lines = 1000\n\n  ## Character encoding to use when interpreting the file contents.  Invalid\n  ## characters are replaced using the unicode replacement character.  When set\n  ## to the empty string the data is not decoded to text.\n  ##   ex: character_encoding = \"utf-8\"\n  ##       character_encoding = \"utf-16le\"\n  ##       character_encoding = \"utf-16be\"\n  ##       character_encoding = \"\"\n  # character_encoding = \"\"\n\n  ## Data format to consume.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Set the tag that will contain the path of the tailed file. If you don't want this tag, set it to an empty string.\n  # path_tag = \"path\"\n\n  ## Filters to apply to files before generating metrics\n  ## \"ansi_color\" removes ANSI colors\n  # filters = []\n\n  ## multiline parser/codec\n  ## https://www.elastic.co/guide/en/logstash/2.4/plugins-filters-multiline.html\n  #[inputs.tail.multiline]\n    ## The pattern should be a regexp which matches what you believe to be an indicator that the field is part of an event consisting of multiple lines of log data.\n    #pattern = \"^\\s\"\n\n    ## The field's value must be previous or next and indicates the relation to the\n    ## multi-line event.\n    #match_which_line = \"previous\"\n\n    ## The invert_match can be true or false (defaults to false).\n    ## If true, a message not matching the pattern will constitute a match of the multiline filter and the what will be applied. (vice-versa is also true)\n    #invert_match = false\n\n    #After the specified timeout, this plugin sends the multiline event even if no new pattern is found to start a new event. The default is 5s.\n    #timeout = 5s\n"
	},
	{
		"Name": "tcp_listener",
		"Description": " Generic TCP listener",
		"SampleConfig": "[[inputs.tcp_listener]]\n  # socket_listener plugin\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n"
	},
	{
		"Name": "teamspeak",
		"Description": " Reads metrics from a Teamspeak 3 Server via ServerQuery",
		"SampleConfig": "[[inputs.teamspeak]]\n  ## Server address for Teamspeak 3 ServerQuery\n  # server = \"127.0.0.1:10011\"\n  ## Username for ServerQuery\n  username = \"serverqueryuser\"\n  ## Password for ServerQuery\n  password = \"secret\"\n  ## Nickname of the ServerQuery client\n  nickname = \"telegraf\"\n  ## Array of virtual servers\n  # virtual_servers = [1]\n"
	},
	{
		"Name": "temp",
		"Description": " Read metrics about temperature",
		"SampleConfig": "[[inputs.temp]]\n  # no configuration\n"
	},
	{
		"Name": "tengine",
		"Description": " Read Tengine's basic status information (ngx_http_reqstat_module)",
		"SampleConfig": "[[inputs.tengine]]\n  ## An array of Tengine reqstat module URI to gather stats.\n  urls = [\"http://127.0.0.1/us\"]\n\n  ## HTTP response timeout (default: 5s)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "tomcat",
		"Description": " Gather metrics from the Tomcat server status page.",
		"SampleConfig": "[[inputs.tomcat]]\n  ## URL of the Tomcat server status\n  # url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n\n  ## HTTP Basic Auth Credentials\n  # username = \"tomcat\"\n  # password = \"s3cret\"\n\n  ## Request timeout\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "trig",
		"Description": " Inserts sine and cosine waves for demonstration purposes",
		"SampleConfig": "[[inputs.trig]]\n  ## Set the amplitude\n  amplitude = 10.0\n"
	},
	{
		"Name": "twemproxy",
		"Description": " Read Twemproxy stats data",
		"SampleConfig": "[[inputs.twemproxy]]\n  ## Twemproxy stats address and port (no scheme)\n  addr = \"localhost:22222\"\n  ## Monitor pool name\n  pools = [\"redis_pool\", \"mc_pool\"]\n"
	},
	{
		"Name": "udp_listener",
		"Description": " Generic UDP listener",
		"SampleConfig": "[[inputs.udp_listener]]\n  # see https://github.com/influxdata/telegraf/tree/master/plugins/inputs/socket_listener\n"
	},
	{
		"Name": "unbound",
		"Description": " A plugin to collect stats from the Unbound DNS resolver",
		"SampleConfig": "[[inputs.unbound]]\n  ## Address of server to connect to, read from unbound conf default, optionally ':port'\n  ## Will lookup IP if given a hostname\n  server = \"127.0.0.1:8953\"\n\n  ## If running as a restricted user you can prepend sudo for additional access:\n  # use_sudo = false\n\n  ## The default location of the unbound-control binary can be overridden with:\n  # binary = \"/usr/sbin/unbound-control\"\n\n  ## The default location of the unbound config file can be overridden with:\n  # config_file = \"/etc/unbound/unbound.conf\"\n\n  ## The default timeout of 1s can be overridden with:\n  # timeout = \"1s\"\n\n  ## When set to true, thread metrics are tagged with the thread id.\n  ##\n  ## The default is false for backwards compatibility, and will be changed to\n  ## true in a future version.  It is recommended to set to true on new\n  ## deployments.\n  thread_as_tag = false\n"
	},
	{
		"Name": "uwsgi",
		"Description": " Read uWSGI metrics.",
		"SampleConfig": "[[inputs.uwsgi]]\n  ## List with urls of uWSGI Stats servers. Url must match pattern:\n  ## scheme://address[:port]\n  ##\n  ## For example:\n  ## servers = [\"tcp://localhost:5050\", \"http://localhost:1717\", \"unix:///tmp/statsock\"]\n  servers = [\"tcp://127.0.0.1:1717\"]\n\n  ## General connection timeout\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "varnish",
		"Description": " A plugin to collect stats from Varnish HTTP Cache",
		"SampleConfig": "[[inputs.varnish]]\n  ## If running as a restricted user you can prepend sudo for additional access:\n  #use_sudo = false\n\n  ## The default location of the varnishstat binary can be overridden with:\n  binary = \"/usr/bin/varnishstat\"\n\n  ## Additional custom arguments for the varnishstat command\n  # binary_args = [\"-f\", \"MAIN.*\"]\n\n  ## The default location of the varnishadm binary can be overridden with:\n  adm_binary = \"/usr/bin/varnishadm\"\n\n  ## Custom arguments for the varnishadm command\n  # adm_binary_args = [\"\"]\n\n  ## Metric version defaults to metric_version=1, use metric_version=2 for removal of nonactive vcls\n  ## Varnish 6.0.2 and newer is required for metric_version=2.\n  metric_version = 1\n\n  ## Additional regexps to override builtin conversion of varnish metrics into telegraf metrics.\n  ## Regexp group \"_vcl\" is used for extracting the VCL name. Metrics that contain nonactive VCL's are skipped.\n  ## Regexp group \"_field\" overrides the field name. Other named regexp groups are used as tags.\n  # regexps = ['^XCNT\\.(?P\u003c_vcl\u003e[\\w\\-]*)(\\.)*(?P\u003cgroup\u003e[\\w\\-.+]*)\\.(?P\u003c_field\u003e[\\w\\-.+]*)\\.val']\n\n  ## By default, telegraf gather stats for 3 metric points.\n  ## Setting stats will override the defaults shown below.\n  ## Glob matching can be used, ie, stats = [\"MAIN.*\"]\n  ## stats may also be set to [\"*\"], which will collect all stats\n  stats = [\"MAIN.cache_hit\", \"MAIN.cache_miss\", \"MAIN.uptime\"]\n\n  ## Optional name for the varnish instance (or working directory) to query\n  ## Usually append after -n in varnish cli\n  # instance_name = instanceName\n\n  ## Timeout for varnishstat command\n  # timeout = \"1s\"\n"
	},
	{
		"Name": "vault",
		"Description": " Read metrics from the Vault API",
		"SampleConfig": "[[inputs.vault]]\n  ## URL for the Vault agent\n  # url = \"http://127.0.0.1:8200\"\n\n  ## Use Vault token for authorization.\n  ## Vault token configuration is mandatory.\n  ## If both are empty or both are set, an error is thrown.\n  # token_file = \"/path/to/auth/token\"\n  ## OR\n  token = \"s.CDDrgg5zPv5ssI0Z2P4qxJj2\"\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = /path/to/cafile\n  # tls_cert = /path/to/certfile\n  # tls_key = /path/to/keyfile\n"
	},
	{
		"Name": "vsphere",
		"Description": "vm_metric_exclude = [ \"*\" ]",
		"SampleConfig": ""
	},
	{
		"Name": "webhooks",
		"Description": " A Webhooks Event collector",
		"SampleConfig": "[[inputs.webhooks]]\n  ## Address and port to host Webhook listener on\n  service_address = \":1619\"\n\n  [inputs.webhooks.filestack]\n    path = \"/filestack\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.github]\n    path = \"/github\"\n    # secret = \"\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.mandrill]\n    path = \"/mandrill\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.rollbar]\n    path = \"/rollbar\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.papertrail]\n    path = \"/papertrail\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n\n  [inputs.webhooks.particle]\n    path = \"/particle\"\n\n    ## HTTP basic auth\n    #username = \"\"\n    #password = \"\"\n  \n  [inputs.webhooks.artifactory]\n    path = \"/artifactory\"\n"
	},
	{
		"Name": "win_eventlog",
		"Description": " Input plugin to collect Windows Event Log messages",
		"SampleConfig": "[[inputs.win_eventlog]]\n  ## Telegraf should have Administrator permissions to subscribe for some Windows Events channels\n  ## (System log, for example)\n\n  ## LCID (Locale ID) for event rendering\n  ## 1033 to force English language\n  ## 0 to use default Windows locale\n  # locale = 0\n\n  ## Name of eventlog, used only if xpath_query is empty\n  ## Example: \"Application\"\n  # eventlog_name = \"\"\n\n  ## xpath_query can be in defined short form like \"Event/System[EventID=999]\"\n  ## or you can form a XML Query. Refer to the Consuming Events article:\n  ## https://docs.microsoft.com/en-us/windows/win32/wes/consuming-events\n  ## XML query is the recommended form, because it is most flexible\n  ## You can create or debug XML Query by creating Custom View in Windows Event Viewer\n  ## and then copying resulting XML here\n  xpath_query = '''\n  \u003cQueryList\u003e\n    \u003cQuery Id=\"0\" Path=\"Security\"\u003e\n      \u003cSelect Path=\"Security\"\u003e*\u003c/Select\u003e\n      \u003cSuppress Path=\"Security\"\u003e*[System[( (EventID \u0026gt;= 5152 and EventID \u0026lt;= 5158) or EventID=5379 or EventID=4672)]]\u003c/Suppress\u003e\n    \u003c/Query\u003e\n    \u003cQuery Id=\"1\" Path=\"Application\"\u003e\n      \u003cSelect Path=\"Application\"\u003e*[System[(Level \u0026lt; 4)]]\u003c/Select\u003e\n    \u003c/Query\u003e\n    \u003cQuery Id=\"2\" Path=\"Windows PowerShell\"\u003e\n      \u003cSelect Path=\"Windows PowerShell\"\u003e*[System[(Level \u0026lt; 4)]]\u003c/Select\u003e\n    \u003c/Query\u003e\n    \u003cQuery Id=\"3\" Path=\"System\"\u003e\n      \u003cSelect Path=\"System\"\u003e*\u003c/Select\u003e\n    \u003c/Query\u003e\n    \u003cQuery Id=\"4\" Path=\"Setup\"\u003e\n      \u003cSelect Path=\"Setup\"\u003e*\u003c/Select\u003e\n    \u003c/Query\u003e\n  \u003c/QueryList\u003e\n  '''\n\n  ## System field names:\n  ##   \"Source\", \"EventID\", \"Version\", \"Level\", \"Task\", \"Opcode\", \"Keywords\", \"TimeCreated\",\n  ##   \"EventRecordID\", \"ActivityID\", \"RelatedActivityID\", \"ProcessID\", \"ThreadID\", \"ProcessName\",\n  ##   \"Channel\", \"Computer\", \"UserID\", \"UserName\", \"Message\", \"LevelText\", \"TaskText\", \"OpcodeText\"\n\n  ## In addition to System, Data fields can be unrolled from additional XML nodes in event.\n  ## Human-readable representation of those nodes is formatted into event Message field,\n  ## but XML is more machine-parsable\n\n  # Process UserData XML to fields, if this node exists in Event XML\n  process_userdata = true\n\n  # Process EventData XML to fields, if this node exists in Event XML\n  process_eventdata = true\n\n  ## Separator character to use for unrolled XML Data field names\n  separator = \"_\"\n\n  ## Get only first line of Message field. For most events first line is usually more than enough\n  only_first_line_of_message = true\n\n  ## Parse timestamp from TimeCreated.SystemTime event field.\n  ## Will default to current time of telegraf processing on parsing error or if set to false\n  timestamp_from_event = true\n\n  ## Fields to include as tags. Globbing supported (\"Level*\" for both \"Level\" and \"LevelText\")\n  event_tags = [\"Source\", \"EventID\", \"Level\", \"LevelText\", \"Task\", \"TaskText\", \"Opcode\", \"OpcodeText\", \"Keywords\", \"Channel\", \"Computer\"]\n\n  ## Default list of fields to send. All fields are sent by default. Globbing supported\n  event_fields = [\"*\"]\n\n  ## Fields to exclude. Also applied to data fields. Globbing supported\n  exclude_fields = [\"TimeCreated\", \"Binary\", \"Data_Address*\"]\n\n  ## Skip those tags or fields if their value is empty or equals to zero. Globbing supported\n  exclude_empty = [\"*ActivityID\", \"UserID\"]\n"
	},
	{
		"Name": "win_perf_counters",
		"Description": " # Input plugin to counterPath Performance Counters on Windows operating systems",
		"SampleConfig": "# [[inputs.win_perf_counters]]\n#   ## By default this plugin returns basic CPU and Disk statistics.\n#   ## See the README file for more examples.\n#   ## Uncomment examples below or write your own as you see fit. If the system\n#   ## being polled for data does not have the Object at startup of the Telegraf\n#   ## agent, it will not be gathered.\n#   ## Settings:\n#   # PrintValid = false # Print All matching performance counters\n#   # Whether request a timestamp along with the PerfCounter data or just use current time\n#   # UsePerfCounterTime=true\n#   # If UseWildcardsExpansion params is set to true, wildcards (partial wildcards in instance names and wildcards in counters names) in configured counter paths will be expanded\n#   # and in case of localized Windows, counter paths will be also localized. It also returns instance indexes in instance names.\n#   # If false, wildcards (not partial) in instance names will still be expanded, but instance indexes will not be returned in instance names.\n#   #UseWildcardsExpansion = false\n#   # When running on a localized version of Windows and with UseWildcardsExpansion = true, Windows will\n#   # localize object and counter names. When LocalizeWildcardsExpansion = false, use the names in object.Counters instead\n#   # of the localized names. Only Instances can have wildcards in this case. ObjectName and Counters must not have wildcards when this\n#   # setting is false.\n#   #LocalizeWildcardsExpansion = true\n#   # Period after which counters will be reread from configuration and wildcards in counter paths expanded\n#   CountersRefreshInterval=\"1m\"\n#   ## Accepts a list of PDH error codes which are defined in pdh.go, if this error is encountered it will be ignored\n#   ## For example, you can provide \"PDH_NO_DATA\" to ignore performance counters with no instances\n#   ## By default no errors are ignored\n#   ## You can find the list here: https://github.com/influxdata/telegraf/blob/master/plugins/inputs/win_perf_counters/pdh.go\n#   ## e.g.: IgnoredErrors = [\"PDH_NO_DATA\"]\n#   # IgnoredErrors = []\n#\n#   [[inputs.win_perf_counters.object]]\n#     # Processor usage, alternative to native, reports on a per core.\n#     ObjectName = \"Processor\"\n#     Instances = [\"*\"]\n#     Counters = [\n#       \"% Idle Time\",\n#       \"% Interrupt Time\",\n#       \"% Privileged Time\",\n#       \"% User Time\",\n#       \"% Processor Time\",\n#       \"% DPC Time\",\n#     ]\n#     Measurement = \"win_cpu\"\n#     # Set to true to include _Total instance when querying for all (*).\n#     # IncludeTotal=false\n#     # Print out when the performance counter is missing from object, counter or instance.\n#     # WarnOnMissing = false\n#     # Gather raw values instead of formatted. Raw value is stored in the field name with the \"_Raw\" suffix, e.g. \"Disk_Read_Bytes_sec_Raw\".\n#     # UseRawValues = true\n#\n#   [[inputs.win_perf_counters.object]]\n#     # Disk times and queues\n#     ObjectName = \"LogicalDisk\"\n#     Instances = [\"*\"]\n#     Counters = [\n#       \"% Idle Time\",\n#       \"% Disk Time\",\n#       \"% Disk Read Time\",\n#       \"% Disk Write Time\",\n#       \"% User Time\",\n#       \"% Free Space\",\n#       \"Current Disk Queue Length\",\n#       \"Free Megabytes\",\n#     ]\n#     Measurement = \"win_disk\"\n#\n#   [[inputs.win_perf_counters.object]]\n#     ObjectName = \"PhysicalDisk\"\n#     Instances = [\"*\"]\n#     Counters = [\n#       \"Disk Read Bytes/sec\",\n#       \"Disk Write Bytes/sec\",\n#       \"Current Disk Queue Length\",\n#       \"Disk Reads/sec\",\n#       \"Disk Writes/sec\",\n#       \"% Disk Time\",\n#       \"% Disk Read Time\",\n#       \"% Disk Write Time\",\n#     ]\n#     Measurement = \"win_diskio\"\n#\n#   [[inputs.win_perf_counters.object]]\n#     ObjectName = \"Network Interface\"\n#     Instances = [\"*\"]\n#     Counters = [\n#       \"Bytes Received/sec\",\n#       \"Bytes Sent/sec\",\n#       \"Packets Received/sec\",\n#       \"Packets Sent/sec\",\n#       \"Packets Received Discarded\",\n#       \"Packets Outbound Discarded\",\n#       \"Packets Received Errors\",\n#       \"Packets Outbound Errors\",\n#     ]\n#     Measurement = \"win_net\"\n#\n#\n#   [[inputs.win_perf_counters.object]]\n#     ObjectName = \"System\"\n#     Counters = [\n#       \"Context Switches/sec\",\n#       \"System Calls/sec\",\n#       \"Processor Queue Length\",\n#       \"System Up Time\",\n#     ]\n#     Instances = [\"------\"]\n#     Measurement = \"win_system\"\n#\n#   [[inputs.win_perf_counters.object]]\n#     # Example counterPath where the Instance portion must be removed to get data back,\n#     # such as from the Memory object.\n#     ObjectName = \"Memory\"\n#     Counters = [\n#       \"Available Bytes\",\n#       \"Cache Faults/sec\",\n#       \"Demand Zero Faults/sec\",\n#       \"Page Faults/sec\",\n#       \"Pages/sec\",\n#       \"Transition Faults/sec\",\n#       \"Pool Nonpaged Bytes\",\n#       \"Pool Paged Bytes\",\n#       \"Standby Cache Reserve Bytes\",\n#       \"Standby Cache Normal Priority Bytes\",\n#       \"Standby Cache Core Bytes\",\n#     ]\n#     Instances = [\"------\"] # Use 6 x - to remove the Instance bit from the counterPath.\n#     Measurement = \"win_mem\"\n#\n#   [[inputs.win_perf_counters.object]]\n#     # Example query where the Instance portion must be removed to get data back,\n#     # such as from the Paging File object.\n#     ObjectName = \"Paging File\"\n#     Counters = [\n#       \"% Usage\",\n#     ]\n#     Instances = [\"_Total\"]\n#     Measurement = \"win_swap\"\n"
	},
	{
		"Name": "win_services",
		"Description": " Input plugin to report Windows services info.",
		"SampleConfig": "[[inputs.win_services]]\n  ## Names of the services to monitor. Leave empty to monitor all the available services on the host. Globs accepted. Case sensitive.\n  service_names = [\n    \"LanmanServer\",\n    \"TermService\",\n    \"Win*\",\n  ]\n  excluded_service_names = ['WinRM'] # optional, list of service names to exclude\n"
	},
	{
		"Name": "wireguard",
		"Description": " Collect Wireguard server interface and peer statistics",
		"SampleConfig": "[[inputs.wireguard]]\n  ## Optional list of Wireguard device/interface names to query.\n  ## If omitted, all Wireguard interfaces are queried.\n  # devices = [\"wg0\"]\n"
	},
	{
		"Name": "wireless",
		"Description": " Monitor wifi signal strength and quality",
		"SampleConfig": "[[inputs.wireless]]\n  ## Sets 'proc' directory path\n  ## If not specified, then default is /proc\n  # host_proc = \"/proc\"\n"
	},
	{
		"Name": "x509_cert",
		"Description": " Reads metrics from a SSL certificate",
		"SampleConfig": "[[inputs.x509_cert]]\n  ## List certificate sources, support wildcard expands for files\n  ## Prefix your entry with 'file://' if you intend to use relative paths\n  sources = [\"tcp://example.org:443\", \"https://influxdata.com:443\",\n            \"udp://127.0.0.1:4433\", \"/etc/ssl/certs/ssl-cert-snakeoil.pem\",\n            \"/etc/mycerts/*.mydomain.org.pem\", \"file:///path/to/*.pem\"]\n\n  ## Timeout for SSL connection\n  # timeout = \"5s\"\n\n  ## Pass a different name into the TLS request (Server Name Indication).\n  ## This is synonymous with tls_server_name, and only one of the two\n  ## options may be specified at one time.\n  ##   example: server_name = \"myhost.example.org\"\n  # server_name = \"myhost.example.org\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  # tls_server_name = \"myhost.example.org\"\n"
	},
	{
		"Name": "xtremio",
		"Description": " # Gathers Metrics From a Dell EMC XtremIO Storage Array's V3 API",
		"SampleConfig": "[[inputs.xtremio]]\n  ## XtremIO User Interface Endpoint\n  url = \"https://xtremio.example.com/\" # required\n\n  ## Credentials\n  username = \"user1\"\n  password = \"pass123\"\n\n  ## Metrics to collect from the XtremIO\n  # collectors = [\"bbus\",\"clusters\",\"ssds\",\"volumes\",\"xms\"]\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use SSL but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "zfs",
		"Description": " Read metrics of ZFS from arcstats, zfetchstats, vdev_cache_stats, pools and datasets",
		"SampleConfig": "[[inputs.zfs]]\n  ## ZFS kstat path. Ignored on FreeBSD\n  ## If not specified, then default is:\n  # kstatPath = \"/proc/spl/kstat/zfs\"\n\n  ## By default, telegraf gather all zfs stats\n  ## Override the stats list using the kstatMetrics array:\n  ## For FreeBSD, the default is:\n  # kstatMetrics = [\"arcstats\", \"zfetchstats\", \"vdev_cache_stats\"]\n  ## For Linux, the default is:\n  # kstatMetrics = [\"abdstats\", \"arcstats\", \"dnodestats\", \"dbufcachestats\",\n  #     \"dmu_tx\", \"fm\", \"vdev_mirror_stats\", \"zfetchstats\", \"zil\"]\n\n  ## By default, don't gather zpool stats\n  # poolMetrics = false\n\n  ## By default, don't gather dataset stats\n  ## On FreeBSD, if the user has enabled listsnapshots in the pool property,\n  ## telegraf may not be able to correctly parse the output.\n  # datasetMetrics = false\n"
	},
	{
		"Name": "zipkin",
		"Description": " This plugin implements the Zipkin http server to gather trace and timing data needed to troubleshoot latency problems in microservice architectures.",
		"SampleConfig": "[[inputs.zipkin]]\n  # path = \"/api/v1/spans\" # URL path for span data\n  # port = 9411 # Port on which Telegraf listens\n"
	},
	{
		"Name": "zookeeper",
		"Description": " Reads 'mntr' stats from one or many zookeeper servers",
		"SampleConfig": "[[inputs.zookeeper]]\n  ## An array of address to gather stats about. Specify an ip or hostname\n  ## with port. ie localhost:2181, 10.0.0.1:2181, etc.\n\n  ## If no servers are specified, then localhost is used as the host.\n  ## If no port is specified, 2181 is used\n  servers = [\":2181\"]\n\n  ## Timeout for metric collections from all servers.  Minimum timeout is \"1s\".\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # enable_tls = true\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## If false, skip chain \u0026 host verification\n  # insecure_skip_verify = true\n"
	}
]